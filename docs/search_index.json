[
["index.html", "Piecemeal R 1 Welcome", " Piecemeal R Tutorial for Data Exploration with R Last updated: 2018-06-26 1 Welcome Welcome to a tutorial website for data analysis and visualization with R. Two objectives of this site are: Introducing R’s modern data analytics tools (Section 2) Providing topic-based tutorials (Section 4) Additional materials include: Essential concepts for R programming (Section 3) List of additional resources (Section 5) "],
["about.html", "About", " About This site is created and maintained by Kota Minegishi, an assistant professor at the University of Minnesota. He is an agricultural economist and works in the Department of Animal Science. "],
["2-intro.html", "2 Introduction", " 2 Introduction Prerequisites In what follows below, we assume that you have R and RStudio (free/open-source IDE) installed. Sneak Peek If you are new to R, the following cheatsheets give you a good idea of how it is like to conduct data analyses in R: Base R, RStudio IDE, dplyr, ggplot2. If you find the materials in this introduction too technical, please start with ModernDive, an open-source textbook that gave an initial inspiration to start this site. Also, more information on R is available in Section 3, as well as various resources listed in Section 5. "],
["2-1-materials.html", "2.1 Materials", " 2.1 Materials R is continuously evolving with user-contributed R packages, or a bundle of user-developed programs. Recent developments such as tidy, dplyr, and ggplot2 (often referred to as tidyverse tools) have greatly streamlined the coding for data manipulation and analysis, which is the starting point for learning R chosen for this site. This tidyverse syntax gives you an intuitive and powerful data operation language to wrangle, visualize, and analyze data. Following dplyr’s documentation, let’s start with a sample dataset of airplane departures and arrivals. This contains information on about 337,000 flights departed from New York City in 2013 (source: Bureau of Transportation Statistics). In the R console (i.e., the left bottom pane in RStudio), type install.packages(&quot;nycflights13&quot;) and hit enter. Then, load the package via library(nycflights13), in the current computing environment (called R session). Its built-in data frames will be added to your R session. Generally, R packages are installed locally on your computer on an as-needed basis. To install several more packages that we will use, copy the following code and execute it in your R console. # Since we&#39;re just starting, don&#39;t worry about understanding the code here # &quot;#&quot;&quot; symbole is for adding comments that are helpful to humans but are ignored by R required_pkgs &lt;- c(&quot;nycflights13&quot;, &quot;dplyr&quot;, &quot;ggplot2&quot;, &quot;lubridate&quot;, &quot;knitr&quot;, &quot;tidyr&quot;, &quot;broom&quot;) # creating a new object &quot;required_pkgs&quot; containing strings &quot;nycflights13&quot;, &quot;dplyr&quot;,.. # c(): concatenates string names here # &lt;-: an assignment operator from right to left new_pkgs &lt;- required_pkgs[!(required_pkgs %in% installed.packages())] # checking whether &quot;required_pkgs&quot; are already installed # [ ]:extraction by logical TRUE or FALSE # %in%: checks whether items on the left are members of the items on the right. # !: a negation operator if (length(new_pkgs)) { install.packages(new_pkgs, repos = &quot;http://cran.rstudio.com&quot;) } Once packages are downloaded and installed on your computer, they become available for your libraries. In each R session, we load libraries we need (instead of all existing libraries). Here we load the following; library(dplyr) # for data manipulation library(ggplot2) # for figures library(lubridate) # for date manipulation library(nycflights13) # sample data of NYC flights library(knitr) # for table formatting library(tidyr) # for table formatting library(broom) # for table formatting Let’s see the data. class(flights) # shows the class attribute dim(flights) # obtains dimention of rows and columns ## [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; ## [1] 336776 19 head(flights) # displays first seveal rows and columns ## # A tibble: 6 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2. 830 ## 2 2013 1 1 533 529 4. 850 ## 3 2013 1 1 542 540 2. 923 ## 4 2013 1 1 544 545 -1. 1004 ## 5 2013 1 1 554 600 -6. 812 ## 6 2013 1 1 554 558 -4. 740 ## # ... with 12 more variables: sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, ## # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, ## # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, ## # time_hour &lt;dttm&gt; dim() returns the dimensions of a data frame, and head() returns the first several rows and columns. The flights dataset contains information on dates, actual departure times, and arrival times, etc. The variables are arranged in columns, and each row is an observation of flight data. In R, we refer to a dataset as data frame, which is a class of R object. The data frame class is more general than the matrix class in that it can contain variables of more than one mode (numeric, character, factor etc). "],
["2-2-crafts.html", "2.2 Crafts", " 2.2 Crafts We will focus on six data wrangling functions in the dplyr package: filter(): extracts rows (observations) of a data frame based on logical vectors. select(): extracts columns (variables) of a data frame based on column names. arrange(): orders rows of a data frame based on column names. summarise(): collapses a data frame into summary statistics based on summary functions (e.g., statistics functions) specified with column names. mutate(): creates new variables and adds them to the existing columns based on window functions (e.g., transforming operations) specified with column names. group_by(): assigns rows into groups within a data frame based on column names. The very first argument in all these functions is a data frame, followed by logical vectors, column names, or other kinds of items. This allows for applying these functions sequentially through the first argument; for example, func3(func2(func1(data,...), ...), ...). This can be rewritten as data %&gt;% func1(...) %&gt;% func2(...) %&gt;% func3(...) via a pipe operator %&gt;%. This lets us express a sequence of data wrangling operations in plain English. Specifically, we read %&gt;% as then, so that the above example becomes; start with the data, then apply func1(...), then apply func2(...), then func3(..). Say, we want to find the average of delays in departures and arrivals from New York to the St. Paul-Minneapolis airport (MSP). We can construct the following sequence of instructions: start with the flight data frame, apply filter() to extract the rows of flights to MSP, and then apply summarise() to calculate the mean. flights %&gt;% # data frame &quot;flights&quot;, then filter(dest == &quot;MSP&quot;) %&gt;% # filter rows, then summarise( # summarise departure and arrival delays for their means # and call them mean_dep_delay and mean_arr_delay respectively mean_dep_delay = mean(dep_delay, na.rm = TRUE), mean_arr_delay = mean(arr_delay, na.rm = TRUE) ) # calculate the mean, while removing NA values ## # A tibble: 1 x 2 ## mean_dep_delay mean_arr_delay ## &lt;dbl&gt; &lt;dbl&gt; ## 1 13.3 7.27 In summarise(), one can use summary functions that maps a vector to a scaler. Examples include functions like mean(), sd() (standard deviation), quantile(), min(), max(), and n() (observation count in the dplyr package). Each time we apply the %&gt;% operator above, we pass a modified data frame from one data wrangling operation to another through the first argument. The above code is equivalent to summarise( # data frame &quot;flights&quot; is inside filter(), which is inside summarise() filter(flights, dest == &quot;MSP&quot;), mean_dep_delay = mean(dep_delay, na.rm = TRUE), mean_arr_delay = mean(arr_delay, na.rm = TRUE) ) ## # A tibble: 1 x 2 ## mean_dep_delay mean_arr_delay ## &lt;dbl&gt; &lt;dbl&gt; ## 1 13.3 7.27 You will quickly discover that %&gt;% operator makes the code much easier to read, write, and edit and how that inspires you to explore the data more. Let’s add a few more lines to the previous example. Say, now we want to see the average delay by carrier and sort the results by the number of observations (e.g. flights) in descending order. Okay, what do we do? We make a sequence of data wrangling operations in plain English and translate that into code by replacing then with pipe operator %&gt;%. For example, try thinking this way; “start with the data frame flights; then (%&gt;%) filter() to extract the rows of flights to MSP; then group rows by carrier; then summarise() data for the number of observations and the means; then arrange() the results by the observation count in descending order.” flight_stats_MSP &lt;- flights %&gt;% # assign the results to an object named &quot;flight_stats&quot; filter(dest == &quot;MSP&quot;) %&gt;% group_by(carrier) %&gt;% # group rows by carrier summarise( n_obs = n(), # count number of rows mean_dep_delay = mean(dep_delay, na.rm = TRUE), mean_arr_delay = mean(arr_delay, na.rm = TRUE) ) %&gt;% arrange(desc(n_obs)) # sort by n_obs in descending order flight_stats_MSP # show flight_stats object ## # A tibble: 6 x 4 ## carrier n_obs mean_dep_delay mean_arr_delay ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 DL 2864 10.7 4.04 ## 2 EV 1773 17.1 10.5 ## 3 MQ 1293 8.26 9.56 ## 4 9E 1249 19.7 8.09 ## 5 OO 4 0.750 -2.00 ## 6 UA 2 -6.00 -5.50 Tip #1: Frame tasks in a sequence and use pipe operator %&gt;% accordingly. The carrier variable is expressed in the International Air Transportation Association (IATA) code, so let’s add a column of carrier names by joining another data frame called airlines. In RStudio, you can find this data frame under the Environment tab (in the upper right corner); switch the display option from Global Environment to package:nycflights13. To inspect the data frame, type View(airlines) in the R console. Also, by typing data() you can see a list of all datasets that are loaded with libraries. # left_join(a,b, by=&quot;var&quot;) joins two data frames a and b by matching rows of b to a # by identifier variable &quot;var&quot;. # kable() prints a better-looking table here left_join(flight_stats_MSP, airlines, by=&quot;carrier&quot;) %&gt;% kable(digits=2) carrier n_obs mean_dep_delay mean_arr_delay name DL 2864 10.65 4.04 Delta Air Lines Inc. EV 1773 17.09 10.53 ExpressJet Airlines Inc. MQ 1293 8.26 9.56 Envoy Air 9E 1249 19.66 8.09 Endeavor Air Inc. OO 4 0.75 -2.00 SkyWest Airlines Inc. UA 2 -6.00 -5.50 United Air Lines Inc. In the next example, we add new variables to flights using mutate(). flights %&gt;% # keep only columns named &quot;dep_delay&quot; and &quot;arr_delay&quot; select(dep_delay, arr_delay) %&gt;% mutate( gain = arr_delay - dep_delay, gain_rank = round(percent_rank(gain), digits = 2) # Note: we can immediately use the &quot;gain&quot; variable we just defined. ) ## # A tibble: 336,776 x 4 ## dep_delay arr_delay gain gain_rank ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2. 11. 9. 0.810 ## 2 4. 20. 16. 0.880 ## 3 2. 33. 31. 0.940 ## 4 -1. -18. -17. 0.220 ## 5 -6. -25. -19. 0.180 ## 6 -4. 12. 16. 0.880 ## 7 -5. 19. 24. 0.920 ## 8 -3. -14. -11. 0.370 ## 9 -3. -8. -5. 0.540 ## 10 -2. 8. 10. 0.820 ## # ... with 336,766 more rows We extracted specific columns of flights by select() and added new columns defined in mutate(). mutate() differs from summarise() in that mutate() adds new columns to the data frame, while summarise() collapses the data frame into a summary table. There are roughly five types of window functions that are commonly used inside mutate(): summary functions, which are interpreted as a vector of repeated values (e.g., a column of an identical mean value) ranking or ordering functions (e.g., row_number(), min_rank(), dense_rank(), cume_dist(), percent_rank(), and ntile()) offset functions, say defining a lagged variable in time series data (lead() and lag()) cumulative aggregates (e.g., cumsum(), cummin(), cummax(), cumall(), cumany(), and cummean()) fixed-window rolling aggregates such as a windowed mean, median, etc. To look up documentations of these function, for example, type ?cumsum. Tip #2: In the beginning you might find it confusing to choose between summarise() and mutate(). Just remeber mutate() is for creating new variables or overwriting existing variables. By no means, the use of summarise() and mutate() is not restricted to these functions listed above. You can define your own function and apply inside summarise() or mutate(). Let’s quickly go over what a function is in R and how you can use a custom function in the tidyverse syntax. In R, we use function() to define a function, which consists of a function name, input arguments separated by comma, and a body containing tasks to be performed (multiple expressions are bundled by brackets { }, and the last expression is returned as an output). your_function_name &lt;- function(input_arg1, input_arg2) { task1 task2 . . . output_to_return } For a function having only a single expression to execute, we can omit brackets { }. another_function &lt;- function(input args) task_and_output_in_a_single_expression Let’s go through a few examples. vec1 &lt;- c(1:10, NA, NA) vec1 ## [1] 1 2 3 4 5 6 7 8 9 10 NA NA my_mean &lt;- function(x, na.rm=TRUE) mean(x, na.rm = na.rm) # sets the default of &quot;na.rm&quot; argument to be TRUE. mean(vec1) # returns NA ## [1] NA my_mean(vec1) ## [1] 5.5 my_mean(vec1, na.rm=FALSE) # returns NA ## [1] NA my_zscore &lt;- function(x, remove_na=TRUE, digits=2) { zscore &lt;- (x - my_mean(x, na.rm = remove_na))/sd(x, na.rm = remove_na) round(zscore, digits=digits) } # calculates a z-score of vector x my_zscore(vec1) ## [1] -1.49 -1.16 -0.83 -0.50 -0.17 0.17 0.50 0.83 1.16 1.49 NA ## [12] NA Let’s try using functions my_mean() and my_zscore() in summarise() and mutate(). flights %&gt;% select(dep_delay) %&gt;% summarise( mean_dep_delay_na = mean(dep_delay), # returns NA mean_dep_delay_1 = mean(dep_delay, na.rm = TRUE), # passing argument na.rm = TRUE mean_dep_delay_2 = my_mean(dep_delay) # using my_mean() ) %&gt;% kable(digits=2) mean_dep_delay_na mean_dep_delay_1 mean_dep_delay_2 NA 12.64 12.64 flights_gain &lt;- flights %&gt;% select(dep_delay, arr_delay) %&gt;% mutate( gain = arr_delay - dep_delay, gain_mean = my_mean(gain), # returns the same mean for all rows gain_z2 = my_zscore(gain) # using my_zscore() ) head(flights_gain) %&gt;% # show the first several rows kable(digits=2) dep_delay arr_delay gain gain_mean gain_z2 2 11 9 -5.66 0.81 4 20 16 -5.66 1.20 2 33 31 -5.66 2.03 -1 -18 -17 -5.66 -0.63 -6 -25 -19 -5.66 -0.74 -4 12 16 -5.66 1.20 summarise_all() and mutate_all() apply summary functions like mean() and sd() to all columns in a data frame. But, we can also use any custom function that returns appropriate output (i.e., a scalar output for summarise_all() and a vector output for mutate_all()). For example, here are several ways to calculate the means when the data frame contains missing values. # Calculation fails due to NA values flights_gain %&gt;% select(dep_delay, arr_delay, gain) %&gt;% summarise_all(mean) %&gt;% kable(digits=2) dep_delay arr_delay gain NA NA NA # filter rows that contain any NA value flights_gain %&gt;% select(dep_delay, arr_delay, gain) %&gt;% filter(!is.na(dep_delay) &amp; !is.na(arr_delay)) %&gt;% summarise_all(mean) %&gt;% kable(digits=2) dep_delay arr_delay gain 12.56 6.9 -5.66 # pass argument na.rm=TRUE to mean() flights_gain %&gt;% select(dep_delay, arr_delay, gain) %&gt;% summarise_all(funs(mean), na.rm=TRUE) %&gt;% kable(digits=2) dep_delay arr_delay gain 12.64 6.9 -5.66 # use a custom function with default na.rm=TRUE flights_gain %&gt;% select(dep_delay, arr_delay, gain) %&gt;% summarise_all(funs(my_mean)) %&gt;% kable(digits=2) dep_delay arr_delay gain 12.64 6.9 -5.66 To practice the tydiverse syntax above, start with data extraction by combining filter(), select(), and arrange(). Then, try summarising data via summarise() and creating new variables via mutate(), followed by adding additional layers of tasks such as grouping statistics via group_by() and filtering data by filter(). Exercise Try the following exercises using flights data. Find the set of all origin-carrier combinations whose destination is MSP. Hint: use filter(), select(), and unique(). (Ans.: 10 unique combinations.) Sort that by origin. Hint: use arrange(). Find the mean and standard deviation of air time from New York to MSP. Hint: use filter() and summarise(). Find the average arrival delay as a percentage of air time from New York to MSP. Hint: use filter(), mutate(), and summarise(). (Ans.: 4.50%.) Do the above calculation by carrier. Hint: use filter(), mutate(), group_by(), and summarise(). (Ans.: ranges from -3.57% to 4.97%.) Do the above calculation by origin and carrier and sort the results by origin. Do that in z-score using the grand mean and grand s.d. to find any origin-carrier combination exhibits statistically significant arrival delay (in percentage of air time). Hint: do the following all inside mutate(), define a percentage arrival delay variable, define variables for its mean and s.d., and define a z-statistic using those three variables. (Ans.: ranges from -0.118 to 1.584.) Take time to do these exercises since we will be using the tydiverse syntax in next section. "],
["2-3-arts.html", "2.3 Arts", " 2.3 Arts Now we will cover the tools of data visualization via ggplot2. The ggplot2 syntax has three essential components for generating graphics: data, aes, and geom, implementing the following philosophy; A statistical graphic is a mapping of data variables to aesthetic attributes of geometric objects. — (Wilkinson 2005) Coding complex graphics via ggplot() may appear at first intimidating, yet it is very simple once you understand the three primary components: data: a data frame e.g., the first argument in ggplot(data, ...). aes: specifications for x-y variables and the variables that differentiate geom objects by color , shape, or size. e.g., aes(x = var_x, y = var_y, shape = var_z) geom: geometric objects such as points, lines, bars, etc. with parameters given in the (), e.g., geom_point(), geom_line(), geom_histogram() We can further refine a plot by adding secondary components or characteristics such as stat: data transformation, overlay of statistical inferences etc. scales: scaling data points etc. coord: Cartesian coordinates, polar coordinates, mapping projections etc. facet: laying out multiple plot panels in a grid etc. However, don’t worry about learning details. You don’t need to know them all. All you need is a basic but solid understanding of the three primary components that makes up the structure of the ggplot syntax. You can find everything else by Internet search. Tip #3. Learn to ask questions in Internet search for what you want to accomplish with your custom plots. You will frequently find answers in Stack Overflow. Let’s generate five common types of plots: scatter-plots, line-graphs, boxplots, histograms, and barplots. To provide a context, we will use these plots to explore potential causes of flight departure delays. First, let’s consider the possibility of congestion at an airport during certain times of the day or certain seasons. We can use barplots to see whether there is any obvious pattern in the flight distribution across flight origins (i.e., airports) in New York City. A barplot shows observation counts (e.g., rows) by category. ggplot(data = flights, # the first argument is the data frame mapping = aes(x = origin)) + # the second argument is mapping, which is aes() geom_bar() # after &quot;+&quot; operator of ggplot(), we add geom_XXX() elements We can make the plot more informative and aesthetic. ggplot(data = flights, mapping = aes(x = origin, fill = origin)) + # here &quot;fill&quot; gives bars distinct colors geom_bar() + facet_wrap( ~ hour) # &quot;facet_wrap( ~ var)&quot; generates a grid of plots by var Another way to see the same information is a histogram. flights %&gt;% filter(hour &gt;= 5) %&gt;% # exclude hour earlier than 5 a.m. ggplot(aes(x = hour, fill = origin)) + geom_histogram(binwidth = 1, color = &quot;white&quot;) While mornings and late afternoons tend to get busy, there is not much difference in the number of flights across airports. Exercise Generate a bar graph showing the number of flights by carrier in the flights dataset. Hint: use aes(x=...) and + geom_bar(). Add color coded origins of flights to the previous graph. Hint: use aes(x=..., fill=...). Filter the flights data for the date of Janauary 1st and generate a scatter plot of distance and air time. Hint: use filter(), ggplot(aes(x=..., y=...)), and + geom_point(alpha=0.1). Add a fitted linear curve to the previous plot. Hint: use + geom_smooth(method=&quot;lm&quot;). Further filter the data for the distance range of [0, 2800] and carrier to be either “AA (American Airways)”, “DL (Delta)”, and “WN (Southwest)” and color-code the graph by carrier. Hint: use filter(..., carrier %in% c(&quot;AA&quot;, &quot;DL&quot;, &quot;WN&quot;)) and aes(x=..., y=..., color=...) and geom_smooth(method=&quot;lm&quot;, se=FALSE). References "],
["2-4-more-examples-1.html", "2.4 More examples 1", " 2.4 More examples 1 The rest of the section provides more examples of dplyr and ggplot2 functions in the context of continued exploration of the flight data. Let’s see if there are distinct patters of departure delays over the course of a year. We can do this by taking the average of departure delays for each day by flight origin and plot the data as a time series using line-graphs. delay_day &lt;- flights %&gt;% group_by(origin, year, month, day) %&gt;% summarise(dep_delay = mean(dep_delay, na.rm = TRUE)) %&gt;% mutate(date = as.Date(paste(year, month, day), format=&quot;%Y %m %d&quot;)) %&gt;% filter(!is.na(dep_delay)) # exclude rows with dep_delay == NA delay_day %&gt;% # &quot;facet_grid( var ~ .)&quot; is similar to &quot;facet_wrap( ~ var)&quot; ggplot(aes(x = date, y = dep_delay)) + geom_line() + facet_grid( origin ~ . ) Seasonal patterns seem similar across airports, and summer months appear to be busier on average. Let’s see how closely these patterns compare across the three line-graphs (EWR, JFK, and LGA) in summer months. delay_day %&gt;% filter(&quot;2013-07-01&quot; &lt;= date, &quot;2013-08-31&quot; &gt;= date) %&gt;% ggplot(aes(x = date, y = dep_delay, color = origin)) + geom_line() We can see similar patterns of spikes across airports occurring on certain days, indicating a tendency for the three airports to get busy on the same days. Would this mean that the three airports tend to be congested at the same time? In the previous figure, there seems to be some cyclical pattern of delays. A good place to start would be comparing delays by day of the week. Here is a function to calculate day of the week for a given date. # Input: date in the format as in &quot;2017-01-23&quot; # Output: day of week my_dow &lt;- function(date) { # as.POSIXlt(date)[[&#39;wday&#39;]] returns integers 0, 1, 2, .. 6, for Sun, Mon, ... Sat. # We extract one item from a vector (Sun, Mon, ..., Sat) by position numbered from 1 to 7. dow &lt;- as.POSIXlt(date)[[&#39;wday&#39;]] + 1 c(&quot;Sun&quot;, &quot;Mon&quot;, &quot;Tue&quot;, &quot;Wed&quot;, &quot;Thu&quot;, &quot;Fri&quot;, &quot;Sat&quot;)[dow] # extract &quot;dow&quot;-th element } Sys.Date() # Sys.Date() returns the current date ## [1] &quot;2018-06-22&quot; my_dow(Sys.Date()) ## [1] &quot;Fri&quot; Now, let’s take a look at the mean delay by day of the week using boxplots. delay_day &lt;- flights %&gt;% group_by(year, month, day) %&gt;% summarise(dep_delay = mean(dep_delay, na.rm = TRUE)) %&gt;% mutate(date = as.Date(paste(year, month, day), format=&quot;%Y %m %d&quot;), # date defined by as.Data() function dow = my_dow(date), weekend = dow %in% c(&quot;Sat&quot;, &quot;Sun&quot;) # %in% operator: A %in% B returns TRUE/FALSE for whether each element of A is in B. ) # show the first 10 elements of &quot;dow&quot; variable in &quot;delay_day&quot; data frame delay_day$dow[1:10] ## [1] &quot;Tue&quot; &quot;Wed&quot; &quot;Thu&quot; &quot;Fri&quot; &quot;Sat&quot; &quot;Sun&quot; &quot;Mon&quot; &quot;Tue&quot; &quot;Wed&quot; &quot;Thu&quot; delay_day &lt;- delay_day %&gt;% mutate( # add a sorting order (Mon, Tue, ..., Sun) and overwrite dow dow = ordered(dow, levels = c(&quot;Mon&quot;, &quot;Tue&quot;, &quot;Wed&quot;, &quot;Thu&quot;, &quot;Fri&quot;, &quot;Sat&quot;, &quot;Sun&quot;)) ) delay_day$dow[1:10] ## [1] Tue Wed Thu Fri Sat Sun Mon Tue Wed Thu ## Levels: Mon &lt; Tue &lt; Wed &lt; Thu &lt; Fri &lt; Sat &lt; Sun delay_day %&gt;% filter(!is.na(dep_delay)) %&gt;% ggplot(aes(x = dow, y = dep_delay, fill = weekend)) + geom_boxplot() It appears that delays are on average longer on Thursdays and Fridays and shorter on Saturdays. This is plausible if more people are traveling on Thursdays and Fridays before the weekend, and less are traveling on Saturdays to enjoy the weekend. Are Saturdays really less busy? Let’s find out. flights_dow &lt;- flights %&gt;% mutate(date = as.Date(paste(year, month, day), format=&quot;%Y %m %d&quot;), dow = ordered(my_dow(date), levels = c(&quot;Mon&quot;, &quot;Tue&quot;, &quot;Wed&quot;, &quot;Thu&quot;, &quot;Fri&quot;, &quot;Sat&quot;, &quot;Sun&quot;)), weekend = dow %in% c(&quot;Sat&quot;, &quot;Sun&quot;) ) # count flight numbers by flights_dow %&gt;% group_by(dow) %&gt;% summarise( nobs = n() ) ## # A tibble: 7 x 2 ## dow nobs ## &lt;ord&gt; &lt;int&gt; ## 1 Mon 50690 ## 2 Tue 50422 ## 3 Wed 50060 ## 4 Thu 50219 ## 5 Fri 50308 ## 6 Sat 38720 ## 7 Sun 46357 # visualize this as a bar plot flights_dow %&gt;% ggplot(aes(x = dow)) + geom_bar() Yes, Saturdays are less busy for the airports in terms of flight numbers. Could we generalize this positive relationship between the number of flights and the average delays, which we find across days of the week? To explore this, we can summarize the data into the average delays by date-hour and see if the busyness of a particular hour of a particular day is correlated with the mean delay. Let’s visualize these data using a scatter plot. delay_day_hr &lt;- flights %&gt;% group_by(year, month, day, hour) %&gt;% # grouping by date-hour summarise( n_obs = n(), dep_delay = mean(dep_delay, na.rm = TRUE) ) %&gt;% mutate(date = as.Date(paste(year, month, day), format=&quot;%Y %m %d&quot;), dow = my_dow(date) ) %&gt;% ungroup() # it&#39;s a good practice to remove group_by() attribute plot_delay &lt;- delay_day_hr %&gt;% filter(!is.na(dep_delay)) %&gt;% ggplot(aes(x = n_obs, y = dep_delay)) + geom_point(alpha = 0.1) # plot of n_obs against the average dep_delay # where each point represents an date-hour average # &quot;alpha = 0.1&quot; controls the degree of transparency of points plot_delay Along the horizontal axis, we can see how the number of flights is distributed across date-hours. Some days are busy, and some hours busier still. It appears that there are two clusters in the number of flights, showing very slow date-hours (e.g., less than 10 flights flying out of New York city per hour) and normal date-hours (e.g., about 50 to 70 flights per hour). We might guess that the delays in the slow hours are caused by bad weather. On the other hand, for normal hours we may wonder if the excess delays are caused by congestion at the airports. To see this, let’s fit a curve that captures the relationships between n_obs and dep_delay. Our hypothesis is that the delay would become longer as the number of flights increases, which would result in an upward-sloped curve. plot_delay + geom_smooth() # geom_smooth() addes a layer of fitted curve(s) ## `geom_smooth()` using method = &#39;gam&#39; We cannot see any clear pattern. How about fitting a curve by day of the week? plot_delay + # additional aes() argument for applying different colors to the day of the week geom_smooth(aes(color = dow), se=FALSE) ## `geom_smooth()` using method = &#39;gam&#39; Surprisingly, the delay does not seem to increase with the flights. There are more delays on Thursdays and Fridays and less delays on Saturdays, but we see no evidence of flight congestion as a cause of delay. Let’s take a closer look at the distribution of the delays. If it is not normally distributed, we may want to apply a transformation. delay_day_hr %&gt;% filter(!is.na(dep_delay)) %&gt;% ggplot(aes(x = dep_delay)) + geom_histogram(color = &quot;white&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. The distribution of the average delays are greatly skewed. To apply a logarithmic transformation, here we have to shift the variable by setting its minimum value zero. # define new column called &quot;dep_delay_shifted&quot; delay_day_hr &lt;- delay_day_hr %&gt;% mutate(dep_delay_shifted = dep_delay - min(dep_delay, na.rm = TRUE) + 1) # check summary stats delay_day_hr %&gt;% select(dep_delay, dep_delay_shifted) %&gt;% with( apply(., 2, summary) ) %&gt;% t() # transpose rows and columns ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## dep_delay -18 1.0543 6.571429 12.98602 15.4414 269 13 ## dep_delay_shifted 1 20.0543 25.571429 31.98602 34.4414 288 13 Tips #3: with(data, ...) function allows for referencing variable names inside the data frame (i.e., “var_name” instead of “data$var_name”). This is very useful when you work with various functions that were created outside the tidyverse syntax, while keeping your codes consistent with the tidyverse syntax. Tips #4: apply(data, num, fun) applies function “fun” for each item in dimension “num” (1 = cows, 2= columns) of the data frame. The data referenced by “.” means all variables in the dataset. Now the transformed distribution; # Under the log of 10 transformation, the distribution looks closer to a normal distribution. delay_day_hr %&gt;% filter(!is.na(dep_delay_shifted)) %&gt;% ggplot(aes(x = dep_delay_shifted)) + scale_x_log10() + geom_histogram(color = &quot;white&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. # # Alternatively, one can apply the natural logarithm to transform a variable. Histogram shows no difference here. # delay_day_hr %&gt;% filter(!is.na(dep_delay_shifted)) %&gt;% # ggplot(aes(x = dep_delay_shifted)) + # scale_x_continuous(trans = &quot;log&quot;) + # geom_histogram(color = &quot;white&quot;) The transformed distribution is much less skewed than the original. Now, let’s plot the relationship between delays and flights again. delay_day_hr %&gt;% filter(!is.na(dep_delay_shifted), dep_delay_shifted &gt; 5) %&gt;% ggplot(aes(x = n_obs, y = dep_delay_shifted)) + scale_y_log10() + # using transformation scale_y_log10() geom_point(alpha = 0.1) + geom_smooth() ## `geom_smooth()` using method = &#39;gam&#39; Again we do not see a pattern of more delays for busier hours. It seems that the airports in New York City manage the fluctuating number of flights without causing congestion. "],
["2-5-more-examples-2.html", "2.5 More examples 2", " 2.5 More examples 2 Let’s keep going with exmaples using flights data. Previously, we find that the congestion at the airports is unlikely the cause of delays. Then, what else may explain the patterns of delays? Recall that earlier we observe that some airlines have longer delays than others for NYC-MSP flights. Are some airlines responsible for the delays? Let’s take a look at the average delays by carrier. stat_carrier &lt;- flights %&gt;% group_by(carrier) %&gt;% summarise(n_obs = n(), dep_delay = mean(dep_delay, na.rm = TRUE), arr_delay = mean(arr_delay, na.rm = TRUE) ) %&gt;% left_join(airlines, by=&quot;carrier&quot;) %&gt;% arrange(desc(n_obs)) stat_carrier %&gt;% kable(digit=2) carrier n_obs dep_delay arr_delay name UA 58665 12.11 3.56 United Air Lines Inc. B6 54635 13.02 9.46 JetBlue Airways EV 54173 19.96 15.80 ExpressJet Airlines Inc. DL 48110 9.26 1.64 Delta Air Lines Inc. AA 32729 8.59 0.36 American Airlines Inc. MQ 26397 10.55 10.77 Envoy Air US 20536 3.78 2.13 US Airways Inc. 9E 18460 16.73 7.38 Endeavor Air Inc. WN 12275 17.71 9.65 Southwest Airlines Co. VX 5162 12.87 1.76 Virgin America FL 3260 18.73 20.12 AirTran Airways Corporation AS 714 5.80 -9.93 Alaska Airlines Inc. F9 685 20.22 21.92 Frontier Airlines Inc. YV 601 19.00 15.56 Mesa Airlines Inc. HA 342 4.90 -6.92 Hawaiian Airlines Inc. OO 32 12.59 11.93 SkyWest Airlines Inc. While we see some differences, the simple average of delays over various routes, days, and hours of flights may not be a good measure for the comparison across carriers. For example, some carriers may serve the routes and hours that tend to have more delays. Also, given that our dataset covers only the flights from New York City, the comparison may not be nationally representative since carriers use different airports around the country as their regional hubs. For our purposes, instead of delays, let’s compare the average air time among carriers, for whihch we can account for flight’s destination and timing. The differences in air time may indicate some efficiency differences. Let’s first check how air time relates to flight distance. flights %&gt;% filter (month == 1, day == 1, !is.na(air_time)) %&gt;% ggplot(aes(x = distance, y = air_time)) + geom_point(alpha = 0.05) + geom_smooth() ## `geom_smooth()` using method = &#39;loess&#39; air_time and distance show a general linear relationship. We can better account for this relationship if we calculate the average air time for each flight destination from New York City. First, consider using a simple linear regression model of air time to account for varying destinations. The idea is to estimate the regression-mean air time for each destination, remove the destination effects from the total variation of air time, and then compare that across carriers. # make a copy of flights data flights2 &lt;- flights flights2 &lt;- flights2 %&gt;% # handle missing data as NA mutate( idx_na1 = is.na(air_time) | is.na(dest) ) res &lt;- flights2 %&gt;% # estimate a linear model and obtain residuals with( lm( air_time ~ 0 + as.factor(dest), subset=!idx_na1) # lm() estimates a linear model. # &quot;y ~ x&quot;&quot; is the formula for regressing y on x. # &quot;y ~ 0 + x&quot; removes the intercept from the model. # as.factor() converts &quot;dest&quot; to a factor (categorical) class # which is used as a set of dummy variables in the regression. ) %&gt;% residuals() summary(res) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -57.574 -6.909 -0.953 0.000 5.862 144.317 # define a variable containing a vector of NA # replace rows with idx_na1 == FALSE flights2$res &lt;- NA flights2$res[!flights2$idx_na1] &lt;- res flights2 %&gt;% group_by(carrier) %&gt;% summarise( mean_res = mean(res, na.rm = TRUE), # mean residual by carrier sd_res = sd(res, na.rm = TRUE) ) %&gt;% left_join(stat_carrier, by=&quot;carrier&quot;) %&gt;% select(carrier, name, n_obs, dep_delay, arr_delay, mean_res, sd_res) %&gt;% arrange(-n_obs) %&gt;% kable(digit=2) carrier name n_obs dep_delay arr_delay mean_res sd_res UA United Air Lines Inc. 58665 12.11 3.56 -0.87 14.59 B6 JetBlue Airways 54635 13.02 9.46 0.28 11.55 EV ExpressJet Airlines Inc. 54173 19.96 15.80 -0.37 8.94 DL Delta Air Lines Inc. 48110 9.26 1.64 -0.20 12.32 AA American Airlines Inc. 32729 8.59 0.36 0.68 13.86 MQ Envoy Air 26397 10.55 10.77 0.45 8.87 US US Airways Inc. 20536 3.78 2.13 -0.42 9.43 9E Endeavor Air Inc. 18460 16.73 7.38 0.84 8.76 WN Southwest Airlines Co. 12275 17.71 9.65 0.16 12.55 VX Virgin America 5162 12.87 1.76 3.26 17.58 FL AirTran Airways Corporation 3260 18.73 20.12 1.16 8.75 AS Alaska Airlines Inc. 714 5.80 -9.93 -2.13 16.17 F9 Frontier Airlines Inc. 685 20.22 21.92 3.12 15.16 YV Mesa Airlines Inc. 601 19.00 15.56 -0.05 7.06 HA Hawaiian Airlines Inc. 342 4.90 -6.92 5.64 20.69 OO SkyWest Airlines Inc. 32 12.59 11.93 1.02 7.26 It appears to make sense that the average air time is longer for low-cost carriers such as Virgin America, Frontier Airlines, and Hawaiian Airlines. The differences across other carriers, on the other hand, appear small relative to standard errors. To get a sense of whether these differences have any statistical significance, let’s use t-test to compare the mean residual between United Airlines and American Airlines. # t-test comparing UA vs AA for the mean air time flights2 %&gt;% with({ idx_UA &lt;- carrier == &quot;UA&quot; idx_AA &lt;- carrier == &quot;AA&quot; t.test(res[idx_UA], res[idx_AA]) }) ## ## Welch Two Sample t-test ## ## data: res[idx_UA] and res[idx_AA] ## t = -15.722, df = 68826, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -1.741133 -1.355142 ## sample estimates: ## mean of x mean of y ## -0.8689523 0.6791852 With a large number of observations, a seemingly small difference in the means often turns out to be statistically significant. Nonetheless, the average difference of about 1.5 minute air time per flight appears very small. In fact, we can do this sort of pair-wise comparison all at once using a regression. Using carrier fixed effects in addition to destination fixed effects, we can directly compare the mean effects across carriers. We will set United Airlines to be a reference of the carrier fixed effects, so that the fixed effect for United Airlines is set to zero (i.e., omitted category), from which the fixed effects of all other airlines are estimated. flights2$carrier &lt;- relevel(factor(flights2$carrier), ref=&quot;UA&quot;) # reference level is United Airlines flights2$carrier %&gt;% table() ## . ## UA 9E AA AS B6 DL EV F9 FL HA MQ OO ## 58665 18460 32729 714 54635 48110 54173 685 3260 342 26397 32 ## US VX WN YV ## 20536 5162 12275 601 flights2 %&gt;% with({ n_carrier &lt;- unique(carrier) %&gt;% length() n_dest &lt;- unique(dest) %&gt;% length() print(paste(&#39;There are&#39;, n_carrier, &#39;distinct carriers and&#39;, n_dest,&#39;distinct destinations in the data.&#39; )) }) ## [1] &quot;There are 16 distinct carriers and 105 distinct destinations in the data.&quot; With 16 carriers and 105 destinations minus 2 reference levels for carriers and destinations, the total of 119 coefficients will be estimated for the fixed effects. f1 &lt;- flights2 %&gt;% with( lm( air_time ~ as.factor(carrier) + as.factor(dest) ) # fixed effects for carriers and destinations ) tidy(f1)[1:20,] # show the first 20 coefficients ## term estimate std.error statistic p.value ## 1 (Intercept) 247.9884874 0.75069658 330.3445016 0.000000e+00 ## 2 as.factor(carrier)9E 1.8015498 0.12723996 14.1586788 1.702649e-45 ## 3 as.factor(carrier)AA 1.9326712 0.09731105 19.8607572 1.002388e-87 ## 4 as.factor(carrier)AS -1.9071536 0.49596319 -3.8453531 1.204017e-04 ## 5 as.factor(carrier)B6 1.1808039 0.08495098 13.8998267 6.535025e-44 ## 6 as.factor(carrier)DL 0.7531812 0.08722600 8.6348244 5.907432e-18 ## 7 as.factor(carrier)EV 0.4174574 0.11044837 3.7796605 1.570702e-04 ## 8 as.factor(carrier)F9 3.8891981 0.48090201 8.0872985 6.120836e-16 ## 9 as.factor(carrier)FL 2.6434074 0.27600661 9.5773336 1.002386e-21 ## 10 as.factor(carrier)HA 11.0125104 0.89821710 12.2604106 1.503557e-34 ## 11 as.factor(carrier)MQ 1.4592669 0.11892133 12.2708590 1.321669e-34 ## 12 as.factor(carrier)OO 1.8091432 2.21222472 0.8177936 4.134757e-01 ## 13 as.factor(carrier)US 0.1319337 0.13826299 0.9542230 3.399715e-01 ## 14 as.factor(carrier)VX 4.5298528 0.18441295 24.5636378 4.086448e-133 ## 15 as.factor(carrier)WN 1.2226161 0.17520980 6.9780125 2.999500e-12 ## 16 as.factor(carrier)YV 0.5167461 0.52737831 0.9798395 3.271661e-01 ## 17 as.factor(dest)ACK -207.1011095 1.04478912 -198.2228803 0.000000e+00 ## 18 as.factor(dest)ALB -216.6188634 0.95130943 -227.7059972 0.000000e+00 ## 19 as.factor(dest)ANC 165.1365126 4.26930687 38.6799351 0.000000e+00 ## 20 as.factor(dest)ATL -136.1282095 0.75641976 -179.9638460 0.000000e+00 # a function to clean up the coefficient table above clean_lm_rlt &lt;- function(f) { # keep only rows for which column &quot;term&quot; contains &quot;carrier&quot; e.g., rows 2 to 16 above rlt &lt;- tidy(f) %&gt;% filter(grepl(&quot;carrier&quot;,term)) # create column named carrier rlt &lt;- rlt %&gt;% mutate(carrier = gsub(&#39;as.factor\\\\(carrier\\\\)&#39;,&#39;&#39;, term)) # drop column term rlt &lt;- rlt %&gt;% select(-term) # add columns of carrier, name, and n_obs from the stat_carrier data frame stat_carrier %&gt;% select(carrier, name, n_obs) %&gt;% left_join(rlt, by=&quot;carrier&quot;) } clean_lm_rlt(f1) %&gt;% kable(digit=2) carrier name n_obs estimate std.error statistic p.value UA United Air Lines Inc. 58665 NA NA NA NA B6 JetBlue Airways 54635 1.18 0.08 13.90 0.00 EV ExpressJet Airlines Inc. 54173 0.42 0.11 3.78 0.00 DL Delta Air Lines Inc. 48110 0.75 0.09 8.63 0.00 AA American Airlines Inc. 32729 1.93 0.10 19.86 0.00 MQ Envoy Air 26397 1.46 0.12 12.27 0.00 US US Airways Inc. 20536 0.13 0.14 0.95 0.34 9E Endeavor Air Inc. 18460 1.80 0.13 14.16 0.00 WN Southwest Airlines Co. 12275 1.22 0.18 6.98 0.00 VX Virgin America 5162 4.53 0.18 24.56 0.00 FL AirTran Airways Corporation 3260 2.64 0.28 9.58 0.00 AS Alaska Airlines Inc. 714 -1.91 0.50 -3.85 0.00 F9 Frontier Airlines Inc. 685 3.89 0.48 8.09 0.00 YV Mesa Airlines Inc. 601 0.52 0.53 0.98 0.33 HA Hawaiian Airlines Inc. 342 11.01 0.90 12.26 0.00 OO SkyWest Airlines Inc. 32 1.81 2.21 0.82 0.41 The “estimate” column shows the mean difference in air time comapred to United Airlines, accounting for the flight destination. The estimate tends to be more precise (i.e., smaller standard errors) for carriers with a larger number of observations. This time, we find that Virgin America, Air Tran, Frontier Airlines, and Hawaiian Airlines tend to show particularly longer air times than United Airlines. Next, let’s take a step further to account for flight timing as well. We can do this by adding fixed effects for flight dates and hours. flights2 &lt;- flights2 %&gt;% mutate( date_id = month*100 + day ) flights2$date_id %&gt;% unique() %&gt;% length() ## [1] 365 f2 &lt;- flights2 %&gt;% with( lm( air_time ~ as.factor(carrier) + as.factor(dest) + + as.factor(date_id) + as.factor(hour) ) ) lm_rlt2 &lt;- clean_lm_rlt(f2) lm_rlt2 %&gt;% kable(digit=2) carrier name n_obs estimate std.error statistic p.value UA United Air Lines Inc. 58665 NA NA NA NA B6 JetBlue Airways 54635 1.60 0.07 22.50 0.00 EV ExpressJet Airlines Inc. 54173 0.61 0.09 6.67 0.00 DL Delta Air Lines Inc. 48110 0.95 0.07 13.03 0.00 AA American Airlines Inc. 32729 1.84 0.08 22.81 0.00 MQ Envoy Air 26397 1.45 0.10 14.70 0.00 US US Airways Inc. 20536 0.17 0.11 1.51 0.13 9E Endeavor Air Inc. 18460 1.57 0.11 14.72 0.00 WN Southwest Airlines Co. 12275 1.14 0.15 7.82 0.00 VX Virgin America 5162 4.85 0.15 31.57 0.00 FL AirTran Airways Corporation 3260 2.19 0.23 9.58 0.00 AS Alaska Airlines Inc. 714 -2.55 0.41 -6.21 0.00 F9 Frontier Airlines Inc. 685 3.31 0.40 8.29 0.00 YV Mesa Airlines Inc. 601 0.32 0.44 0.73 0.46 HA Hawaiian Airlines Inc. 342 11.79 0.75 15.80 0.00 OO SkyWest Airlines Inc. 32 7.63 1.83 4.17 0.00 lm_rlt2 %&gt;% filter(carrier!=&#39;UA&#39;) %&gt;% ggplot(aes(x = carrier, y = estimate)) + geom_col() + labs(title = &quot;Mean Air Time Compared to United Airlines&quot;) The results are similar to the previous linear mode except that this time SkyWest Airlines shows much longer air time. Our final model is a check for the robustness of the above results. Let’s replace the date and hour fixed effects in the previous model with date-hour fixed effects (i.e., the interaction between date and hour). We could add such fixed effects using time_hour variable defined above. However, that would mean adding nearly 7,000 dummy variables to our linear regression, which is computationally intensive. To work around this issue, we approximate this estimation by pre-processing the dependent variable. Specifically, we calculate the average air time for each combination of time_hour and dest and define a new dependent variable by subtracting this average value from the original air time variable (i.e., the new variable is centered at zero-mean for each combination of time_hour and dest). Then, we estimate a linear model with carrier and destination fixed effects. ## Adding time_hour fixed effects is computationally intensive # f1 &lt;- flights %&gt;% # with( # lm( air_time ~ as.factor(carrier) + as.factor(dest) + as.factor(time_hour)) # ) unique(flights2$time_hour) %&gt;% length() # 6,936 unique time_hour ## [1] 6936 flights2 &lt;- flights2 %&gt;% group_by(dest, time_hour) %&gt;% mutate( air_time_centered = air_time - mean(air_time, na.rm=TRUE) ) f3 &lt;- flights2 %&gt;% with( lm( air_time_centered ~ as.factor(carrier) + as.factor(dest) ) ) lm_rlt3 &lt;- clean_lm_rlt(f3) lm_rlt3 %&gt;% kable(digit=2) # Note: standard errors, t-stat, and p-val are incorrect carrier name n_obs estimate std.error statistic p.value UA United Air Lines Inc. 58665 NA NA NA NA B6 JetBlue Airways 54635 0.82 0.03 32.24 0.00 EV ExpressJet Airlines Inc. 54173 0.88 0.03 26.50 0.00 DL Delta Air Lines Inc. 48110 0.52 0.03 19.85 0.00 AA American Airlines Inc. 32729 1.20 0.03 41.06 0.00 MQ Envoy Air 26397 1.00 0.04 27.84 0.00 US US Airways Inc. 20536 -0.09 0.04 -2.21 0.03 9E Endeavor Air Inc. 18460 1.27 0.04 33.07 0.00 WN Southwest Airlines Co. 12275 1.30 0.05 24.70 0.00 VX Virgin America 5162 3.47 0.06 62.59 0.00 FL AirTran Airways Corporation 3260 1.78 0.08 21.48 0.00 AS Alaska Airlines Inc. 714 -2.86 0.15 -19.15 0.00 F9 Frontier Airlines Inc. 685 1.99 0.14 13.73 0.00 YV Mesa Airlines Inc. 601 0.78 0.16 4.89 0.00 HA Hawaiian Airlines Inc. 342 1.34 0.27 4.96 0.00 OO SkyWest Airlines Inc. 32 3.50 0.67 5.26 0.00 lm_rlt3 %&gt;% filter(carrier!=&#39;UA&#39;) %&gt;% ggplot(aes(x = carrier, y = estimate)) + geom_col() + labs(title = &quot;Mean Air Time Compared to United Airlines: Robustness Check&quot;) The point estimates should be approximately what we would obtain if we regress air_time on the fixed effects of carrier, dest, and time_hour. However, the standard errors are not correctly displayed in the table because the centered variable has a smaller total variation compared to the original air_time variable. (Correct standard errors can be obtained, for example, through a bootstrapping technique.) Overall, we see again a tendency that lower-cost carriers like Sky West Airlines, Virgin America, Frontier Airlines, and Air Tran show particularly longer air time than United Airlines. Jet Blue Airways, another low-cost carrier, shows a less obvious difference from United Airlines, possibly suggesting that their operation focused on the East Cost is efficient for the flights departing from New York City. Hawaiian Airlines and Alaskan Airlines appear to be somewhat different from other carriers perhaps because they are more specialized in particular flight destinations compared to their rivals. In particular, the flights to Hawaii may have distinct delay patterns that are concentrated on peak vacation seasons. "],
["2-6-reflections.html", "2.6 Reflections", " 2.6 Reflections In this introduction, we reviewed the tools of deplyr and ggplot2 packages as a starting point for data analyses and visualization in R. This new generation of tools utilizing the tidyverse syntax is particularly suitable for data exploration. It allows for simple conversions of our inquiry into sequential coding of data manipulation and analysis via pipe operator %&gt;%. Also, manipulating data and creating plots are streamlined via the systematic framework over three primary components data, aes, and geom. "],
["3-essentials.html", "3 Essentials", " 3 Essentials This section provides an overview of the essential concepts for manipulating data and programming in R. We cover most of the materials in The Art of R Programming by Norman Matloff. 3.1 Cheetsheets 3.2 Data types 3.3 Programming 3.4 Housekeeping "],
["3-1-cheatsheets.html", "3.1 Cheatsheets", " 3.1 Cheatsheets Cheatsheets are useful for glancing at various functions. Base R RStudio IDE dplyr ggplot2 Cheatseets are updated, and new ones are added. You can check here. You might find the following useful or intriguing: syntax comparison for data analysis tools string manipulation regular expressions dates and time data imports reports via Rmarkdown online application via Shiny machine-learning tool overview machine-learning via Caret machine-learning via Keras Leaflet maps "],
["3-2-datatypes.html", "3.2 Data types", " 3.2 Data types 3.2.1 Atomic In most cases, each atomic element has a type (mode) of numeric: number logical: TRUE or FALSE (T or F for shortcuts) character: character string factor: a level of categorical variable Other types include date and nonexistent NULL. The factor is also a class of its own, meaning that many R functions apply operations that are specific to the factor class. # assess objects 123, &quot;abc&quot;, and TRUE for their types str(123) # str() returns the structure ## num 123 str(&quot;abc&quot;) ## chr &quot;abc&quot; str(TRUE) ## logi TRUE c(is.numeric(123), is.numeric(&quot;abc&quot;), is.numeric(TRUE)) ## [1] TRUE FALSE FALSE c(is.logical(123), is.logical(&quot;abc&quot;), is.logical(TRUE)) ## [1] FALSE FALSE TRUE c(is.character(123), is.character(&quot;abc&quot;), is.character(TRUE)) ## [1] FALSE TRUE FALSE # &quot;&lt;-&quot; means an assignment from right to left factor1 &lt;- as.factor(c(1,2,3)) # Looks like numeric but not factor1 ## [1] 1 2 3 ## Levels: 1 2 3 factor2 &lt;- as.factor(c(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;)) # Looks like characters but not factor2 ## [1] a b c ## Levels: a b c factor3 &lt;- as.factor(c(TRUE,FALSE,T)) # Looks like logicals but not factor3 ## [1] TRUE FALSE TRUE ## Levels: FALSE TRUE c(is.factor(factor1[1]), is.factor(factor2[1]), is.factor(factor3[1])) ## [1] TRUE TRUE TRUE # Extract the first element (factor1[1] etc.) factor1[1] ## [1] 1 ## Levels: 1 2 3 factor2[2] ## [1] b ## Levels: a b c factor3[3] ## [1] TRUE ## Levels: FALSE TRUE NULL has zero-length. Also, empty numeric, logical, and character objects have zero-length. length(NULL) ## [1] 0 length(numeric(0)) # numeric(N) returns a vector of N zeros ## [1] 0 length(logical(0)) # logical(N) returns a vector of N FALSE objects ## [1] 0 length(character(0)) # character(N) returns a vector of N &quot;&quot; objects ## [1] 0 Each vector has a type of numeric, logical, character, or factor. Each matrix has a type of numeric, logical, or character. A data frame can contain mixed types across columns where each column (e.g., a variable) has a type of numeric, logical, character or factor. vector1 &lt;- c(1, NA, 2, 3) # read as numeric vector1 ## [1] 1 NA 2 3 vector2 &lt;- c(TRUE, FALSE, T, F) # read as logical vector2 ## [1] TRUE FALSE TRUE FALSE vector3 &lt;- c(1, NA, &quot;abc&quot;, TRUE, &quot;TRUE&quot;) # read as character vector3 ## [1] &quot;1&quot; NA &quot;abc&quot; &quot;TRUE&quot; &quot;TRUE&quot; vector4 &lt;- as.factor(c(1, NA, &quot;abc&quot;, TRUE, &quot;TRUE&quot;)) # read as factor vector4 ## [1] 1 &lt;NA&gt; abc TRUE TRUE ## Levels: 1 abc TRUE matrix1 &lt;- matrix(c(1:6), nrow = 3) # read as numeric matrix1 ## [,1] [,2] ## [1,] 1 4 ## [2,] 2 5 ## [3,] 3 6 matrix2 &lt;- matrix(c(TRUE,FALSE,rep(T,3),F), nrow = 3) # read as logical matrix2 ## [,1] [,2] ## [1,] TRUE TRUE ## [2,] FALSE TRUE ## [3,] TRUE FALSE matrix3 &lt;- matrix(c(1,2,3,&quot;a&quot;,&quot;b&quot;,&quot;abc&quot;), nrow = 3) # read as character matrix3 ## [,1] [,2] ## [1,] &quot;1&quot; &quot;a&quot; ## [2,] &quot;2&quot; &quot;b&quot; ## [3,] &quot;3&quot; &quot;abc&quot; df1 &lt;- data.frame( num = c(1,2,3), # read as numeric fac1 = c(&quot;a&quot;,&quot;b&quot;,&quot;abc&quot;), # read as factor logi = c(TRUE, FALSE, T), # read as logical fac2 = c(1,&quot;a&quot;,TRUE) # read as factor ) df1 ## num fac1 logi fac2 ## 1 1 a TRUE 1 ## 2 2 b FALSE a ## 3 3 abc TRUE TRUE df1$num # &quot;$&quot; symbol is used to extract a column ## [1] 1 2 3 df1$fac1 # character type is converted into a factor ## [1] a b abc ## Levels: a abc b df1$logi ## [1] TRUE FALSE TRUE df1$fac2 # mixed types within a column is converted into a factor ## [1] 1 a TRUE ## Levels: 1 a TRUE # additional argument &quot;stringsAsFactors = FALSE&quot; preserves character types. df2 &lt;- data.frame( num = c(1,2,3), # read as numeric char = c(&quot;a&quot;,&quot;b&quot;,&quot;abc&quot;), # read as character logi = c(TRUE, FALSE, T), # read as logical fac2 = as.factor(c(1,&quot;a&quot;,TRUE)), # read as factor stringsAsFactors = FALSE ) df2 ## num char logi fac2 ## 1 1 a TRUE 1 ## 2 2 b FALSE a ## 3 3 abc TRUE TRUE df2$num ## [1] 1 2 3 df2$char ## [1] &quot;a&quot; &quot;b&quot; &quot;abc&quot; df2$logi ## [1] TRUE FALSE TRUE df2$fac2 ## [1] 1 a TRUE ## Levels: 1 a TRUE 3.2.2 Factor A factor object is defined with a set of categorical levels, which may be labeled. The levels are either ordered (defined by ordered()) or unordered (defined by factor()). Ordered factor objects are treated in the specific order by certain statistical and graphical procedures. # We will convert the columns of df into factors df &lt;- data.frame( fac1 = c(0,1,1,4,4,2,2,3), fac2 = c(1,2,3,1,1,2,2,3), fac3 = c(4,2,3,4,4,2,2,3) ) # convert fac1 to ordered factors df$fac1 &lt;- ordered(df$fac1, levels = c(0,4,3,2,1) # defines the order ) df$fac1 ## [1] 0 1 1 4 4 2 2 3 ## Levels: 0 &lt; 4 &lt; 3 &lt; 2 &lt; 1 summary(df$fac1) # gives the table of counts for each level ## 0 4 3 2 1 ## 1 2 1 2 2 # convert fac2 to unordered factors with labels df$fac2 &lt;- factor(df$fac2, levels = c(1,2,3), # no particular order # attach labels to factors: 1=red, 2=blue, 3=green labels = c(&quot;red&quot;, &quot;blue&quot;, &quot;green&quot;) ) df$fac2 ## [1] red blue green red red blue blue green ## Levels: red blue green summary(df$fac2) ## red blue green ## 3 3 2 # convert fac3 to ordered factors with labels df$fac3 &lt;- ordered(df$fac3, levels = c(2,3,4), # attach labels to factors: 2=Low, 3=Medium, 4=High labels = c(&quot;Low&quot;, &quot;Medium&quot;, &quot;High&quot;) ) df$fac3 ## [1] High Low Medium High High Low Low Medium ## Levels: Low &lt; Medium &lt; High summary(df$fac3) ## Low Medium High ## 3 2 3 3.2.3 Matrix matrix() defines a matrix from a vector. The default is to arrange the vector by column (byrow = FALSE). # byrow = FALSE (the default) matrix(data = c(1:6), nrow = 2, ncol = 3, byrow = FALSE, dimnames = NULL) ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 2 4 6 # byrow = TRUE matrix(data = c(1:6), nrow = 2, ncol = 3, byrow = TRUE, dimnames = NULL) ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 4 5 6 # give row and column names to a matrix mat1 &lt;- matrix(data = c(1:6), nrow = 2, ncol = 3, byrow = FALSE, dimnames = list(c(&quot;r1&quot;,&quot;r2&quot;), c(&quot;c1&quot;,&quot;c2&quot;,&quot;c3&quot;))) mat1 ## c1 c2 c3 ## r1 1 3 5 ## r2 2 4 6 dim(mat1) # dimension: row by column ## [1] 2 3 colnames(mat1) ## [1] &quot;c1&quot; &quot;c2&quot; &quot;c3&quot; rownames(mat1) ## [1] &quot;r1&quot; &quot;r2&quot; colnames(mat1) &lt;- c(&quot;v1&quot;,&quot;v2&quot;,&quot;v3&quot;) # change column names by assignment &quot;&lt;-&quot; mat1 ## v1 v2 v3 ## r1 1 3 5 ## r2 2 4 6 # R makes a guess when only nrow or ncol is supplied matrix(data = c(1:6), nrow = 2) ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 2 4 6 matrix(data = c(1:6), ncol = 3) ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 2 4 6 # combine matrices by column via &quot;cbind()&quot; or by row via &quot;rbind()&quot; cbind(mat1,mat1) ## v1 v2 v3 v1 v2 v3 ## r1 1 3 5 1 3 5 ## r2 2 4 6 2 4 6 rbind(mat1,mat1) ## v1 v2 v3 ## r1 1 3 5 ## r2 2 4 6 ## r1 1 3 5 ## r2 2 4 6 There are recycling rules (which does/controls what?) in R. # the vector shorter than the length of all elements of a matrix matrix(data = c(1:4), nrow = 2, ncol= 3) ## Warning in matrix(data = c(1:4), nrow = 2, ncol = 3): data length [4] is ## not a sub-multiple or multiple of the number of columns [3] ## [,1] [,2] [,3] ## [1,] 1 3 1 ## [2,] 2 4 2 # R treats a scaler as a vector of length that conforms cbind() or rbind() cbind(mat1, colA = 1) ## v1 v2 v3 colA ## r1 1 3 5 1 ## r2 2 4 6 1 rbind(mat1, rowA= 1, rowB= 2, rowC= 3) ## v1 v2 v3 ## r1 1 3 5 ## r2 2 4 6 ## rowA 1 1 1 ## rowB 2 2 2 ## rowC 3 3 3 To replace elements of a matrix, we can use assignment operator &lt;-. mat1[1,1] &lt;- 10 mat1 ## v1 v2 v3 ## r1 10 3 5 ## r2 2 4 6 mat1[,2] &lt;- c(7,8) mat1 ## v1 v2 v3 ## r1 10 7 5 ## r2 2 8 6 mat1[,1] &lt;- 0 # recycling rule mat1 ## v1 v2 v3 ## r1 0 7 5 ## r2 0 8 6 Matrix allows for easy extraction for rows and columns separated by comma. mat1 ## v1 v2 v3 ## r1 0 7 5 ## r2 0 8 6 mat1[1, ] # row = 1 and all columns ## v1 v2 v3 ## 0 7 5 mat1[, 1] # all rows and col = 1 ## r1 r2 ## 0 0 mat1[c(TRUE,FALSE),] # by a logical vector ## v1 v2 v3 ## 0 7 5 mat1[, c(TRUE,FALSE)] ## v1 v3 ## r1 0 5 ## r2 0 6 mat1[2,3] # row = 2 and col = 3 ## [1] 6 mat1[1:2, 2:3] # row = 1:2 and col = 2:3 ## v2 v3 ## r1 7 5 ## r2 8 6 mat1[1:2, 2:3][2,2] # subset of a subset ## [1] 6 mat1[, 1][2] # vector extraction is done with one-dimensional index ## r2 ## 0 Important: when a single row or column is extracted, it gets converted to a vector with no dimension. mat1[1,] ## v1 v2 v3 ## 0 7 5 is.matrix(mat1[1, ]) ## [1] FALSE dim(mat1[1,]) ## NULL length(mat1[1, ]) ## [1] 3 # to keep a row or column vector structure, use drop = FALSE mat1[1,, drop = FALSE] ## v1 v2 v3 ## r1 0 7 5 is.matrix(mat1[1,,drop = FALSE]) ## [1] TRUE dim(mat1[1,,drop = FALSE]) ## [1] 1 3 length(mat1[1,,drop = FALSE]) ## [1] 3 mat1[,1, drop = FALSE] ## v1 ## r1 0 ## r2 0 is.matrix(mat1[,1,drop = FALSE]) ## [1] TRUE dim(mat1[,1,drop = FALSE]) ## [1] 2 1 length(mat1[,1,drop = FALSE]) ## [1] 2 Another way of extraction from a matrix is to use row or column names. mat1[,&#39;v1&#39;] ## r1 r2 ## 0 0 mat1[,c(&#39;v1&#39;,&#39;v3&#39;)] ## v1 v3 ## r1 0 5 ## r2 0 6 mat1[&#39;r2&#39;,,drop= FALSE] ## v1 v2 v3 ## r2 0 8 6 apply() applies a function for a specified margin (dimension index number) of the matrix. mat1 ## v1 v2 v3 ## r1 0 7 5 ## r2 0 8 6 apply(mat1,1,mean) # dimension 1 (across rows) ## r1 r2 ## 4.000000 4.666667 apply(mat1,2,mean) # dimension 2 (across columns) ## v1 v2 v3 ## 0.0 7.5 5.5 # one can write a custom function inside apply(). (called annonymous function) # Its argument corresponds to the row or column vector passed by apply(). apply(mat1,2, function(x) sum(x)/length(x) ) # x is the internal vector name ## v1 v2 v3 ## 0.0 7.5 5.5 ans1 &lt;- apply(mat1,2, function(x) { avg = mean(x) sd = sd(x) # return the results as a list list(Avg = avg, Sd = sd) } ) unlist(ans1[[2]]) # results for the second column ## Avg Sd ## 7.5000000 0.7071068 unlist(ans1[[3]]) # results for the third column ## Avg Sd ## 5.5000000 0.7071068 Arrays are a generalization of matrices and can have more than 2 dimensions. array(c(1:18), c(2,3,3)) # dimension 2 by 2 by 3 ## , , 1 ## ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 2 4 6 ## ## , , 2 ## ## [,1] [,2] [,3] ## [1,] 7 9 11 ## [2,] 8 10 12 ## ## , , 3 ## ## [,1] [,2] [,3] ## [1,] 13 15 17 ## [2,] 14 16 18 array(c(1:9), c(2,3,3)) # R recycles the vector ## , , 1 ## ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 2 4 6 ## ## , , 2 ## ## [,1] [,2] [,3] ## [1,] 7 9 2 ## [2,] 8 1 3 ## ## , , 3 ## ## [,1] [,2] [,3] ## [1,] 4 6 8 ## [2,] 5 7 9 3.2.4 Data Frame A data frame is similar to a matrix, but it accepts multiple types (modes) of variables across columns (e.g., a dataset in typical data analysis programs like SAS, SPSS, Stata etc.). In some cases matrices and data frames may be treated interchangeably, but generally they need to be distinguished. Data manipulation functions are often written for data frames, while some base R functions are written for matrices. mymat1 &lt;- matrix(data = c(1:6), nrow = 2, ncol = 3, dimnames = list(c(&quot;r1&quot;,&quot;r2&quot;), c(&quot;c1&quot;,&quot;c2&quot;,&quot;c3&quot;))) mymat1 ## c1 c2 c3 ## r1 1 3 5 ## r2 2 4 6 class(mymat1) ## [1] &quot;matrix&quot; colnames(mymat1) ## [1] &quot;c1&quot; &quot;c2&quot; &quot;c3&quot; names(mymat1) ## NULL mydf1 &lt;- data.frame( mymat1, num = c(1,2), fac1 = c(&quot;a&quot;,&quot;abc&quot;), logi = c(TRUE, FALSE), fac2 = c(1,&quot;a&quot;) ) mydf1 ## c1 c2 c3 num fac1 logi fac2 ## r1 1 3 5 1 a TRUE 1 ## r2 2 4 6 2 abc FALSE a class(mydf1) ## [1] &quot;data.frame&quot; colnames(mydf1) ## [1] &quot;c1&quot; &quot;c2&quot; &quot;c3&quot; &quot;num&quot; &quot;fac1&quot; &quot;logi&quot; &quot;fac2&quot; names(mydf1) # colnames and names are the same ## [1] &quot;c1&quot; &quot;c2&quot; &quot;c3&quot; &quot;num&quot; &quot;fac1&quot; &quot;logi&quot; &quot;fac2&quot; Extracting elements from a data frame is similar to extracting from a matrix, but there are a few additional methods. mydf1[1,] # row = 1 and all columns ## c1 c2 c3 num fac1 logi fac2 ## r1 1 3 5 1 a TRUE 1 mydf1[,1] # all rows and col = 1 ## [1] 1 2 # data frame preserves dimension while extracting a row but not a column dim(mydf1[1,]) ## [1] 1 7 dim(mydf1[,1]) ## NULL dim(mydf1[,1,drop=FALSE]) # use drop = FALSE to keep a column vector ## [1] 2 1 mydf1[,1,drop=FALSE] ## c1 ## r1 1 ## r2 2 mydf1[, c(&#39;c1&#39;,&#39;num&#39;,&#39;logi&#39;)] ## c1 num logi ## r1 1 1 TRUE ## r2 2 2 FALSE class(mydf1[, c(&#39;c1&#39;,&#39;num&#39;,&#39;logi&#39;)]) ## [1] &quot;data.frame&quot; # extraction by column name with &quot;$&quot; symbol: df$varname mydf1$c1 ## [1] 1 2 dim(mydf1$c1) ## NULL # one can use quote &#39; &#39; or &quot; &quot; as well mydf1$&#39;c1&#39; ## [1] 1 2 # similarly, extraction by column name with [[ ]]: df[[&#39;varname&#39;]] mydf1[[&#39;c1&#39;]] ## [1] 1 2 dim(mydf1[[&#39;c1&#39;]]) ## NULL # or by index mydf1[[1]] ## [1] 1 2 # [[ ]] method is useful when passing a variable name as a string set_to_na &lt;- function(df, var) { df[[var]] &lt;- NA df } mydf1 ## c1 c2 c3 num fac1 logi fac2 ## r1 1 3 5 1 a TRUE 1 ## r2 2 4 6 2 abc FALSE a mydf2 &lt;- set_to_na(mydf1, &quot;c2&quot;) mydf2 ## c1 c2 c3 num fac1 logi fac2 ## r1 1 NA 5 1 a TRUE 1 ## r2 2 NA 6 2 abc FALSE a # add a variable mydf1$newvar &lt;- c(4, 4) mydf1 ## c1 c2 c3 num fac1 logi fac2 newvar ## r1 1 3 5 1 a TRUE 1 4 ## r2 2 4 6 2 abc FALSE a 4 mydf1$newvar2 &lt;- mydf1$c2 + mydf1$c3 mydf1 ## c1 c2 c3 num fac1 logi fac2 newvar newvar2 ## r1 1 3 5 1 a TRUE 1 4 8 ## r2 2 4 6 2 abc FALSE a 4 10 apply() may not work well with data frames since data frames are not exactly matrices. We can use simplified apply sapply() or list apply lapply() instead. mydf1 ## c1 c2 c3 num fac1 logi fac2 newvar newvar2 ## r1 1 3 5 1 a TRUE 1 4 8 ## r2 2 4 6 2 abc FALSE a 4 10 # sapply() idx_num &lt;- sapply(mydf1, is.numeric) idx_num ## c1 c2 c3 num fac1 logi fac2 newvar newvar2 ## TRUE TRUE TRUE TRUE FALSE FALSE FALSE TRUE TRUE apply(mydf1[,idx_num], 2, mean) ## c1 c2 c3 num newvar newvar2 ## 1.5 3.5 5.5 1.5 4.0 9.0 sapply(mydf1[,idx_num], mean) ## c1 c2 c3 num newvar newvar2 ## 1.5 3.5 5.5 1.5 4.0 9.0 # lapply() idx_num2 &lt;- unlist(lapply(mydf1, is.numeric)) idx_num2 ## c1 c2 c3 num fac1 logi fac2 newvar newvar2 ## TRUE TRUE TRUE TRUE FALSE FALSE FALSE TRUE TRUE unlist(lapply(mydf1[,idx_num2], mean)) ## c1 c2 c3 num newvar newvar2 ## 1.5 3.5 5.5 1.5 4.0 9.0 3.2.5 List A list is an ordered collection of (possibly unrelated) objects. The objects in a list are referenced by [[1]], [[2]], …, or [[‘var1’]], [[‘var2’]], … etc. mylist1 &lt;- list(v1 = c(1,2,3), v2 = c(&quot;a&quot;,&quot;b&quot;), v3 = factor(c(&quot;blue&quot;,&quot;red&quot;,&quot;orange&quot;,&quot;yellow&quot;)), v4 = data.frame( u1 = c(1:3), u2 = c(&quot;p&quot;,&quot;q&quot;,&quot;r&quot;)) ) mylist1 ## $v1 ## [1] 1 2 3 ## ## $v2 ## [1] &quot;a&quot; &quot;b&quot; ## ## $v3 ## [1] blue red orange yellow ## Levels: blue orange red yellow ## ## $v4 ## u1 u2 ## 1 1 p ## 2 2 q ## 3 3 r # extraction mylist1[[1]] ## [1] 1 2 3 mylist1[[&quot;v2&quot;]] ## [1] &quot;a&quot; &quot;b&quot; mylist1$v3 ## [1] blue red orange yellow ## Levels: blue orange red yellow mylist1$v4$u2 ## [1] p q r ## Levels: p q r # assignment mylist1$v5 &lt;- c(&quot;a&quot;,NA) mylist1$v5 ## [1] &quot;a&quot; NA # a list can be nested mylist1$v6 &lt;- list(y1 = c(2,9), y2 = c(0,0,0,1)) mylist1$v6 ## $y1 ## [1] 2 9 ## ## $y2 ## [1] 0 0 0 1 lapply() is very versatile since the items in a list can be completely unrelated. unlist(lapply(mylist1, class)) ## v1 v2 v3 v4 v5 ## &quot;numeric&quot; &quot;character&quot; &quot;factor&quot; &quot;data.frame&quot; &quot;character&quot; ## v6 ## &quot;list&quot; unlist(lapply(mylist1, attributes)) # some variables have attributes ## v3.levels1 v3.levels2 v3.levels3 v3.levels4 v3.class ## &quot;blue&quot; &quot;orange&quot; &quot;red&quot; &quot;yellow&quot; &quot;factor&quot; ## v4.names1 v4.names2 v4.row.names1 v4.row.names2 v4.row.names3 ## &quot;u1&quot; &quot;u2&quot; &quot;1&quot; &quot;2&quot; &quot;3&quot; ## v4.class v6.names1 v6.names2 ## &quot;data.frame&quot; &quot;y1&quot; &quot;y2&quot; lapply(mylist1, function(x) { if (is.numeric(x)) return(summary(x)) if (is.character(x)) return(x) if (is.factor(x)) return(table(x)) if (is.data.frame(x)) return(head(x)) if (is.list(x)) return(unlist(lapply(x,class))) } ) ## $v1 ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.0 1.5 2.0 2.0 2.5 3.0 ## ## $v2 ## [1] &quot;a&quot; &quot;b&quot; ## ## $v3 ## x ## blue orange red yellow ## 1 1 1 1 ## ## $v4 ## u1 u2 ## 1 1 p ## 2 2 q ## 3 3 r ## ## $v5 ## [1] &quot;a&quot; NA ## ## $v6 ## y1 y2 ## &quot;numeric&quot; &quot;numeric&quot; "],
["3-3-programming.html", "3.3 Programming", " 3.3 Programming 3.3.1 Operator Table 3.1: Basic R Operators Operation Description x + y Addition x - y Subtraction x * y Multiplication x / y Division x ^ y Exponentiation x %% y Modular arithmatic x %/% y Integer division x == y Test for equality x &lt;= y Test for less than or equal to x &gt;= y Test for greater than or equal to x &amp;&amp; y Boolean AND for scalars x || y Boolean OR for scalers x &amp; y Boolean AND for vectors x | y Boolean OR for vectors !x Boolean negation source: (Matloff 2011) 3.3.2 If else if (1 &gt; 0) { print(&quot;result: if&quot;) } else { print(&quot;result: else&quot;) } ## [1] &quot;result: if&quot; # {} brackets can be used to combine multiple expressions # They can be skipped for a single-expression if-else statement. if (1 &gt; 2) print(&quot;result: if&quot;) else print(&quot;result: else&quot;) ## [1] &quot;result: else&quot; ifelse(c(1,2,3) &gt; 2, 1, -1) # return 1 if TRUE and -1 if else ## [1] -1 -1 1 Sys.time() ## [1] &quot;2018-06-26 17:07:55 CDT&quot; time &lt;- Sys.time() hour &lt;- as.integer(substr(time, 12,13)) # sequential if-else statements if (hour &gt; 8 &amp; hour &lt; 12) { print(&quot;morning&quot;) } else if (hour &lt; 18) { print(&quot;afternoon&quot;) } else { print(&quot;private time&quot;) } ## [1] &quot;afternoon&quot; 3.3.3 Loop for (i in 1:3) { print(i) } ## [1] 1 ## [1] 2 ## [1] 3 for (i in c(1,3,5)) print(i) ## [1] 1 ## [1] 3 ## [1] 5 i &lt;- 1 while (i &lt; 5) { print(i) i &lt;- i + 1 } ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 3.3.4 Function We can avoid repeating ourselves with writing similar lines of codes if we turn them into a function. Functions contain a series of tasks that can be applied to varying objects such as different vectors, matrices, characters, data frames, lists, and functions. A function consists of input arguments, tasks (R expressions), and output as an object (e.g. a vector, matrix, character, data frame, list, or function etc.). It can be named or remain anonymous (typically used inside a function like lapply()). ## name_to_be_assigned &lt;- function(input args) { ## tasks ## } The output of the function, aside from those that are printed, saved, or exported, is the very last task (expression). If variable result is created inside the function, having result at the very end will return this item as an output. When multiple objects are created, it is often convenient to return those as a list. Use return() to return a specific item in the middle of the function and skip the rest of the evaluations. For checking errors and halting evaluations, use stop() or stopifnot(). myfun1 &lt;- function() print(&quot;Hello world&quot;) # just returning &quot;Hello world&quot; myfun1() ## [1] &quot;Hello world&quot; myfun2 &lt;- function(var) var^2 myfun2(var = 3) ## [1] 9 myfun3 &lt;- function(var = 1) ifelse(var&gt;0, log(var), var) myfun3() # default argument is var = 1 ## [1] 0 myfun3(2) ## [1] 0.6931472 myfun4 &lt;- function(x1, x2) { if (!is.numeric(x1) | !is.numeric(x2)) stop(&#39;demo of error: numeric args needed&#39;) x1*x2 } # try(myfun4(1, &quot;a&quot;)) myfun4(4, 3) ## [1] 12 3.3.5 Environment A function, formally known as a closure, consists of its arguments (called formals), a body, and an environment. An environment is a collection of existing R objects at the time when the function is created. Functions created at the top level have .GlobalEnv as their environments (R may refer to it as R_GlobalEnv as well). environment() # .GlobalEnv (or R_GlobalEnv) is the top-level environment ## &lt;environment: R_GlobalEnv&gt; f1 &lt;- function(arg1) environment() formals(f1) # arguments of f1() ## $arg1 body(f1) # body of f1() ## environment() environment(f1) # environment of f1(), which is .GlobalEnv ## &lt;environment: R_GlobalEnv&gt; f1() # inside f1 has its own enviornment ## &lt;environment: 0x7fcfc4851118&gt; A function can access to the objects in its environment (i.e., global to the function) and those defined inside (i.e., local to the function) and generally cannot overwrite the global objects. It allows for using common names such as “x1”, “var1” etc. defined inside functions, but those objects are only accessible within the function. a &lt;- NULL # object named &quot;a&quot; in .GlobalEnv f2 &lt;- function() { a &lt;- 1 # object named &quot;a&quot; in an environment inside f2 print(a) environment() } f2() # one instance creating an environment ## [1] 1 ## &lt;environment: 0x7fcfc49a2260&gt; f2() # another instance creating another environment ## [1] 1 ## &lt;environment: 0x7fcfc3e0dc70&gt; a # stays NULL ## NULL ls() # ls() shows all objects of an environment (here .GlobablEnv) ## [1] &quot;a&quot; &quot;ans1&quot; &quot;df&quot; &quot;df1&quot; ## [5] &quot;df2&quot; &quot;f1&quot; &quot;f2&quot; &quot;factor1&quot; ## [9] &quot;factor2&quot; &quot;factor3&quot; &quot;hour&quot; &quot;i&quot; ## [13] &quot;idx_num&quot; &quot;idx_num2&quot; &quot;mat1&quot; &quot;matrix1&quot; ## [17] &quot;matrix2&quot; &quot;matrix3&quot; &quot;mydf1&quot; &quot;mydf2&quot; ## [21] &quot;myfun1&quot; &quot;myfun2&quot; &quot;myfun3&quot; &quot;myfun4&quot; ## [25] &quot;mylist1&quot; &quot;mymat1&quot; &quot;set_to_na&quot; &quot;tbl_operator&quot; ## [29] &quot;time&quot; &quot;vector1&quot; &quot;vector2&quot; &quot;vector3&quot; ## [33] &quot;vector4&quot; rm(list = ls()) # rm() removes items of an environment (here .GlobablEnv) ls() # all gone in GlobalEnv ## character(0) Using global assignment &lt;&lt;- operator, one can bend this general rule of not affecting global objects. This can be useful when it is desirable to make certain objects accessible across multiple functions without explicitly passing them through arguments. a &lt;- NULL b &lt;- NULL f1 &lt;- function() { a &lt;&lt;- 1 # global assignment # another way to assign to GlobalEnv assign(&quot;b&quot;, 2, envir = .GlobalEnv) } f1() a ## [1] 1 b ## [1] 2 a &lt;- 2 f2 &lt;- function() { # Since there is no &quot;a&quot; local to f2, R looks for &quot;a&quot; # in a parent environment, or .GlobalEnv print(a) # g() assigns a number to &quot;a&quot; in g()&#39;s environment g &lt;- function() a &lt;&lt;- 5 a &lt;- 0 # object called &quot;a&quot; local to f2 print(a) # g() updates only the local &quot;a&quot; to f2(), but not &quot;a&quot; in GlobalEnv # R&#39;s scope hierarchy starts from local to its environment g() print(a) } a &lt;- 3 # the first &quot;a&quot; is in .GlobalEnv when f2() is called # the second &quot;a&quot; is local to an instace of f2() # the third &quot;a&quot; is the updated version of the local &quot;a&quot; by g() f2() ## [1] 3 ## [1] 0 ## [1] 5 a # object &quot;a&quot; in GlobalEnv: unchanged by g() ## [1] 3 It is convenient to use &lt;&lt;- if you are sure about which object to overwrite. Otherwise, the use of &lt;&lt;- should be avoided. 3.3.6 Debugging browser() and traceback() are common debugging tools. A debugging session starts where browser() is inserted and allows for a line-by-line execution onward. Putting browser() inside a loop or function is useful because it allows for accessing the objects at a particular moment of execution in its environment. After an error alert, executing traceback() shows at which process the error occurred. Other tools include debug(), debugger(), and stopifnot(). 3.3.7 Stat func. What is going on here? What do we see? Table 3.2: Common R Statistical Distribution Functions Distribution Density_pmf cdf Quantiles Random_draw Normal dnorm( ) pnorm( ) qnorm( ) rnorm( ) Chi square dchisq( ) pchisq( ) qchisq( ) rchisq( ) Binomial dbinom( ) pbinom( ) qbinom( ) rbinom() source: (Matloff 2011) 3.3.8 String func. R has built-in string manipulation functions. They are commonly used for; detecting a certain pattern in a vector (grep() returning a location index vector, grepl() returning a logical vector) replacing a certain pattern with another (gsub()) counting the length of a string (nchar()) concatenating characters and numbers as a string (paste(), paste0(), sprintf()) extracting a segment of a string by character position range (substr()) splitting a string with a particular pattern (strsplit()) finding a character position of a pattern in a string (regexpr()) oasis &lt;- c(&quot;Liam Gallagher&quot;, &quot;Noel Gallagher&quot;, &quot;Paul Arthurs&quot;, &quot;Paul McGuigan&quot;, &quot;Tony McCarroll&quot;) grep(pattern = &quot;Paul&quot;, oasis) ## [1] 3 4 grepl(pattern = &quot;Gall&quot;, oasis) ## [1] TRUE TRUE FALSE FALSE FALSE gsub(&quot;Gallagher&quot;, &quot;Gallag.&quot;, oasis) ## [1] &quot;Liam Gallag.&quot; &quot;Noel Gallag.&quot; &quot;Paul Arthurs&quot; &quot;Paul McGuigan&quot; ## [5] &quot;Tony McCarroll&quot; nchar(oasis) ## [1] 14 14 12 13 14 paste(oasis) ## [1] &quot;Liam Gallagher&quot; &quot;Noel Gallagher&quot; &quot;Paul Arthurs&quot; &quot;Paul McGuigan&quot; ## [5] &quot;Tony McCarroll&quot; paste(oasis, collapse=&quot;, &quot;) ## [1] &quot;Liam Gallagher, Noel Gallagher, Paul Arthurs, Paul McGuigan, Tony McCarroll&quot; sprintf(&quot;%s from %d to %d&quot;, &quot;Oasis&quot;, 1991, 2009) ## [1] &quot;Oasis from 1991 to 2009&quot; substr(oasis, 1, 6) ## [1] &quot;Liam G&quot; &quot;Noel G&quot; &quot;Paul A&quot; &quot;Paul M&quot; &quot;Tony M&quot; strsplit(oasis, split=&quot; &quot;) # split by a blank space ## [[1]] ## [1] &quot;Liam&quot; &quot;Gallagher&quot; ## ## [[2]] ## [1] &quot;Noel&quot; &quot;Gallagher&quot; ## ## [[3]] ## [1] &quot;Paul&quot; &quot;Arthurs&quot; ## ## [[4]] ## [1] &quot;Paul&quot; &quot;McGuigan&quot; ## ## [[5]] ## [1] &quot;Tony&quot; &quot;McCarroll&quot; regexpr(&quot;ll&quot;, oasis[1])[1] ## [1] 8 Common regular expressions used in R include; &quot;[char]&quot; (any string containing either “c”, “h”, “a”, or “r” ) &quot;a.c&quot; (any string containing “a” followed by any letter followed by “c”) &quot;\\\\.&quot; (any string containing symbol “.”). grepl(&quot;[is]&quot;, oasis) ## [1] TRUE FALSE TRUE TRUE FALSE grepl(&quot;P..l&quot;, oasis) ## [1] FALSE FALSE TRUE TRUE FALSE grepl(&quot;\\\\.&quot;, c(&quot;Liam&quot;, &quot;Noel&quot;, &quot;Paul A.&quot;, &quot;Paul M.&quot;, &quot;Tony&quot;)) ## [1] FALSE FALSE TRUE TRUE FALSE 3.3.9 Set func. The functions for common set operations include union(), intersect(), setdiff(), and setequal(). The most commonly used function is %in% operator; X %in% Y returns a logical vector indicating whether an each element of X is a member of Y. c(1,2,3,4,5) %in% c(3,2,5) c(&quot;a&quot;,&quot;b&quot;,&quot;t&quot;,&quot;s&quot;) %in% c(&quot;t&quot;,&quot;a&quot;,&quot;a&quot;) References "],
["3-4-housekeeping.html", "3.4 Housekeeping", " 3.4 Housekeeping 3.4.1 Working directory getwd() returns the current working directly. setwd(new_directory) sets a specified working directory. 3.4.2 R session sessionInfo() shows the current session information. In RStudio, .rs.restartR() restarts a session. 3.4.3 Save &amp; load R objects can be saved and loaded by save(object1, object2, ..., file=&quot;file_name.RData&quot;) and load(file=&quot;file_name.RData&quot;). A ggplot object can be save by ggsave(&quot;file_name.png&quot;). 3.4.4 Input &amp; Output A common method to read and write data files is read.csv(&quot;file_name.csv&quot;) and write.csv(data_frame, file = &quot;file_name.csv&quot;). scan() is a more general function to read data files and interact with user keyboard inputs. file() is also a general function for reading data through connections, which refer to R’s mechanism for various I/O operations. dir() returns the file names in your working directory. A useful function is cat(), which can print a cleaner output to the screen, compared to print(). print(&quot;example&quot;) ## [1] &quot;example&quot; cat(&quot;example\\n&quot;) # end with \\n ## example cat(&quot;some string&quot;, c(1:4), &quot;more string\\n&quot;) ## some string 1 2 3 4 more string cat(&quot;some string&quot;, c(1:4), &quot;more string\\n&quot;, sep=&quot;_&quot;) ## some string_1_2_3_4_more string 3.4.5 Updating R needs regular updates for R distribution, individual R packages, and RStudio. Generally, updating once or twice a year would suffice. For updating RStudio, go to Help and then Check for Updates. Also, RStudio also makes it easy to update packages; go to Tools and the Check for Package Updates. Do these updates when you have time or you know that you need to update a particular package; updating R and R packages can be trickier than it seems. # check R version getRversion() ## [1] &#39;3.4.4&#39; version ## _ ## platform x86_64-apple-darwin15.6.0 ## arch x86_64 ## os darwin15.6.0 ## system x86_64, darwin15.6.0 ## status ## major 3 ## minor 4.4 ## year 2018 ## month 03 ## day 15 ## svn rev 74408 ## language R ## version.string R version 3.4.4 (2018-03-15) ## nickname Someone to Lean On # check installed packages ## installed.packages() # list all packages where an update is available ## old.packages() # update all available packages of installed packages ## update.packages() # update, without prompt ## update.packages(ask = FALSE) For windows users, one can automate the process using installr package. ## --- execute the following --- ## install.packages(&quot;installr&quot;) # install ## setInternet2(TRUE) # only for R versions older than 3.3.0 ## installr::updateR() # updating R. Sometimes, you can accidentally corrupt sample datasets that come with packages. To restore the original datasets, you have to remove the package by remove.packages() and then install it again. Use class(), attributes(), and str() to check for any unrecognized attributes attached to the dataset. Also, if you suspect that you have accidentally corrupted R itself, you should re-install the R distribution. "],
["4-piecemeal-top.html", "4 Piecemeal Topics", " 4 Piecemeal Topics 4.1 dplyr and ggplot exercise 4.2 t-test, bootstrapping, and linear regressions 4.3 Demo: Mixed Effects and LSMEANS "],
["4-1-dplyr.html", "4.1 Unusual Deaths in Mexico", " 4.1 Unusual Deaths in Mexico Materials This is a practice session of dplyr and ggplot2 using a case study related to tidyr package. The case is about investigating the causes of death in Mexico that have unusual temporal patterns within a day. The data on mortality in 2008 have the following pattern by hour; Figure 4.1: Temporal pattern of all causes of death Do you find anything unusual or unexpected? The figure shows several peaks within a day, indicating some increased risk of death during certain times of the day. What could generate these patterns? Wickham, the author of the case study, finds; The causes of [unusual] death fall into three main groups: murder, drowning, and transportation related. Murder is more common at night, drowning in the afternoon, and transportation related deaths during commute times (Wickham 2014). Figure 4.2: Causes of death with unusual temporal courses. Hour of day (hod) on the x-axis and proportion (prop) on the y-axis. Overall hourly death rate shown in grey. Causes of death with more than 350 deaths over a year. We will use two datasets: deaths containing the timing and coded causes of deaths codes containing the look-up table for the coded causes. The dataset deaths has over 53,000 records (rows), so we use head() to look at the first several rows. # &quot;deaths08b&quot; is a renamed dataset with easier-to-read column names head(deaths08b) ## Year of Death (yod) Month of Death (mod) Day of Death (dod) ## 1 2008 1 1 ## 2 2008 1 1 ## 3 2008 1 1 ## 4 2008 1 1 ## 5 2008 1 1 ## 6 2008 1 1 ## Hour of Death (hod) Cause of Death (cod) ## 1 1 B20 ## 2 1 B22 ## 3 1 C18 ## 4 1 C34 ## 5 1 C50 ## 6 1 C50 The dataset codes has 1851 records. This table is generated by DT and webshot packages. In the search box, you can type in key words like “bacteria”, “nutrition”, and “fever”, as well as “assault” and “exposure” to see what items are in the data. We will reproduce this case study and practice using functions of dplyr and ggplot2. Arts &amp; Crafts Let’s recap the key ingredients of dplyr and ggplot2 from the introduction in Section 2. The six important functions in dplyr are: filter(): extracts rows (e.g., observations) of a data frame. We put logical vectors in its arguments. select(): extracts columns (e.g., variables) of a data frame. We put column names in its arguments. arrange(): orders rows of a data frame. We put column names in its arguments. summarise(): collapses a data frame into summary statistics. We put summary functions (e.g., statistics functions) using column names in its arguments. mutate(): creates new variables and adds them to the existing columns. We put window functions (e.g., transforming operations) using column names in its arguments. group_by(): assigns rows into groups within a data frame. We put column names in its arguments. We use piping operator %&gt;% (read as then) to translate a sentence of sequential instructions. For example, start with dataset deaths08, then group the data by month of death, and then summarize the grouped data for the number of observations. deaths08 %&gt;% group_by(mod) %&gt;% # mod: month of death summarise(nobs = n()) # n(): a dplyr funciton to count rows ## # A tibble: 12 x 2 ## mod nobs ## &lt;int&gt; &lt;int&gt; ## 1 1 49002 ## 2 2 41685 ## 3 3 44433 ## 4 4 39845 ## 5 5 41710 ## 6 6 38592 ## 7 7 40198 ## 8 8 40297 ## 9 9 39481 ## 10 10 41671 ## 11 11 43341 ## 12 12 42265 The graphics with ggplot2 consist of three components: data: a data frame e.g., the first argument in ggplot(data, ...). geom: geometric objects such as points, lines, bars, etc. with parameters in parenthesis; e.g., geom_point(), geom_line(), geom_histogram() aes: specifications for x-y variables, as well as variables to differentiate geom objects by color, shape, or size. e.g., aes(x = var_x, y = var_y, shape = var_z) We specify data and aes in ggplot() and then add geom objects followed by + symbol (read as add a layer of); e.g., ggplot(data = dataset, mapping = aes(x = ...)) + geom_point(). The order of layers added by + symbol is mostly interchangeable. Combined with %&gt;% operator, we can think of the code as a sentence. For example, consider a histogram of the line-graph for the total number of deaths above. We can formulate the following sequence of steps; “start with dataset deaths08, then plot data via gglpot() where aes() features hour of day on the x-axis and add a player of geom object geom_histogram().” deaths08 %&gt;% ggplot(aes(x = hod)) + geom_histogram(binwidth = 1, color = &quot;white&quot;) Here are a few more examples. # a summary by month of day and hour of day. # e.g, Jan-1am, ..,Jan-12pm, Feb-1am,..., Feb-12pm, ... n_month_hour &lt;- deaths08 %&gt;% group_by(mod, hod) %&gt;% summarise( nobs = n() ) n_month_hour %&gt;% ggplot(aes(x = hod, y = nobs, color = as.factor(mod))) + geom_point() # &quot;last_plot() + &quot; allows for adding more layers to the previous plot last_plot() + geom_line() Exercise Now it is your turn. The exercise is to reproduce the above results for the unusual causes of deaths. Download materials: case study paper and case study data Set working directly: setwd(your_directory) Load libraries: library(dplyr), library(ggplot2), library(MASS) (used for fitting data by robust regression) Note 1: There is a minor error in the case study where the author accidentally kept several records of data from years other than 2008. This has virtually no effect on the results, and we are seeking to reproduce the same results as in the case study. Note 2: You could look at the code in the paper for hints. However, the code is written with the functions of plyr package, which is sort of like a predecessor of dplyr. Speaking of plyr, when loaded together, plyr and dplyr can cause namespace issues (having the same function names from different packages!). plyr namespace is automatically loaded with ggplot2. If you type ?summarise and ?arrange you will see that these functions are loaded from the two packages. This problem is not uncommon in R, as notified upon loading packages by the warnings about masked functions. To call a function from a specific package, use double-colon operator (e.g., dplyr::summarise()). The reference before :: is called namespace, and you can find out all namespaces loaded in the current session; loadedNamespaces() ## [1] &quot;Rcpp&quot; &quot;knitr&quot; &quot;bindr&quot; &quot;magrittr&quot; &quot;grDevices&quot; ## [6] &quot;munsell&quot; &quot;colorspace&quot; &quot;R6&quot; &quot;rlang&quot; &quot;plyr&quot; ## [11] &quot;stringr&quot; &quot;dplyr&quot; &quot;tools&quot; &quot;utils&quot; &quot;webshot&quot; ## [16] &quot;DT&quot; &quot;grid&quot; &quot;gtable&quot; &quot;xfun&quot; &quot;htmltools&quot; ## [21] &quot;stats&quot; &quot;datasets&quot; &quot;lazyeval&quot; &quot;rprojroot&quot; &quot;digest&quot; ## [26] &quot;assertthat&quot; &quot;tibble&quot; &quot;base&quot; &quot;bookdown&quot; &quot;bindrcpp&quot; ## [31] &quot;ggplot2&quot; &quot;htmlwidgets&quot; &quot;graphics&quot; &quot;glue&quot; &quot;evaluate&quot; ## [36] &quot;rmarkdown&quot; &quot;stringi&quot; &quot;compiler&quot; &quot;pillar&quot; &quot;methods&quot; ## [41] &quot;scales&quot; &quot;backports&quot; &quot;pkgconfig&quot; For example, the following commands produce different results; mtcars %&gt;% group_by(cyl) %&gt;% dplyr::summarise(avgwt = mean(wt)) ## # A tibble: 3 x 2 ## cyl avgwt ## &lt;dbl&gt; &lt;dbl&gt; ## 1 4. 2.29 ## 2 6. 3.12 ## 3 8. 4.00 mtcars %&gt;% group_by(cyl) %&gt;% plyr::summarise(avgwt = mean(wt)) ## avgwt ## 1 3.21725 One way to ensure that we refer to dplyr functions, we can overwrite function names and pass the arguments; # force function namespace for key dplyr functions select &lt;- function(...) dplyr::select(...) filter &lt;- function(...) dplyr::filter(...) arrange &lt;- function(...) dplyr::arrange(...) summarise &lt;- function(...) dplyr::summarise(...) summarize &lt;- function(...) dplyr::summarise(...) mutate &lt;- function(...) dplyr::mutate(...) group_by &lt;- function(...) dplyr::group_by(...) Part A. Display overall hourly deaths We will reproduce the following plot: Hints: Filter NA in the hour of day (hod) variable and define a new dataset Use group_by(), summarise(), n() to obtain death counts by group Use ggplot() + geom_line() to produce plot Use + labs( x = &quot;x lable&quot;, y = &quot;y label&quot;) for axis labels see help file of scale_y_continous() for comma (use ?function_name for help) Part B. Count deaths per hour, per disease We will reproduce the following table: The table shows the first 15 rows of a transformed dataset. Panel (a) shows a frequency (i.e. the number of rows) for each combination of hour of day (hod) and cause of death (cod), supplemented by the disease description in panel (b). Panel (c) shows the proportion (prop) of these combinations within the cause of death; which likely a given cause of death is observed across hours of the day? Panel (d) presents the overall hourly death counts and rates (freq_all and prop_all); if every hour has the same probability of death, we would see prop_all \\(\\approx\\) 0.042 (i.e., 1/24). Here, we see the author’s idea of identifying “unusual deaths” by looking at how “prop” of each hod-cod pairs deviates from “prop_all” (see figure 4.2). What would this comparison mean? Intuitively, we can think of “prop_all” as the normal death rate in a given hour. The key here is to recognize how the author defines unusual deaths. Clearly, there are a small set of common causes of death and a large set of many rare (and obscure) causes of death. The causes of death that are simply rare are not particularly interesting to examine. Here, the author defines unusual deaths as the deaths occurred in unusual hours of the day and examines the associated causes. The variable “prop” describes how each cause of death appears each hour of the day as its total proportion of deaths by that cause. Hints for creating panel (a) Use more than one variable in group_by() Use summarise() with n() to obtain death counts by group Hints for creating panel (b) Use left_join() to add the information from dataset codes Hints for creating panel (c) Use mutate() with sum() on the joined dataset above Hints for creating panel (d) Create a new data frame by using summarise() on the joined and mutated data frame above. summarise() will reduce the dimension of the data frame to its summary, which is the basis of panel (d). Once the desired summary is created, merge it to the data frame of panels (a)-(c). Before using summarise() above, use group_by() to specify new grouping First create freq_all variable via summarise() with n(), use ungroup(), then create prop_all variable via mutate() with sum(). Call this data frame overall_freq, which will be used again at the very end. Use left_join() to join panels (a)-(c) and panel (d) (overall_freq), which we refer to as master_hod data frame. Hints for displaying the same rows as in the Table 16 above Create a subset of the master_hod data under a new name Use filter() to select cod being either “I21”, “N18”, “E84”, or “B16” and hod being greater or equal to 8 and smaller or less than 11 Use select() to pick columns in a desired order and arrange() to sort Part C. Find outliers We will reproduce the following plots: Figure 4.3: Figure 2: (a) Plot of n vs deviation. Variability of deviation is dominated by sample size: small samples have large variability. (b) Log-log plot makes it easy to see the pattern of variation as well as unusually high values. The blue line is a robust line of best fit. We will create a deviation variable named dist by taking the mean of squared differences between prop and prop_all. The above figures show the the number of deaths n by each cause of death and this distance measure dist in the raw-data scale (left) and in the log scale (right). The author’s intuition is that we may observe a pattern that the more common the cause, the smaller the deviation (dist) tends to be. The author uses a linear model to account for such a relationship between the variability and sample size in the logarithmic scale. Once the model is defined and estimated, we can identify “outliers” that deviate largely from its prediction. Those outliers then become the candidates for usual deaths. Hints Use group_by() and summarise() on the master_hod data frame to generate n with function sum() and dist by mean((prop - prop_all)^2) Filter this summary for n &gt; 50 and call it devi_cod (deviations by cause of death) Use ggplot() + geom_point() with data = devi_cod to produce the raw-scale figure Additionally use scale_x_log10(), scale_y_log10(), and geom_smooth(method = &quot;rlm&quot;, se = FALSE) to produce the log-scale figure See help for scale_x_log10() to adjust axis labels (look for “comma”) Technically speaking, we should change the axis labels to indicate the logarithmic transformation, but we skip it here. Let’s not worry about reproducing the exact grids as they appear in the paper Part D. Fit data by a regression and plot residuals We will reproduce the following plot: Figure 4.4: Figure 3: Residuals from a robust linear model predicting log(dist) by log(n). Horizontal line at 1.5 shows threshold for further exploration. The figure is a plot of the regression residuals resid of log(dist) on log(n). By visual inspection, the points lying above the horizontal line at resid=1.5 are considered to be “unusual causes of deaths” by the author. Here the author used the robust linear model (rlm()) regression, but the syntax is mostly the same as that of the standard linear model regression (lm() ). Here is an example of regression by lm(). df &lt;- data.frame( x1 &lt;- c(1:10), y1 &lt;- c(1,3,2,4,6,5,7,5,7,8) ) df %&gt;% ggplot(aes(x = x1, y = y1)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) The geom_smooth() is estimating the following linear regression: \\[ y1 = intercept + coefficient * x1 + residual\\] The model is estimated by lm() as follows; f1 &lt;- lm(formula = y1 ~ x1, data = df) Let’s see what we get out of the estimation results f1. class(f1) # class &quot;lm&quot; ## [1] &quot;lm&quot; summary(f1) # summary() knows how to summarise an object of class &quot;lm&quot; ## ## Call: ## lm(formula = y1 ~ x1, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.52727 -0.57273 -0.02727 0.52273 1.54545 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.0000 0.6924 1.444 0.186656 ## x1 0.6909 0.1116 6.192 0.000262 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.014 on 8 degrees of freedom ## Multiple R-squared: 0.8273, Adjusted R-squared: 0.8058 ## F-statistic: 38.34 on 1 and 8 DF, p-value: 0.0002618 coefficients(f1) # coefficient point estimate ## (Intercept) x1 ## 1.0000000 0.6909091 vcov(f1) # coefficient variance-covariance matrix ## (Intercept) x1 ## (Intercept) 0.47939394 -0.06848485 ## x1 -0.06848485 0.01245179 predict(f1) # predicted (fitted) values with the estimated coefficients ## 1 2 3 4 5 6 7 8 ## 1.690909 2.381818 3.072727 3.763636 4.454545 5.145455 5.836364 6.527273 ## 9 10 ## 7.218182 7.909091 resid(f1) # residuals: ## 1 2 3 4 5 6 ## -0.69090909 0.61818182 -1.07272727 0.23636364 1.54545455 -0.14545455 ## 7 8 9 10 ## 1.16363636 -1.52727273 -0.21818182 0.09090909 Let’s get back to our exercise and reproduce the figure above. Hints Run a regression by rlm with formula = log(dist) ~ log(n) and store the residuals in devi_cod data. To read more about linear regressions, see the help file of lm() (type ?lm). For adding a column of residuals, you can use assignment devi_cod$resid &lt;- your_residuals. Plot the residual against log-scale n Note: Check the dataset devi_cod for missing values of dist and n before running a regression (you should not have missing values in this case). Most regression functions, including lm() and rlm(), drop any row with missing values. This becomes an issue if we want to add a new column containing predicted values or residuals to the original dataset. (When rows containing missing values are dropped, the vector generated by predict() or resid() will be shorter than the number of rows in the original dataset.) Use ggplot() + geom_point() structure for the plot Add + scale_x_log10() and + geom_hline(yintercept = 1.5) Part E. Visualize unusual causes of death We will reproduce the following plots: The first figure is the unusual causes of deaths in devi_cod with a relatively large number of deaths (n &gt; 350) and the second is that of a relatively small number of deaths (n &lt;= 350). Hints Using the cut-toff value resid &gt; 1.5, filter devi_cod and call it unusual data frame. Join master_hod and unusual data frames. Then create two subsets of data with conditions n &gt; 350 and n &lt;= 350. Use ggplot() + geom_line() structure with + facet_warp(~ disease, ncol = 3) To include the overall hourly proportions of deaths (prop_all) representing the average of all causes of deaths in a given hour, add another layer by geom_line(aes(...), data = overall_freq) with a local aes() argument and a data argument. With the data argument, variables in another data frame can be combined (assuming the axes have the same measurements), and here we use the overall_freq data frame from the panel (d) portion of Table 16 above. last_plot() %+% another_data_frame reproduces a plot of the same structure with a different data frame The Key Click here Reflections Let’s recap. The author (Wickham) investigates the temporal pattern of death in Mexico to find the causes of death that have unusual temporal patterns within a day. Here are the five steps used in his approach. A. Visualize the overall hourly frequency of death within a day B. Construct variables to compare the proportion of death for each hour per cause against the overall proportion of hourly death C. Plot the data to identify a general relationship among key variables D. Create a linear model (i.e., data point = model prediction + residual) E. Visualize the temporal pattern of the “unusual” cases, or the causes of death that have relatively large residuals While these steps may not serve as a template for your data analysis, the thought process and techniques in the above exercise will be applicable to various situations. In case you need to see more examples, here are additional dplyr and ggplot2 tutorials. R for data science 100 Free Tutorials for Learning R RPubs - Data Processing with dplyr &amp; tidyr Discovering Python &amp; R Aggregating and analyzing data with dplyr genomics class dplyr tutorial dplyr Tutorial (With 50 Examples) References "],
["4-2-boot.html", "4.2 Action, Romance, and Chicks", " 4.2 Action, Romance, and Chicks Materials This session covers t-test, bootstrapping, and linear regressions in the same context, so that you can learn these concepts together and how to apply them in R. We will use a dataset on movies. movies2 &lt;- movies %&gt;% dplyr::select(title, year, budget, rating, Action, Romance) %&gt;% filter((Action ==1 | Romance ==1), !( Action == 1 &amp; Romance == 1), budget &gt; 0, year &gt;= 1970) %&gt;% mutate(budget = budget/10^6) summary(movies2) ## title year budget rating ## Length:1206 Min. :1970 Min. : 0.001 Min. :1.500 ## Class :character 1st Qu.:1992 1st Qu.: 3.500 1st Qu.:5.025 ## Mode :character Median :1998 Median : 15.000 Median :6.000 ## Mean :1996 Mean : 27.549 Mean :5.902 ## 3rd Qu.:2002 3rd Qu.: 40.000 3rd Qu.:6.800 ## Max. :2005 Max. :200.000 Max. :9.800 ## Action Romance ## Min. :0.0000 Min. :0.0000 ## 1st Qu.:0.0000 1st Qu.:0.0000 ## Median :1.0000 Median :0.0000 ## Mean :0.5887 Mean :0.4113 ## 3rd Qu.:1.0000 3rd Qu.:1.0000 ## Max. :1.0000 Max. :1.0000 The extracted data movies2 contain IMDB ratings of Action and Romance movies (excluding those of both Action and Romance genres) that are released between 1970 and 2005 and have known budgets. Action and Romance movies are about 59% and 41% of the data respectively. The average rating is 5.9. Let’s look at the distribution of the release years and ratings in this dataset. movies2$year %&gt;% table ## . ## 1970 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984 ## 12 10 11 8 11 7 9 10 7 4 11 16 10 8 13 ## 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 ## 27 17 18 15 22 22 19 20 30 32 51 55 64 77 64 ## 2000 2001 2002 2003 2004 2005 ## 80 94 110 103 103 36 We see that more data are available for years 1999-2004 than other years. movies2 %&gt;% ggplot(aes(x=rating)) + geom_histogram(binwidth = 0.5, color = &quot;white&quot;, fill = &quot;dodgerblue&quot;) The distribution of rating is somewhat skewed to the left. Let’s see how Action and Romance movies compare. movies2 &lt;- movies2 %&gt;% mutate(genre = ifelse(Action==1, &quot;Action&quot;, &quot;Romance&quot;)) movies2 %&gt;% ggplot(aes(x=rating)) + geom_histogram(binwidth = 0.5, color = &quot;white&quot;, fill = &quot;dodgerblue&quot;) + facet_grid(genre ~ .) movies2 %&gt;% group_by(genre) %&gt;% summarise(mean = mean(rating), sd = sd(rating), n = n()) ## # A tibble: 2 × 4 ## genre mean sd n ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Action 5.729859 1.419404 710 ## 2 Romance 6.147581 1.262879 496 Romance genre gets a slightly higher average rating than Action. For the sake of discussion, suppose that movies2 is the population (or the universe) of our movie data, meaning that it contains all possible observations (movies) that fit our criteria (i.e. Action or Romance movies released in 1970-2005 with known budgets). Then, the population mean ratings for Action and Romance movies are 5.73 and 6.15 respectively. Now consider a sampling world. In almost all situations, the researcher does not have population data and has to work with a sample drawn from the population. Knowing that what we have is only a sample, we make statistical inferences for the property of the population. For example, using a sample of Action and Romance movies, we can compare their average ratings at certain statistical significance. Let’s simulate our sampling world. Here we randomly draw 30 observations from each genre and calculate summary statistics. set.seed(2017) # Fix a starting point of random number generations for reproducibility movie_sample &lt;- movies2 %&gt;% group_by(genre) %&gt;% sample_n(30) movie_sample %&gt;% ggplot(aes(x=rating)) + geom_histogram(binwidth = 0.5, color = &quot;white&quot;, fill = &quot;dodgerblue&quot;) + facet_grid(genre ~ .) movie_sample %&gt;% group_by(genre) %&gt;% summarize(mean = mean(rating), std_dev = sd(rating), n = n()) %&gt;% kable(digits = 3) genre mean std_dev n Action 5.730 1.248 30 Romance 6.333 1.208 30 Here is an another view; movie_sample %&gt;% ggplot(aes( x = genre, y = rating)) + geom_point() + geom_boxplot() To compare the mean ratings between genres, a common practice is to test the equality of means, say \\(\\mu_A\\) and \\(\\mu_R\\) of Action and Romance movies respectively. The null and alternative hypotheses are: \\(H_0: \\mu_A = \\mu_R\\) (equivalently, \\(\\mu_A - \\mu_R = 0\\)) \\(H_A: \\mu_A \\neq \\mu_R\\) t-test Let’s begin with reviewing some fundamental concepts of statistics. Let \\(y_{i}\\) is an independently and identically distributed (i.i.d.) random variable for observation \\(i = 1, .., N\\) drawn from some distribution with population mean \\(\\mu = E[y_i]\\) and standard deviation \\(\\sigma = \\sqrt{E[(y_i - \\mu)^2]}\\) where \\(E[.]\\) is an expectation operator over the random variable. The sample mean and standard deviation of \\(y_{i}\\) are defined as \\[\\bar{y} = \\frac{\\sum_i y_i}{N}, \\quad s =\\sqrt{\\frac{\\sum_i (y_{i} - \\bar{y})^2}{(N-1)}}.\\] We commonly take the average \\(\\bar{y}\\), which serves as an unbiased estimate of \\(\\mu\\). Yet, how close is \\(\\bar{y}\\) to \\(\\mu\\)? The statistical theory gives us a probabilistic answer for inferring population mean \\(\\mu\\). For example, if we know that \\(y_i\\) is normally distributed with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), then it follows that the sample mean \\(\\bar{y}\\) is normally distributed with mean \\(\\mu\\) and standard deviation \\(\\sigma/\\sqrt{N}\\). The distribution of an estimate like this is called sampling distribution, and in special cases it is exactly known. With known* \\(\\mu\\) and \\(\\sigma\\), we know how fast the sample mean \\(\\bar{y}\\) approaches to the population mean \\(\\mu\\) as the sample size \\(N\\) increases. This is done by calculating the z-statistic \\[z = \\frac{\\bar{y} - \\mu}{\\sigma/\\sqrt N}\\] and comparing it to the standard normal distribution table. In other words, to make an inference, we look at the relationship of \\(\\bar{y}\\), \\(\\mu\\), \\(\\sigma\\), and \\(N\\) as described in the z-statistic, for which the shape of the distribution is known. This allows us to infer the representation of observed \\(\\bar{y}\\) if we were to repeat random samples of size \\(N\\) and calculate the sample mean many times. In most situations, we do not know \\(\\mu\\) or \\(\\sigma\\) of the population, and we are unsure whether the underlying distribution is really normal. But, that’s okay. We can still make inferences for the population mean \\(\\mu\\). Under some regularity conditions (e.g. the existence of a finite mean and variance of the random variable), the Central Limit Theorem tells us that regardless the underlying distribution, the sampling distribution of \\(\\bar{y}\\) is approximately normal. In a world with unknown \\(\\mu\\) or \\(\\sigma\\), we approximate the standard normal distribution with a Student’s t distribution. The t-statistic is calculated as \\[t = \\frac{\\bar{y} - \\mu}{s/\\sqrt N}\\] where \\(s\\) is the consistent estimate of \\(\\sigma\\), and we compare it to the t distribution table at \\(N-1\\) degrees of freedom (one degree of freedom is reduced for the estimate \\(s\\)). The Student’s t distribution is fatter-tailed than the standard normal distribution (due to the estimated standard error on the denominator), and it approaches to the standard normal distribution as the sample size \\(N\\) increases. For given significance level \\(\\alpha\\), the \\(1-\\alpha\\) confidence interval is \\[t_{N, 1-\\alpha/2} \\le \\frac{\\bar{y} - \\mu}{s/\\sqrt N} \\le t_{N, \\alpha/2}\\] where \\(t_{N, 1-\\alpha/2}\\) and \\(t_{N, 1-\\alpha/2}\\) are the lower and upper bounds of the t-statistic and are found in the t-distribution table. Since the t-distribution is symmetric, \\(- t_{N, 1-\\alpha/2} = t_{N, 1-\\alpha/2}\\). For example, at \\(\\alpha=0.05\\) and \\(N&gt;1000\\), we have \\(t_{N, 1-\\alpha/2} \\approx -1.96\\) and \\(t_{N, 1-\\alpha/2}=1.96\\). Thus, for a large \\(N\\), the confidence interval of \\(\\mu\\) is given by \\[ \\bar{y} - 1.96 \\:\\: s/\\sqrt{N} \\le \\mu \\le \\bar{y} + 1.96 \\:\\: s/\\sqrt{N}.\\] Let’s get back to the comparison of ratings between genres. How do we test our hypothesis \\(\\mu_A = \\mu_R\\)? Intuitively, we can make estimates of \\(\\mu_A\\) and \\(\\mu_R\\) by the corresponding sample means \\(\\bar{y}_A\\) and \\(\\bar{y}_R\\). Then, it’s a matter of making statistical inferences about \\(\\bar{y}_A - \\bar{y}_R\\) for how close they would be to \\(\\mu_A - \\mu_R\\). We calculate the t-statistic of \\(\\bar{y}_A - \\bar{y}_R\\) and infer the probability of rejecting \\(H_0: \\mu_A - \\mu_R = 0\\). Let \\(\\bar{y}_{A}\\) and \\(s_A\\) be the sample mean and standard deviation of ratings for Action movies and \\(\\bar{y}_{R}\\) and \\(s_R\\) be those for Romance movies. A typical approach called Welch’s t-test statistic uses \\[t = \\frac{\\bar{y}_A - \\bar{y}_R}{s_\\Delta}\\] where \\[s_\\Delta = \\sqrt{\\frac{s_A^2}{N_A} + \\frac{s^2_R}{N_R}}\\] is sort of a joint standard deviation of \\(y_{iA} - y_{iR}\\). Its degree of freedom has a somewhat complicated form but is approximately \\((N_A-1) + (N_R-1)\\) in many situations. For your information (not need to memorize), it is formally given as \\[d.f. = \\frac{s^2_\\Delta}{(s_A^2/N_A)^2/(N_A - 1) + (s_R^2/N_R)^2/(N_R - 1) }.\\] The Welch’s t-test statistic can be manually calculated as follows. # Welch&#39;s t-stat for the mean difference of two groups mean_ratings &lt;- movie_sample %&gt;% group_by(genre) %&gt;% summarize(mean = mean(rating), sd = sd(rating)) sample_diff &lt;- mean_ratings$mean[1] - mean_ratings$mean[2] sample_diff_sd &lt;- sqrt(mean_ratings$sd[1]^2/30 + mean_ratings$sd[2]^2/30) # N = 30 sample_t &lt;- sample_diff/sample_diff_sd c(sample_diff, sample_diff_sd, sample_t) ## [1] -0.6033333 0.3171889 -1.9021261 The observed mean difference is -0.603, for which the t-statistic is -1.902 at approximately 58 degrees of freedom. Let’s visualize this t-statistic against its theoretical distribution, which can be approximated by many random draws from the Student’s t distribution. many_t_df58 &lt;- data.frame(t = rt(10^6, 58)) # one million random draws from t-dist with 58 d.f. many_t_df58 %&gt;% ggplot(aes(x=t)) + geom_histogram(color=&#39;white&#39;, binwidth = 0.1) + geom_vline(xintercept = sample_t) # add vertical line at -1.902 The t distribution is centered at zero and has symmetric tails. We can think of the area of each bar representing the probability that a random draw of t-stat falls in that bin, and the total area of bars on the left of our t-statistic (-1.902) represents the probability that a random draw of t-stat is smaller than -1.902. By applying the two-tail t test, we can calculate the probability that a random draw of t-stat is more extreme than our t-statistic (i.e., being located toward either of the tails); nrow(subset(many_t_df58, abs(t)&gt;= abs(sample_t)))/nrow(many_t_df58) ## [1] 0.061964 Let’s visualize this probability; many_t_df58 %&gt;% ggplot(aes(x=t)) + geom_histogram(color=&#39;white&#39;, binwidth = 0.1) + geom_histogram(data = subset(many_t_df58, abs(t)&gt;= abs(sample_t)), color=&#39;white&#39;, fill=&quot;red&quot;, binwidth = 0.1) + geom_vline(xintercept = sample_t) where the highlighted area represents the probability of type I error, or the rejection of the null hypothesis \\(H_0\\) when it is in fact true, and represents the p-value (0.062 in this case). If we set our tolerance level for making a type I error at the probability of 10% or less (\\(\\alpha = 0.1\\)), we conclude that we reject the null hypothesis \\(H_0\\) (i.e., a finding of a statistically significant difference in ratings between Action and Romance genres) since the p-value is smaller than \\(\\alpha\\). If we set our tolerance level at \\(\\alpha = 0.05\\), we fail to reject \\(H_0\\) (no statistically significant effect). We can visualize how the probability of rejections (called rejection regions) associated with \\(\\alpha = 0.1\\) (orange below) and \\(\\alpha = 0.05\\) (green) compare to our t-statistic; many_t_df58 %&gt;% ggplot(aes(x=t)) + geom_histogram(color=&#39;white&#39;, binwidth = 0.1) + # rejection regions with critical val for a = 0.1 geom_histogram(data = subset(many_t_df58, abs(t)&gt;= abs(qt(.95,58))), color=&#39;white&#39;, fill=&quot;orange&quot;, binwidth = 0.1) + # rejection regions with critical val for a = 0.05 geom_histogram(data = subset(many_t_df58, abs(t)&gt;= abs(qt(.975,58))), color=&#39;white&#39;, fill=&quot;green&quot;, binwidth = 0.1) + geom_vline(xintercept = sample_t) Now that we know what the Welch’s t-test does, we can simply use R’s function to conduct a t-test; movie_sample %&gt;% with(t.test( rating ~ genre )) # using &quot;formula&quot; input ## ## Welch Two Sample t-test ## ## data: rating by genre ## t = -1.9021, df = 57.937, p-value = 0.06213 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -1.23827054 0.03160388 ## sample estimates: ## mean in group Action mean in group Romance ## 5.730000 6.333333 Recall that we started with knowing that the population mean rating for Romance is slightly higher than for Action. Here, it seems reasonable that a random sample of 30 observations from each genre leads us to find a statistically significant difference at the 10% level. Bootstrapping Now we will introduce a concept of bootstrapping. Recall that our statistic, say \\(\\bar{y}\\), conceptually has its distribution, depending on the version of random sample that we happen to draw. The idea here is to mimic the process of having many versions of random samples through simulations, so that we generate a simulated distribution of the statistic. Then, we can make statistical inferences without invoking the statistical theory of the approximate distribution via the Central Limit Theorem. There are many ways to conduct bootstrapping. For simplicity, we will use some of the most common practices. First, we make many random draws of \\(y_{iA}\\) and \\(y_{iR}\\) from our sample movie_sample with replacement (i.e., each time the drawn observation is put back into the pool) where subscript \\(i\\) stands for an observation of movie record. We can use do() from mosaic package and sample_n() from dplyr package. set.seed(2017) boot1 &lt;- mosaic::do(5000) * # repeat the following expression ({...}) for 5000 times ({ movie_sample %&gt;% group_by(genre) %&gt;% # sample 30 obs from each genre with replacement sample_n(30, replace=TRUE) %&gt;% summarize(mean = mean(rating), sd = sd(rating), n =n()) %&gt;% data.frame() }) head(boot1) ## genre mean sd n .row .index ## 1 Action 5.536667 1.0473228 30 1 1 ## 2 Romance 6.173333 1.3284508 30 2 1 ## 3 Action 5.683333 1.3164957 30 1 2 ## 4 Romance 6.760000 1.0682244 30 2 2 ## 5 Action 5.630000 0.8317617 30 1 3 ## 6 Romance 6.333333 1.1931799 30 2 3 The column .index shows the index of bootstrap replications, and the column .row nested in .index gives the indicator of calculated results by genre within each replication. Next, we calculate the bootstrap version of our estimates such as \\(\\bar{y}_A - \\bar{y}_R\\) and Welch’s t statistic. boot1_w &lt;- boot1 %&gt;% dplyr::select(-.row) %&gt;% # get rid of .row column reshape(idvar= &quot;.index&quot;, timevar=&quot;genre&quot;, # reshape into a &quot;wide-form&quot; dataset direction=&quot;wide&quot;) head(boot1_w) ## .index mean.Action sd.Action n.Action mean.Romance sd.Romance n.Romance ## 1 1 5.536667 1.0473228 30 6.173333 1.328451 30 ## 3 2 5.683333 1.3164957 30 6.760000 1.068224 30 ## 5 3 5.630000 0.8317617 30 6.333333 1.193180 30 ## 7 4 5.480000 1.0768408 30 6.170000 1.303087 30 ## 9 5 5.513333 1.1340295 30 6.293333 1.235937 30 ## 11 6 5.950000 1.2601998 30 6.440000 1.131554 30 boot1_w &lt;- boot1_w %&gt;% mutate( bt_diff = (mean.Action - mean.Romance), # difference bt_sd = sqrt(sd.Action^2/n.Action + sd.Romance^2/n.Romance), bt_t = bt_diff/bt_sd # Welch&#39;s t-stat ) Here is how sample estimate sample_diff compares with the histogram of its bootstrap counterpart bt_diff. boot1_w %&gt;% ggplot(aes(x = bt_diff, fill = bt_diff &gt; 0)) + geom_histogram(color = &quot;white&quot;, bins = 40) + geom_vline(xintercept = sample_diff) + theme(legend.position=&quot;bottom&quot;) Here the bt_diff values that are greater than zero are marked by a different color (green) since they suggest the opposite conclusion that the mean rating is higher for Action than for Romance. Depending on the random draw of a bootstrap replication, one could have the opposite result in some of the times. The question is how often that happens. Using the bootstrap estimates, we can estimate the confidence interval in a few ways. For example, to estimate a 95% confidence interval, one can take the 2.5th and 97.5th percentiles of the distribution shown above in the histogram. # confidence interval quantile(boot1_w$bt_diff, c(0.025, 0.975)) # version 1 ## 2.5% 97.5% ## -1.20333333 0.01333333 Another approach is to calculate a bootstrap standard deviation and apply \\(t_{df,\\alpha/2} \\le (\\bar{y}_A - \\bar{y}_R)/s_{bt} \\le t_{df,1-\\alpha/2}\\) where \\(\\bar{y}_A - \\bar{y}_R\\) is the mean difference in ratings between Action and Romance movies, \\(s_{bt}\\) is an estimated standard deviation of \\(\\bar{y}_A - \\bar{y}_R\\), and \\(- t_{df,\\alpha/2}=t_{df,\\alpha/2}=2.00\\) for the t distribution with 58 degrees of freedom. Note that here we do not need \\(\\sqrt{N}\\) in the confidence interval calculation (unlike the above discussion where \\(s\\) was the standard deviation of rating individual \\(y_i\\) instead of the standard deviation of the average). sample_diff_sd_boot &lt;- sd(boot1_w$bt_diff) # version 2 c(sample_diff - 2.00 * sample_diff_sd_boot, sample_diff + 2.00 * sample_diff_sd_boot) ## [1] -1.21881724 0.01215058 For two-tail test, we focus on extreme values on both tails of the distribution. We can visualize this by centering the above graph at sample_diff and examining the values on both tails away from the center. By extending the fill code bt_diff &gt; 0 from the previous histogram, we can subtract sample_diff and take the absolute values. Then the new version of fill code is; abs(bt_diff - sample_diff) &gt; abs(sample_diff). Essentially, we are approximating the distribution of \\((\\bar{y}_A-\\bar{y}_R) - (\\mu_A -\\mu_R)\\) (i.e., sample_diff compared to the population difference) by the distribution of \\((\\bar{y}^*_A-\\bar{y}^*_R) - (\\bar{y}_A-\\bar{y}_R )\\) (i.e., bootstrap estimate \\(\\bar{y}^*_A-\\bar{y}^*_R =\\) bt_diff compared to sample_diff. boot1_w %&gt;% ggplot(aes(x = bt_diff - sample_diff, fill = abs(bt_diff - sample_diff) &gt; abs(sample_diff))) + geom_histogram(color = &quot;white&quot;, bins = 40) + theme(legend.position=&quot;bottom&quot;) This centered histogram with colored tails helps us visualize whether our sample estimate sample_diff is representative; how closely did the bootstrap estimates fall centered around sample_diff? The areas of the extreme values correspond to the rejection regions for the two-tail t-test, and the middle part corresponds to the confidence interval. By summing the area of the rejection regions, we can estimate the p-value, representing the probability that a bootstrap estimate falls in either of the colored tails; # p-value boot1_w %&gt;% with(sum(abs(bt_diff - sample_diff) &gt; abs(sample_diff))/length(bt_diff)) ## [1] 0.0512 Another version of bootstrapping would be to simulate Welch’s t-statistic directly, instead of simulating the mean difference. An estimate in the form of t-statistic has a pivotal quality (meaning that the distribution of the statistic does not depend on unknown parameters). Welch’s t-statistic follows the Student’s t distribution with a given degree of freedom, which does not depend on any unknown parameter such as the population mean or variance. A pivotal statistic is suitable for bootstrapping. Here are the parallel results for bootstrapping Welch’s t-statistic. boot1_w %&gt;% ggplot(aes(x = bt_t, fill = bt_t &gt; 0)) + geom_histogram(color = &quot;white&quot;, bins = 40) + geom_vline(xintercept = sample_t) + theme(legend.position=&quot;bottom&quot;) To convert a confidence interval of Welch’s t-statistic \\((\\bar{y}_A - \\bar{y}_R)/s_\\Delta\\) into a confidence interval of the difference in mean ratings \\(\\bar{y}_A - \\bar{y}_R\\), we multiply the former by \\(s_\\Delta=\\) sample_diff_sd; # confidence interval quantile(boot1_w$bt_t, c(0.025, 0.975)) * sample_diff_sd # version 1 ## 2.5% 97.5% ## -1.31094136 0.01345958 sample_t_sd_boot &lt;- boot1_w %&gt;% with(sd(bt_t - mean(bt_t))) # version 2 c(sample_t - 2.00 * sample_t_sd_boot, sample_t + 2.00 * sample_t_sd_boot) * sample_diff_sd ## [1] -1.27547455 0.06880788 # centered at observed Welch&#39;s t-statistic boot1_w %&gt;% ggplot(aes(x = bt_t - sample_t, fill = abs(bt_t - sample_t) &gt; abs(sample_t))) + geom_histogram(color = &quot;white&quot;, bins = 40) + theme(legend.position=&quot;top&quot;) # p-value boot1_w %&gt;% with(sum(abs(bt_t - sample_t) &gt; abs(sample_t))/length(bt_t)) ## [1] 0.0716 Linear Models Let’s extend our discussion on bootstrapping to linear regression. While the formal discussion of linear models has not been covered so far in this site, we can develop an intuitive understanding on how they works. Start with the following model \\[ y_i = a_0 + b_1 x_{i} + \\varepsilon_i\\] where \\(y_i\\) is the dependent variable of observation \\(i\\), \\(x_i\\) is an independent variable, \\(a_0\\) and \\(b_1\\) are parameters for the intercept and the slope of \\(x\\), and \\(\\varepsilon_i\\) is the residual error term. Depending on the assumption of the error term \\(\\varepsilon_i\\), we could estimate different models on the same equation. One of the key assumptions we need is that the error \\(\\varepsilon_i\\) is uncorrelated with the independent variable \\(x_i\\). Here is an example of estimating the Ordinary Least Squares (OLS) via lm() function. d0 &lt;- data.frame( y = c(1, 3, 5, 2, 6, 7, 8, 3), x = c(0, 2, 8, 1, 6, 8, 10, 4)) lm( y ~ x, data = d0) %&gt;% summary() ## ## Call: ## lm(formula = y ~ x, data = d0) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.3966 -0.3683 0.2207 0.5145 0.8972 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.2213 0.5109 2.390 0.053994 . ## x 0.6469 0.0856 7.557 0.000279 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8338 on 6 degrees of freedom ## Multiple R-squared: 0.9049, Adjusted R-squared: 0.8891 ## F-statistic: 57.11 on 1 and 6 DF, p-value: 0.0002787 Coefficient estimates are \\(a_0 = 1.2213\\) and \\(b_1 = 0.6469\\) with the standard errors of \\(sd(a_0) = 0.5109\\) and \\(sd(b1) = 0.0856\\), suggesting that the t-values of \\(2.390\\) and \\(7.557\\) for testing whether these coefficients are statistically different from zero, or \\(H_0: a_0=0\\) and \\(H_0: b_1 =0\\). The standard error for the residual is estimated with the assumption that \\(\\varepsilon_i\\) is i.i.d. normal with mean zero and some standard deviation \\(\\sigma\\). “Pr(&gt;|t|)” shows the p-values of the coefficient estimates, and the statistical significance is indicated with symbols “***”, “**” etc. Let’s not worry about other metrics here. Let’s visualize the above regression; d0 %&gt;% ggplot(aes( x = x, y = y)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se=FALSE) With only two variables, a plot like this gives us a clear picture of how the regression relates to the data points. The straight line is the fitted regression equation \\(\\widehat{a}_0 + \\widehat{b}_1 x_i\\) where hat \\(\\widehat{}\\) notation represents an estimate. The vertical distance (and its direction) from the fitted line is the residual \\(\\widehat{\\varepsilon_i}=y_i - \\widehat{a}_0 + \\widehat{b}_1 x_i\\). The OLS estimates are the best linear unbiased estimators (BLUE). Now, let \\(y_{ij}\\) be the rating of movie \\(i\\) in genre \\(j\\) where \\(j\\) is either \\(A\\) (Action) or \\(R\\) (Romance). Also, let 1(cond) be the indicator function that takes a value of one if condition cond is true and zero otherwise. Then, we can test the mean difference in movie ratings between Action and Romance by a linear regression. \\[ y_{ij} = b_A \\: 1(j=A) + b_R \\: 1(j=R) + \\varepsilon_{ij}\\] where the means of Action and Romance movies are estimated by \\(b_A\\) and \\(b_R\\) respectively. Under the standard OLS assumptions, \\(b_A\\) and \\(b_R\\) are equivalent to the sample means (calculated by the usual summing and dividing by the number of observations), and the estimates of the variances for \\(\\hat{b}_A\\) and \\(\\hat{b}_R\\) are obtained by matrix algebra (which we will cover in a future session). For rotational brevity, this equation may be written as \\[ y_{ij} = \\alpha_j + \\varepsilon_{ij}\\] where \\(\\alpha_j\\) is \\(\\alpha_A = b_A\\) for \\(j=A\\) and \\(\\alpha_R = b_R\\) for \\(j=R\\). This model yields the following; # &quot;0 +&quot; eliminates the intercept lm(rating ~ 0 + genre, data = movie_sample) %&gt;% summary() ## ## Call: ## lm(formula = rating ~ 0 + genre, data = movie_sample) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.3333 -0.6583 -0.1300 0.7942 3.2700 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## genreAction 5.7300 0.2243 25.55 &lt;2e-16 *** ## genreRomance 6.3333 0.2243 28.24 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.228 on 58 degrees of freedom ## Multiple R-squared: 0.9615, Adjusted R-squared: 0.9602 ## F-statistic: 725 on 2 and 58 DF, p-value: &lt; 2.2e-16 Note that the t-statistics test hypotheses \\(H_0: b_A = 0\\) and \\(H_0: b_R = 0\\), which differ from our interest, \\(H_0: b_A = b_R\\). We can rewrite the above equation as \\[ y_{ij} = a_0 + \\beta_R \\: 1(j=R) + \\varepsilon_{ij}\\] where \\(\\beta_R = b_R - b_A\\). Moreover, we can rewrite this as \\[ y_{ij} = a_0 + \\alpha_j + \\varepsilon_{ij}\\] where \\(\\alpha_A\\) serves as a reference group and hence is excluded from the coefficient estimates. This yields; ols1 &lt;- lm( rating ~ genre, data = movie_sample) summary(ols1) ## ## Call: ## lm(formula = rating ~ genre, data = movie_sample) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.3333 -0.6583 -0.1300 0.7942 3.2700 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.7300 0.2243 25.548 &lt;2e-16 *** ## genreRomance 0.6033 0.3172 1.902 0.0621 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.228 on 58 degrees of freedom ## Multiple R-squared: 0.05872, Adjusted R-squared: 0.04249 ## F-statistic: 3.618 on 1 and 58 DF, p-value: 0.06212 where the estimate of \\(\\alpha_R\\) is equivalent with the calculated difference sample_diff, and its t-statistic is approximately the same as sample_t above. The sign is switched since our estimate here is \\(b_R - b_A\\) instead of \\(b_A - b_R\\). Like the above output from Welch’s t-statistic, based on the statistical theory the OLS output here provides the estimate of standard errors for coefficients. Now let’s derive our estimate of standard errors by applying bootstrapping to the OLS model. In doing so, we create several functions here. The first two functions extract “formula” and the dependent variable from a lm class object. getFormula &lt;- function(model) gsub(&quot;()&quot;,&quot;&quot;, model$call[2]) # gsub() substitues characters getFormula(ols1) ## [1] &quot;rating ~ genre&quot; getDependentVar &lt;- function(model) { str &lt;- getFormula(model) gsub(&quot; &quot;,&quot;&quot;, substr(str, 1, (regexpr(&quot;~&quot;,str)[1]-1))) # substr() takes a substring } getDependentVar(ols1) ## [1] &quot;rating&quot; The next function takes a lm class object (i.e., an output of lm()) with a specified number of bootstrap replications and produces bootstrap versions of the coefficient estimates as an output. Assume that the distribution of \\(\\varepsilon_i\\) is a normal distribution with mean \\(0\\) and variance \\(\\hat{\\sigma}^2\\) where \\(\\hat{\\sigma} = \\sum\\hat{\\varepsilon}^2_i/(N-k)\\) is the OLS estimate of \\(\\sigma\\) with \\(N-k\\) degrees of freedom (d.f. is the number of observations minus the number of parameters). Generate a bootstrapped dependent variable by combining the predicted part of the linear model \\(\\hat{a}_0 + \\hat{\\alpha}_j\\) and an random draw of bootstrap error term \\(\\varepsilon^b_i\\), or \\(y^b_{ij} = \\hat{a}_0 + \\hat{\\alpha}_j + \\varepsilon^b_i\\). In each bootstrap replication \\(b=1, .., B\\), replace \\(y_{ij}\\) with its bootstrap counterpart \\(y^b_{ij}\\) and run the OLS estimation, and we repeat this process for \\(B\\) times (we set \\(B=5000\\)). run_ols_boot &lt;- function(lm_rlt, num_do = 5000) { # calculate the standard deviation of the residuals N &lt;- length(lm_rlt$residuals) sd_res &lt;- (sum(lm_rlt$residuals^2)/lm_rlt$df.residual) %&gt;% sqrt() dep_var &lt;- getDependentVar(lm_rlt) do(num_do) * ({ data_bt &lt;- lm_rlt$model # replace the dependent variable with its bootstrap counterpart data_bt[[dep_var]] &lt;- lm_rlt$fitted.values + # the predicted component + rnorm(N, mean = 0, sd = sd_res) # random draws from the error distribution # run the OLS model with the same formula but with a new, bootstrap dataset ols_bt &lt;- lm(as.formula(getFormula(lm_rlt)), data = data_bt) coef(ols_bt) # get coefficients }) } set.seed(2017) # run bootstrap with our function bt_est_ols1 &lt;- run_ols_boot(ols1, 5000) Let’s compare the estimates of standard errors between the default OLS and our bootstrap results. sample_ols1 &lt;- tidy(ols1) # summary of the original OLS estimates bt_sd_ols1 &lt;- apply(bt_est_ols1, 2, sd) # calculate bootstrap standard errors bt_ols1 &lt;- cbind(coeff = sample_ols1$estimate, # copy the coeff from the OLS result sd = bt_sd_ols1, # use bootstrap standard errors tstat = sample_ols1$estimate/bt_sd_ols1) # OLS estimates with statistical inferences by statistic theory sample_ols1 ## term estimate std.error statistic p.value ## 1 (Intercept) 5.7300000 0.2242864 25.547688 2.999063e-33 ## 2 genreRomance 0.6033333 0.3171889 1.902126 6.212427e-02 # OLS estimates with statistical inferences by bootstrapping bt_ols1 ## coeff sd tstat ## Intercept 5.7300000 0.2224649 25.756876 ## genreRomance 0.6033333 0.3137163 1.923181 In this case they are pretty close. Let’s visualize this. sample_Romance &lt;- sample_ols1$estimate[2] bt_est_ols1 %&gt;% ggplot(aes(x = genreRomance - sample_Romance, fill = (abs(genreRomance - sample_Romance) &gt;= abs(sample_Romance)))) + geom_histogram(color = &quot;white&quot;, bins = 40) + theme(legend.position=&quot;bottom&quot;) Here are the estimated confidence interval and p-value by bootstrapping. # confidence interval by bootstrapping quantile(bt_est_ols1$genreRomance, c(0.025, 0.975)) # version 1 ## 2.5% 97.5% ## -0.01474822 1.22427177 c(&#39;2.5%&#39; = sample_Romance - 2.00 * bt_sd_ols1[2], &#39;97.5%&#39; = sample_Romance + 2.00 * bt_sd_ols1[2]) # version 2 ## 2.5%.genreRomance 97.5%.genreRomance ## -0.0240993 1.2307660 # p-value bt_est_ols1 %&gt;% with( sum(abs(genreRomance - sample_Romance) &gt; abs(sample_Romance))/length(genreRomance) ) ## [1] 0.0548 These results are also very close to what we saw for the Welch’s t-statistic and its bootstrap estimates above. Now let’s take a step further into regression modeling. The regression allows us to utilize additional variables in the model. Let’s try adding a linear effect of movie budget. It seems reasonable to hypothesize that the higher the budget, the better a movie can be since the director can employ famous actors and actresses, expensive movie settings, or computer graphics. Then, our estimation equation becomes \\[ y_{ij} = a_0 + \\alpha_j + \\beta_1 \\:budget_i + \\varepsilon_{ij}.\\] ols2 &lt;- lm( rating ~ genre + budget, data = movie_sample) summary(ols2) ## ## Call: ## lm(formula = rating ~ genre + budget, data = movie_sample) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.20205 -0.69226 -0.07884 0.71999 2.92299 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.424309 0.304038 17.841 &lt;2e-16 *** ## genreRomance 0.753435 0.330186 2.282 0.0263 * ## budget 0.006944 0.004717 1.472 0.1465 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.216 on 57 degrees of freedom ## Multiple R-squared: 0.09319, Adjusted R-squared: 0.06137 ## F-statistic: 2.929 on 2 and 57 DF, p-value: 0.06155 The new estimate of \\(\\alpha_R\\) is 0.753 with standard error 0.330. A common interpretation goes like this; the estimated relative effect of Romance to Action genre on the movie rating is 0.753 at the 5% significance level, while controlling for the effect of movie budget. Note that the effect of budget itself estimation. Recall that an important assumption is that the error term is uncorrelated with independent variables. Here the above model relies on the assumption; \\[E[\\varepsilon_{ij} \\:| \\: genre_j,\\: budget_i] = E[\\varepsilon_{ij} ] = 0\\] where \\(E[v | u]\\) denotes the conditional mean of \\(v\\) given \\(u\\). It says that the information of \\(genre_j\\) and \\(budget_i\\) does not affect the mean of the error distribution. There is no definitive way to test this assumption, and it could be violated in several ways. The most relevant case here is what is known as omitted variable bias; some unobserved attributes of the movie (which are conceptually a part of the error \\(\\varepsilon_{ij}\\)) may be correlated with both \\(y_{ij}\\) and \\(budget_i\\). For example, many people might agree that the use of explosions make action movies more exciting and romance movies more dramatic. Then, suppose that the story taking place in a war-time setting can increase the movie rating and also inflate the movie budget. In such a case, the OLS estimates could be biased via the omitted variable (an indicator for having a war-time setting). In a future session, we will talk more about the potential sources of bias. The bottom line: every model is incorrect when applied to some real-world data, and the “error term” captures the deviation from the model prediction. The correctness is always a matter of degree, and that’s why we care about the error term; how is the predicted error distributed? Inspecting the predicted error for its properties is important. However, the true error in regression analysis is essentially conceptual unless the analyst knows exactly how the data are generated. Randomized control trials (RCT) give the researcher confidence in such data generating process. In observational studies, the statistical modeling of observed data tends to an art as much as a science, requiring careful considerations of regression models. Returning to our topic, let’s add another variable to our model? The movies in the dataset were released between 1970 and 2005, during which movie-goers’ preferences or movie production costs may have changed systematically. By accounting for a quadratic time trend, we estimate the following \\[ y_{ij} = a_0 + \\alpha_j + \\beta_1 \\:budget_i + \\beta_2 \\: year_i + \\beta_3 \\: year^2_i + \\varepsilon_{ij}\\] ols3 &lt;- lm( rating ~ genre + budget + year + I(year^2), data = movie_sample) # I() allows the user to construct a new variable on the fly. summary(ols3) ## ## Call: ## lm(formula = rating ~ genre + budget + year + I(year^2), data = movie_sample) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.02921 -0.70194 0.03311 0.60606 3.00738 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.108e+03 7.024e+03 0.727 0.4701 ## genreRomance 7.918e-01 3.254e-01 2.434 0.0182 * ## budget 8.452e-03 4.700e-03 1.798 0.0776 . ## year -5.087e+00 7.058e+00 -0.721 0.4741 ## I(year^2) 1.268e-03 1.773e-03 0.715 0.4776 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.196 on 55 degrees of freedom ## Multiple R-squared: 0.1544, Adjusted R-squared: 0.09288 ## F-statistic: 2.51 on 4 and 55 DF, p-value: 0.05214 When accounting for the time trend, the coefficient for budget is statistically significant at the 10% level, and the residual standard error is slightly reduced from 1.216 to 1.196. That seems like an improvement, and the model equation looks sensible. Now do you feel more confident in these results? Let’s see if we can conduct a further check. We assumed a common quadratic time trend for Action and Romance movies. Let’s take a step back and visualize the time trend in the data using ggplot() + geom_jitter() + geom_smooth(). By default geom_smooth() uses a flexible form to fit the relationship between x and y variables. movie_sample %&gt;% ggplot(aes( x = year, y = rating, color = genre)) + geom_jitter() + geom_smooth(se =F) ## `geom_smooth()` using method = &#39;loess&#39; We can immediately spot that in our sample, we only have a few movies from the 1970-1984 period. Also, we see that the time trend seems to be shifted around year 2000. Let’s set aside those older movies in our model. We can also control for the impact of movie budget (via \\(\\hat{\\beta_1} \\: budget_i\\) with our estimate \\(\\hat{\\beta_1}\\) and replacing it with the sample average (\\(\\hat{\\beta_1} \\: E[budget_i]\\)). And here is an updated version holding the effect of budget constant at the sample mean; movie_sample %&gt;% filter(year &gt;= 1985) %&gt;% ggplot(aes( x = year, y = rating - ols3$coefficients[&#39;budget&#39;]*(budget - mean(budget)), color = genre)) + geom_jitter() + geom_smooth(se =FALSE) ## `geom_smooth()` using method = &#39;loess&#39; We still see some shift in time trend around 2000, at which the trends between Action and Romance movies start to diverge. Year 2000 may be the beginning of increasing computer graphics due to its decreasing production costs. That could be a turning point, especially for Action movies. Now what can we do? Assigning different time trends for Action and Romance would be a possibility if our objective were to simply find the model that best fits the data. However, that is not be a good idea if we are interested in comparing the average ratings of the two genres. After all, the genre-specific time trend is a part of the difference between genres that we want to compare. The best we could do seems that we let the time trend vary before and after 2000, while assuming the common trend for both genres. Here is a relatively simple solution. \\[ \\begin{align} \\nonumber y_{ij} &amp;= a_0 + \\alpha_{j} + \\beta_1 \\:budget_i + \\beta_2 \\: year_i + \\beta_3 \\: year^2_i \\\\ \\nonumber &amp;+ (a_{0,M} + \\alpha_{j, M} + \\beta_{1,M} \\:budget_i + \\beta_{2,M} \\: year_i + \\beta_{3,M} \\: year^2_i) \\:M_i + \\varepsilon_{ij} \\end{align} \\] where \\(M_i = 1(year_i\\ge2000)\\) is an indicator variable for post-millennium years. The items in parenthesis multiplied by \\(M_i\\) are the interaction terms between the baseline variables and the millennium indicator. These additional terms captures the additional effects that only apply to post-millennium movies. The interaction terms can be constructed with * symbol in lm(). # filter data and also remove group-class attribute movie_sample2 &lt;- movie_sample %&gt;% filter(year&gt;=1985) %&gt;% ungroup() # prepare variables movie_sample2 &lt;- movie_sample2 %&gt;% mutate(year_sq = year^2, ge2000 = ifelse(year &gt;= 2000, 1, 0), year_ge2000 = year * ge2000, year_sq_ge2000 = year_sq * ge2000, budget_ge2000 = budget * ge2000) ols4 &lt;- lm( rating ~ genre*(1 + ge2000) + year + year_sq + year_ge2000 + year_sq_ge2000 + budget + budget_ge2000, data = movie_sample2) summary(ols4) ## ## Call: ## lm(formula = rating ~ genre * (1 + ge2000) + year + year_sq + ## year_ge2000 + year_sq_ge2000 + budget + budget_ge2000, data = movie_sample2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.8226 -0.6920 -0.1258 0.6058 3.5089 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.649e+05 5.796e+04 -2.845 0.006515 ** ## genreRomance 1.801e+00 4.946e-01 3.642 0.000662 *** ## ge2000 -8.242e+04 4.076e+05 -0.202 0.840627 ## year 1.656e+02 5.820e+01 2.846 0.006498 ** ## year_sq -4.159e-02 1.461e-02 -2.847 0.006482 ** ## year_ge2000 8.120e+01 4.072e+02 0.199 0.842791 ## year_sq_ge2000 -2.000e-02 1.017e-01 -0.197 0.844965 ## budget 1.448e-02 8.152e-03 1.776 0.082040 . ## budget_ge2000 -4.906e-03 1.100e-02 -0.446 0.657487 ## genreRomance:ge2000 -1.497e+00 7.143e-01 -2.096 0.041371 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.119 on 48 degrees of freedom ## Multiple R-squared: 0.3259, Adjusted R-squared: 0.1995 ## F-statistic: 2.578 on 9 and 48 DF, p-value: 0.01649 The results show that accounting for the effects of budget and distinct time trends for two time spans 1985-1999 and 2000-2005, on average the Romance movie has a 1.801 (\\(\\alpha \\le 0.001\\)) higher rating during 1985-1999 and 0.304 ( = 1.801 - 1.497) higher rating during 2000-2005, compared to the Action movie. To see whether the total effect for the latter period (\\(\\alpha_R + \\alpha_{R,M}\\)) is statistically different from zero, we can use Wald test (using a function from aod package). The standard notation is \\[H_0: \\Gamma \\beta = r\\] where \\(\\beta\\) is the coefficients of the linear model, \\(\\Gamma\\) is a matrix that specifies linear combinations of \\(\\beta\\), and \\(r\\) is a column vector of constants. In this case, we only have a single equation for \\(H_0\\) (i.e. a single row), namely \\(H_0: \\alpha_R + \\alpha_{R, M} = 0\\). This corresponds to \\(\\Gamma = [0\\: 1\\: 0\\: 0\\: 0\\: 0\\: 0\\: 0\\: 0\\: 1]\\) (the second and tenth coefficients corresponding to \\(\\alpha_R\\) and \\(\\alpha_{R, M}\\)) and \\(r = [0\\: 0\\: 0\\: 0\\: 0\\: 0\\: \\: 0\\: 0\\: 0\\: 0]&#39;\\). The test statistic \\(\\alpha_R + \\alpha_{R, M}\\) approximately follows the chi-square distribution. gamma &lt;- matrix(c(0, 1, 0, 0, 0, 0, 0, 0, 0, 1), nrow=1) wald.test(Sigma = vcov(ols4), b=coef(ols4), L=gamma) ## Wald test: ## ---------- ## ## Chi-squared test: ## X2 = 0.35, df = 1, P(&gt; X2) = 0.56 which shows the p-value of 0.56. Thus, we fail to reject \\(H_0: \\alpha_R + \\alpha_{R, M} = 0\\) at the 10% significance level. This means that there is not significant difference between genres for the 2000-2005 period. We can visualize the results in data plots. First, let’s observe raw data points of ratings and predicted ratings by the model separately. Then, we overlay time trends of predicted ratings (curves) on top of raw data points. Here we use the default option of geom_smooth() to fit curves. # generate predicted ratings movie_sample2 &lt;- movie_sample2 %&gt;% mutate(fit = fitted.values(ols4)) movie_plot0 &lt;- movie_sample2 %&gt;% ggplot( aes( x = year, y = rating, color = genre)) + geom_jitter() # raw-data ratings movie_plot0 # predicted ratings + fitted curves on predicted ratings movie_sample2 %&gt;% ggplot( aes( x = year, y = fit, color = genre)) + geom_jitter() + geom_smooth(se=FALSE) ## `geom_smooth()` using method = &#39;loess&#39; # raw-data ratings + fitted curves on predicted ratings movie_plot0 + geom_smooth( aes( x = year, y = fit, color = genre), se=FALSE) ## `geom_smooth()` using method = &#39;loess&#39; Using the predicted parameters \\(\\widehat{\\beta}_{1}\\) and \\(\\widehat{\\beta}_{1,M}\\), we can further account for the varying effects of budget (\\(\\widehat{\\beta}_1 \\:budget_i + \\widehat{\\beta}_{1,M} \\:budget_i \\: M_i\\)) by replacing them with the sample average (\\(\\widehat{\\beta}_1 \\:E[budget_i] + \\widehat{\\beta}_{1,M} \\:E[budget_i | M_i] \\: M_i\\)). This shows the prediction while holding the effect of budgets at the sample mean. Additionally, we can exactly fit our specification of two-segment quadratic time trends # generating rating data at the sample mean budget movie_sample2 &lt;- movie_sample2 %&gt;% mutate( fit = fitted.values(ols4), budget_eff_adj = - ols4$coefficients[&#39;budget&#39;]*(budget - mean(budget)) + - ols4$coefficients[&#39;budget_ge2000&#39;]*(budget_ge2000 - mean(budget_ge2000)), rating2 = rating + budget_eff_adj, fit2 = fit + budget_eff_adj ) movie_plot1 &lt;- movie_sample2 %&gt;% ggplot( aes( x = year, y = rating2, color = genre)) + geom_jitter() + labs(y = &quot;simulated rating (at mean budget)&quot;) # simulated ratings at the sample mean budget movie_plot1 # predicted ratings and excat fitted curves at the sample mean budget movie_sample2 %&gt;% ggplot( aes( x = year, y = fit2, color = genre)) + geom_point() + geom_smooth( data =movie_sample2 %&gt;% filter(year &lt; 2000), aes( x = year, y = fit2, color = genre), method=&quot;lm&quot;, formula = y ~ x + I(x^2), se=FALSE) + # add fit2 curve for year &gt;= 2000 geom_smooth( data =movie_sample2 %&gt;% filter(year &gt;= 2000), aes( x = year, y = fit2, color = genre), method=&quot;lm&quot;, formula = y ~ x + I(x^2), se=FALSE) # simulated ratings and excat fitted curves at the sample mean budget movie_plot1 + # add fit2 curve for year &lt; 2000 geom_smooth( data =movie_sample2 %&gt;% filter(year &lt; 2000), aes( x = year, y = fit2, color = genre), method=&quot;lm&quot;, formula = y ~ x + I(x^2), se=FALSE) + # add fit2 curve for year &gt;= 2000 geom_smooth( data =movie_sample2 %&gt;% filter(year &gt;= 2000), aes( x = year, y = fit2, color = genre), method=&quot;lm&quot;, formula = y ~ x + I(x^2), se=FALSE) The first plot shows how the data would appear when holding the budget level constant at the sample mean. The second plot shows how the exact fitted curves match the predicted ratings again at the sample mean budget. The third plot combines the two, showing the variation in the data and the model fit while holding the budget effect constant. In summary, we find that accounting for time trends and movie budgets, the Romance movie has a 1.801 (\\(\\alpha ≤0.001\\)) higher rating than the Action movie for 1985-1999, but the difference is insignificant for 2000-2005. The results from the two sample t-test and various regression models may be correct in their own assumptions, but we see that the conclusions can differ depending on how we execute the analysis. As researchers, rarely do we ask the right question in our first try, and that is why the tools of and intuitions for exploratory data analysis are so valuable. Exercise Now it is your turn. Download materials: We will use the same movie2 data data as shown above Set working directly: setwd(your_directory) Load libraries: library(dplyr), library(ggplot2), library(broom), library(tidyr), library(mosaic), library(lme4) Part A: Observe the Central Limit Theorem (CLT) Load movies2 data, which we treat as the population. Let rating \\(y_{ij}\\) denote observation \\(i\\) in genre \\(j=A\\), \\(R\\) for Action or Romance. Calculate population means \\(\\mu_{A}\\) and \\(\\mu_{R}\\) and standard errors \\(\\sigma_{A}\\) and \\(\\sigma_{R}\\) for Action and Romance movies. Calculate the population mean and standard deviation of difference \\(y_{iA} - y_{iR}\\). Hint: the variance of the sum of uncorrelated variables is \\(Var[a + b] = Var[a]\\) and \\(Var[b]\\) for variables \\(a\\) and \\(b\\). What is the predicted distribution of the sample mean difference \\(\\bar{y}_{A} - \\bar{y}_{R}\\) by the CLT? Draw a random sample of 30 observations from each genre and summarize them for stats (mean, sd, and number of observations). The sampling function in dplyr is sample_n(). Hint: Look back to see what we did above and copy the procedure; having the same data format is important for the later part of the exercise. Turn the previous step d into a function, for which the input argument is a sample size and the output is the summary statistics by genre. Call it my_movie_samples(). Apply this function to generate a set of 100 bootstrap replications using mosaic::do(100) * { function(N=30) }. Reshape the bootstrap results via reshape_movie_samples(), plot its density distribution via density_sample_movies(), and calculate summary statistics via stats_sample_movies() using the following functions; reshape_movie_samples &lt;- function(bt_samples) { bt_samples %&gt;% data.frame() %&gt;% # don&#39;t forget to use data.frame() dplyr::select(-.row) %&gt;% reshape(idvar= &quot;.index&quot;, timevar=&quot;genre&quot;, direction=&quot;wide&quot;) %&gt;% mutate(bt_diff = (mean.Action - mean.Romance)) } density_sample_movies &lt;- function(rehsaped_samples, N, B) { rehsaped_samples %&gt;% ggplot(aes(x = bt_diff)) + geom_density(fill = &quot;steelblue&quot;, adjust = 2, alpha = .75) + xlim(c(-2, 2) + pop_diff) + geom_vline(xintercept = mean(rehsaped_samples$bt_diff), color = &quot;steelblue&quot;, size = 1) + geom_vline(xintercept = pop_diff, color = &quot;yellow&quot;, size = 1) + # CTL prediction mean stat_function(fun = dnorm, colour = &quot;yellow&quot;, size =1, # CTL prediction distribution args = list(mean = pop_diff, sd = pop_sigma/sqrt(rehsaped_samples$n.Action[1]))) + labs(title = paste0(&quot;Bootstrop: &quot;, B, &quot;, Num observations:&quot;, N )) } stats_sample_movies &lt;- function(reshaped_samples) { reshaped_samples %&gt;% summarize( diff_mean = mean(bt_diff), diff_sd = sd(bt_diff), p_val = sum(bt_diff&gt;0)/length(bt_diff)*2, theory_mean = pop_diff, theory_sd = pop_sigma/sqrt(length(bt_diff)), abs_error_mean = abs(diff_mean - theory_mean), abs_error_sd = abs(diff_sd - theory_sd) ) } Review the above functions to understand each line. Use ?function_name for look-up. Observe how p_val in stats_sample_movies() relates to the area of the density generated by density_sample_movies(). Also, check what theoretical sd in stats_sample_movies() calculates (for example, sd of what?). Change N and B several times to observe how they influence the results. Part B: Analyze the performance of CLT Pick 6 values between 0 and 120 for the number of observations N and store them as a vector named loc_N: i.e., loc_N &lt;- c(20, 30, ...). Pick 5 values between 100 and 5000 for the number of bootstrap replications B and store them as a vector named loc_B. Conduct 30 simulations of bootstrapping for each combination of N and B from loc_N and loc_B and store the results of density plots and summary stats in nested lists using the following code (simply copy and execute the code, note: function my_movie_samples() is from item e of part A); list_density &lt;- list() list_stats &lt;- list() # This will take some time for (idx_N in 1:length(loc_N)) { list_density[[idx_N]] &lt;- list() list_stats[[idx_N]] &lt;- list() for (idx_B in 1:length(loc_B)) { print(paste0(&#39;N =&#39;, loc_N[idx_N],&#39;, B = &#39;, loc_B[idx_B])) my_boot1 &lt;- mosaic::do(loc_B[idx_B]) * { my_movie_samples(loc_N[idx_N]) } reshaped_my_boot1 &lt;- reshape_movie_samples(my_boot1) list_density[[idx_N]][[idx_B]] &lt;- density_sample_movies(reshaped_my_boot1, loc_N[idx_N], loc_B[idx_B]) list_stats[[idx_N]][[idx_B]] &lt;- stats_sample_movies(reshaped_my_boot1) } } Print the density plots and observe how they vary with \\(N\\) (simply copy and execute the code). Do this for the largest \\(B\\) first, then the smallest \\(B\\). You can use the following code and use the arrows (&lt;-, -&gt;) in the Plots Pane of Rstudio. How would you characterize the results? # Use Plots Pane in RStudio &lt;- -&gt; to observe the influence of N for (idx_N in 1:length(loc_N)) print(list_density[[idx_N]][[which(loc_B==max(loc_B))]]) # dispersion decreases with N for (idx_N in 1:length(loc_N)) print(list_density[[idx_N]][[which(loc_B==min(loc_B))]]) Use the following code to extract the results from the nested lists. extract_list_stats_N &lt;- function(seq, idx_B, stat) { lapply(c(1:length(seq)), function (idx_N) list_stats[[idx_N]][[idx_B]][[stat]]) %&gt;% unlist() } extract_list_stats_B &lt;- function(seq, idx_N, stat) { lapply(c(1:length(seq)), function (idx_B) list_stats[[idx_N]][[idx_B]][[stat]]) %&gt;% unlist() } max_B &lt;- which(loc_B==max(loc_B)) # index of max B max_N &lt;- which(loc_N==max(loc_N)) # index of max N results_N &lt;- data.frame( N = loc_N, p_val = extract_list_stats_N(loc_N, max_B, &quot;p_val&quot;), abs_error_mean = extract_list_stats_N(loc_N, max_B, &quot;abs_error_mean&quot;), abs_error_sd = extract_list_stats_N(loc_N, max_B, &quot;abs_error_sd&quot;) ) results_B &lt;- data.frame( B = loc_B, p_val = extract_list_stats_B(loc_B, max_N, &quot;p_val&quot;), abs_error_mean = extract_list_stats_B(loc_B, max_N, &quot;abs_error_mean&quot;), abs_error_sd = extract_list_stats_B(loc_B, max_N, &quot;abs_error_sd&quot;) ) Use ggplot() on results_N to characterize the relationships between sample size N and p_val, between N and abs_error_mean, and between N and abs_error_sd. Which relationship shows a clear pattern? Why? Hint: use geom_point() and geom_smooth(). How does this relate to the CLT? Use ggplot() on results_B to characterize the relationships between bootstrap size B and p_val, between B and abs_error_mean, and between B and abs_error_sd. Which relationship shows a clear pattern? Why? Part C: Analyze data with linear models You will analyze ChickWeight data that is a part of the sample datasets automatically loaded when you start R. You will run linear models and get some practice on fixed effects (FE) and random effects (RE) models. The ChickWeight dataset contains data of a diet experiment on early growth of chicks. There are four variables: weight (gm), Time (days), Chick (id), and Diet (1 through 4 types). Run the following code to observe the basic structure of the data. ?ChickWeight # description shows up in the Help pane ChickWeight2 &lt;- ChickWeight # make a copy that we may modify head(ChickWeight2) table(ChickWeight2$Chick) table(ChickWeight2$Diet) table(ChickWeight2$Chick, ChickWeight2$Diet) ChickWeight2 %&gt;% ggplot(aes(x = Time, y = weight, color = Diet)) + geom_point(size = .25, alpha=.5) + facet_wrap(~Chick) How would you go about analyzing the effect of Diets on weight growth? Let \\(weight_{ijt}\\) be the weight of chick \\(i\\) in Diet group \\(j\\) observed in time \\(t\\). You will run the following linear models with lm(), see the summary via summary(), and interpret the effects of four Diet types. Let’ start with a model of diet-specific intercepts and a quadratic time trend given by \\[ weight_{ijt} = \\alpha_j + \\beta_1\\: time_t + \\beta_2 \\: time^2_t + \\varepsilon_{ijt}. \\] Hint: Use I(Time^2) in the formula. Next try a fixed effect (FE) model given by \\[ weight_{ijt} = \\alpha_j + \\beta_1\\: time_t + \\beta_2 \\: time^2_t + \\alpha_i + \\varepsilon_{ijt} \\] where \\(\\alpha_i\\) is a fixed effect representing a fixed intercept for each Chick. You may be surprised by the result. Next try a random-effect (RE) model given by \\[ weight_{ijt} = \\alpha_j + \\beta_1\\: time_t + \\beta_2 \\: time^2_t + v_{it}, \\quad v_{it} = \\alpha_i + \\varepsilon_{ijt}\\] where \\(\\alpha_i\\) is a random effect representing a random intercept for each Chick assumed to be normally distributed. Use lmer() from lme4 package instead of lm() and replace variable Chick with random intercept (1 | Chick) in the formula. You probably observed that some of the above models yield very different results. Why? Are some of these models wrong? In fact, it could be that all of them are wrong. In the experiment, Diet types are probably randomly assigned across Chicks, and in that sense there is no obvious source of bias in the linear model construction. Thus, you should get similar results across models if your modeling approach is on the right track. Now go back to the initial plot of weight growth by chick and think of how else you could approach the problem. Did it occur to you that the weight probably started out about the same across Diet groups and then took different paths given the Diet type? Let’s try varying linear time trends with the shared initial average weight at \\(Time_t=0\\). \\[ weight_{ijt} = \\alpha_{0} + \\beta_{1j}\\: time_t + \\varepsilon_{ijt}\\] where \\(\\beta_{1j}\\) is a (fixed) Diet-specific linear time trend. Hint: use Diet*Time and -Diet in the formula to create the interaction terms between Time and Diet and suppress the fixed intercept of Diet. Now try \\[ weight_{ijt} = \\alpha_{0} + \\beta_{1j}\\: time_t + \\alpha_i + \\varepsilon_{ijt}\\] where \\(\\alpha_i\\) is a Chick fixed effect. Now try \\[ weight_{ijt} = \\alpha_{0} + \\beta_{1j}\\: time_t + v_{ijt}, \\quad v_{ijt} =\\alpha_i + \\varepsilon_{ijt}\\] where \\(\\alpha_i\\) is a Chick random effect. This time you should get pretty similar results across models in d, e, and f. How would you interpret the coefficient \\(\\beta_{1j}\\) of interaction terms between Time and Diet? Let’s visualize what’s going on. Try ggplot(aes(x=..., y=..., color=...)) + geom_jitter() + geom_smooth() to produce a scatter plot with smooth fitted curves by Diet types. Then, replace geom_smooth() with geom_smooth(method = &quot;lm&quot;, formula = y ~ x) for linear fit, followed by geom_smooth(method = &quot;lm&quot;, formula = y ~ x + I(x^2)) for quadratic fit. Do the plots make intuitive sense? Now try Diet-specific quadratic time trends; \\[ weight_{ijt} = \\alpha_{0} + \\beta_{1j}\\: time_t + \\beta_{2j} \\: time^2_t + \\varepsilon_{ijt}\\] where \\(\\alpha_0\\) is the common intercept across Diets. We may again suppress Diet-specific intercepts (using -Diet) by assuming that the average weight was the same across Diet groups at \\(Time = 0\\). Repeat this for the fixed effect and random effect models. How would you interpret the coefficient estimates on those quadratic time trends? To visualize the results from h in a stylistic manner, we will replace the data points with the 25th, 50th, and 75th percentiles of weight for each Diet and Time. Then, overlay the results from h on this plot. Hint: add variables to ChickWeight2 via group_by(Diet, Time) and mutate(varname = quantile(...)). Create a new ggplot(aes(x=..., y=..., ymin=..., ymax=..., color=...)) with geom_point(position = position_dodge(width = 1)) (adding points with y variable) and geom_linerange(position = position_dodge(width = 1) (adding vertical bars for 25th to 75th range with ymin and ymax variables). To overlay a regression result, use prev_data_plot + geom_smooth(aes( x = ..., y = predict(reg_model_name), color=...), formula = ...). The Key Click here Reflections To be written. If you are interested in bootstrapping random effects, here is an example. "],
["4-3-mixed.html", "4.3 Demo. Mixed Effects Models and LSMEANS", " 4.3 Demo. Mixed Effects Models and LSMEANS Mixed effects models employing fixed and random parameters are popular in the analysis of experimental data. In some disciplines, there is a strong tradition of summarizing results with “LSMEANS”, a SAS’s terminology. Here we will briefly demonstrate mixed effect estimation and lsmeans in R. library(dplyr) library(ggplot2) library(lme4) library(lsmeans) # make a copy of dataset ChickWeight2 &lt;- ChickWeight Mixed Effect Models Let \\(weight_{ijt}\\) be the weight of chick \\(i\\) in Diet group \\(j\\) observed in time \\(t\\). Consider modeling the effect of Diet on weight. (If you are new to R estimation syntax, reading this may be helpful.) Model 1 linear time trend \\(\\beta_{1}\\), Diet fixed effects \\(\\alpha_{j}\\), Chick random effects \\(u_i\\) \\[ weight_{ijt} = \\alpha_{j} + \\beta_{1}\\: time_t + v_{ijt}, \\:\\: v_{ijt} = u_i+ \\varepsilon_{ijt}\\] where $ v_{ijt}$ is a composite error term consisting of Chick random effects \\(u_i\\) and random error component \\(\\varepsilon_{ijt}\\). model_1 &lt;- ChickWeight2 %&gt;% with(lmer(weight ~ Diet + Time + (1 | Chick))) # summary including marginal effects (coefficients) summary(model_1) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: weight ~ Diet + Time + (1 | Chick) ## ## REML criterion at convergence: 5584 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.0591 -0.5779 -0.1182 0.4962 3.4515 ## ## Random effects: ## Groups Name Variance Std.Dev. ## Chick (Intercept) 525.4 22.92 ## Residual 799.4 28.27 ## Number of obs: 578, groups: Chick, 50 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 11.2438 5.7887 1.942 ## Diet2 16.2100 9.4643 1.713 ## Diet3 36.5433 9.4643 3.861 ## Diet4 30.0129 9.4708 3.169 ## Time 8.7172 0.1755 49.684 ## ## Correlation of Fixed Effects: ## (Intr) Diet2 Diet3 Diet4 ## Diet2 -0.550 ## Diet3 -0.550 0.339 ## Diet4 -0.550 0.339 0.339 ## Time -0.307 -0.015 -0.015 -0.011 # lsmeans: projected mean for each Diet lsmeans(model_1, specs=c(&quot;Diet&quot;)) ## Loading required namespace: lmerTest ## Diet lsmean SE df lower.CL upper.CL ## 1 104.6748 5.510541 47.38 93.59138 115.7582 ## 2 120.8848 7.694168 45.64 105.40941 136.3602 ## 3 141.2181 7.694168 45.64 125.74275 156.6935 ## 4 134.6877 7.702571 45.83 119.19541 150.1800 ## ## Degrees-of-freedom method: satterthwaite ## Confidence level used: 0.95 Model 2: discrete time fixed effects \\(\\beta_{t}\\), Diet fixed effects \\(\\alpha_{j}\\) and Chick random effects \\(u_i\\) \\[ weight_{ijt} = \\alpha_{j} + \\beta_{t} + v_{ijt}, \\:\\: v_{ijt} = u_i+ \\varepsilon_{ijt}\\] # add a factor time variable ChickWeight2 &lt;- ChickWeight2 %&gt;% mutate(Time_fac = as.factor(Time)) model_2 &lt;- ChickWeight2 %&gt;% with(lmer(weight ~ Diet + Time_fac + (1 | Chick))) # summary including marginal effects (coefficients) summary(model_2) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: weight ~ Diet + Time_fac + (1 | Chick) ## ## REML criterion at convergence: 5499.1 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.3235 -0.5158 -0.0179 0.4625 3.2968 ## ## Random effects: ## Groups Name Variance Std.Dev. ## Chick (Intercept) 523.3 22.87 ## Residual 770.2 27.75 ## Number of obs: 578, groups: Chick, 50 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 24.424 6.616 3.692 ## Diet2 16.310 9.427 1.730 ## Diet3 36.643 9.427 3.887 ## Diet4 30.230 9.434 3.205 ## Time_fac2 8.160 5.550 1.470 ## Time_fac4 18.660 5.587 3.340 ## Time_fac6 33.006 5.587 5.908 ## Time_fac8 49.945 5.587 8.940 ## Time_fac10 66.537 5.587 11.909 ## Time_fac12 87.945 5.587 15.741 ## Time_fac14 101.947 5.620 18.141 ## Time_fac16 125.667 5.653 22.229 ## Time_fac18 147.774 5.653 26.139 ## Time_fac20 167.253 5.687 29.407 ## Time_fac21 175.710 5.723 30.703 ## ## Correlation matrix not shown by default, as p = 15 &gt; 12. ## Use print(x, correlation=TRUE) or ## vcov(x) if you need it # lsmeans: projected mean for each Diet lsmeans(model_2, specs=c(&quot;Diet&quot;)) ## Diet lsmean SE df lower.CL upper.CL ## 1 106.3072 5.488820 47.51 95.26823 117.3461 ## 2 122.6167 7.664476 45.76 107.20212 138.0312 ## 3 142.9500 7.664476 45.76 127.53546 158.3645 ## 4 136.5373 7.672861 45.96 121.10587 151.9687 ## ## Results are averaged over the levels of: Time_fac ## Degrees-of-freedom method: satterthwaite ## Confidence level used: 0.95 # lsmeans: projected mean for each combination of Time_fac and Diet lsmeans(model_2, specs=c(&quot;Time_fac&quot;,&quot;Diet&quot;)) ## Time_fac Diet lsmean SE df lower.CL upper.CL ## 0 1 24.42351 6.615760 99.01 11.29643 37.55059 ## 2 1 32.58351 6.615760 99.01 19.45643 45.71059 ## 4 1 43.08306 6.669652 101.11 29.84904 56.31707 ## 6 1 57.43000 6.669652 101.11 44.19598 70.66401 ## 8 1 74.36877 6.669652 101.11 61.13476 87.60278 ## 10 1 90.96061 6.669652 101.11 77.72660 104.19462 ## 12 1 112.36877 6.669652 101.11 99.13476 125.60278 ## 14 1 126.37031 6.707010 103.14 113.06217 139.67845 ## 16 1 150.09078 6.744365 105.23 136.70852 163.47304 ## 18 1 172.19716 6.744365 105.23 158.81490 185.57942 ## 20 1 191.67605 6.768521 106.60 178.24586 205.10624 ## 21 1 200.13339 6.805510 108.76 186.62981 213.63698 ## 0 2 40.73301 8.543480 70.20 23.78093 57.68510 ## 2 2 48.89301 8.543480 70.20 31.94093 65.84510 ## 4 2 59.39256 8.555171 70.64 42.41728 76.36785 ## 6 2 73.73950 8.555171 70.64 56.76422 90.71479 ## 8 2 90.67828 8.555171 70.64 73.70299 107.65356 ## 10 2 107.27012 8.555171 70.64 90.29483 124.24540 ## 12 2 128.67828 8.555171 70.64 111.70299 145.65356 ## 14 2 142.67982 8.571485 71.16 125.67216 159.68747 ## 16 2 166.40029 8.588795 71.72 149.35829 183.44229 ## 18 2 188.50667 8.588795 71.72 171.46467 205.54867 ## 20 2 207.98556 8.607256 72.31 190.90693 225.06419 ## 21 2 216.44290 8.626723 72.94 199.32564 233.56016 ## 0 3 61.06635 8.543480 70.20 44.11426 78.01844 ## 2 3 69.22635 8.543480 70.20 52.27426 86.17844 ## 4 3 79.72590 8.555171 70.64 62.75061 96.70118 ## 6 3 94.07284 8.555171 70.64 77.09755 111.04812 ## 8 3 111.01161 8.555171 70.64 94.03633 127.98690 ## 10 3 127.60345 8.555171 70.64 110.62816 144.57873 ## 12 3 149.01161 8.555171 70.64 132.03633 165.98690 ## 14 3 163.01315 8.571485 71.16 146.00549 180.02080 ## 16 3 186.73362 8.588795 71.72 169.69162 203.77562 ## 18 3 208.84000 8.588795 71.72 191.79800 225.88200 ## 20 3 228.31889 8.607256 72.31 211.24026 245.39752 ## 21 3 236.77623 8.626723 72.94 219.65897 253.89349 ## 0 4 54.65362 8.547798 70.33 37.69297 71.61428 ## 2 4 62.81362 8.547798 70.33 45.85297 79.77428 ## 4 4 73.31317 8.559459 70.77 56.32938 90.29696 ## 6 4 87.66011 8.559459 70.77 70.67632 104.64390 ## 8 4 104.59889 8.559459 70.77 87.61509 121.58268 ## 10 4 121.19072 8.559459 70.77 104.20693 138.17452 ## 12 4 142.59889 8.559459 70.77 125.61509 159.58268 ## 14 4 156.60042 8.575756 71.29 139.58429 173.61655 ## 16 4 180.32089 8.593047 71.85 163.27046 197.37133 ## 18 4 202.42728 8.593047 71.85 185.37684 219.47772 ## 20 4 221.90616 8.630538 73.05 204.78134 239.03099 ## 21 4 230.36351 8.650333 73.70 213.19940 247.52761 ## ## Degrees-of-freedom method: satterthwaite ## Confidence level used: 0.95 Model 3: Diet fixed effects on liear-time growth rates \\(\\beta_{1j}\\), Chick random effects \\(u_i\\) \\[ weight_{ijt} = \\alpha_{0} + \\beta_{1j}\\: time_t + v_{ijt}, \\:\\: v_{ijt} = u_i+ \\varepsilon_{ijt}\\] Note: we assume common intercept \\(\\alpha_{0}\\) at Time=0 across Diet types. model_3 &lt;- ChickWeight2 %&gt;% with(lmer(weight ~ Diet*Time - Diet + (1 | Chick))) # summary including marginal effects (coefficients) summary(model_3) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: weight ~ Diet * Time - Diet + (1 | Chick) ## ## REML criterion at convergence: 5488 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.3226 -0.5908 -0.0699 0.5435 3.5918 ## ## Random effects: ## Groups Name Variance Std.Dev. ## Chick (Intercept) 532.9 23.09 ## Residual 643.0 25.36 ## Number of obs: 578, groups: Chick, 50 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 28.1852 3.8206 7.377 ## Time 6.7729 0.2435 27.818 ## Diet2:Time 1.8442 0.3857 4.782 ## Diet3:Time 4.4756 0.3857 11.605 ## Diet4:Time 2.9418 0.3906 7.532 ## ## Correlation of Fixed Effects: ## (Intr) Time Dt2:Tm Dt3:Tm ## Time -0.287 ## Diet2:Time 0.008 -0.581 ## Diet3:Time 0.008 -0.581 0.366 ## Diet4:Time 0.004 -0.573 0.361 0.361 # lsmeans: projected mean for each Diet at given Time values lsmeans(model_3, specs=c(&quot;Diet&quot;), at=list(Time=c(5, 10, 20))) ## Loading required namespace: lmerTest ## Diet Time lsmean SE df lower.CL upper.CL ## 1 5 62.04951 3.661222 60.89 54.72817 69.37084 ## 2 5 71.27029 3.808503 68.67 63.65444 78.88614 ## 3 5 84.42731 3.808503 68.67 76.81146 92.04317 ## 4 5 76.75832 3.814692 68.99 69.13009 84.38655 ## 1 10 95.91377 3.895676 71.79 88.12360 103.70394 ## 2 10 114.35534 4.399435 101.23 105.55780 123.15287 ## 3 10 140.66939 4.399435 101.23 131.87185 149.46692 ## 4 10 125.33140 4.432729 103.29 116.46729 134.19551 ## 1 20 163.64230 5.254660 164.97 153.13457 174.15003 ## 2 20 200.52543 6.624401 248.93 187.27864 213.77222 ## 3 20 253.15353 6.624401 248.93 239.90674 266.40033 ## 4 20 222.47756 6.728268 255.00 209.02306 235.93206 ## ## Degrees-of-freedom method: satterthwaite ## Confidence level used: 0.95 Model 4: Diet fixed effects on quadratic-time growth rates \\(\\beta_{1j}\\) and \\(\\beta_{2j}\\), Chick random effects \\(u_i\\) \\[ weight_{ijt} = \\alpha_{0} + \\beta_{1j}\\: time_t + \\beta_{2j}\\: time_t^2 + v_{ijt}, \\:\\: v_{ijt} = u_i+ \\varepsilon_{ijt}\\] Note: we assume common intercept \\(\\alpha_{0}\\) at Time=0 across Diet types. model_4 &lt;- ChickWeight2 %&gt;% with(lmer(weight ~ Diet:Time + Diet:I(Time^2) - Diet + (1 | Chick))) # summary including marginal effects (coefficients) summary(model_4) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: weight ~ Diet:Time + Diet:I(Time^2) - Diet + (1 | Chick) ## ## REML criterion at convergence: 5463.7 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -4.0129 -0.5185 -0.0110 0.5288 3.4598 ## ## Random effects: ## Groups Name Variance Std.Dev. ## Chick (Intercept) 522.4 22.86 ## Residual 600.6 24.51 ## Number of obs: 578, groups: Chick, 50 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 37.99451 4.14966 9.156 ## Diet1:Time 4.53465 0.85256 5.319 ## Diet2:Time 5.78814 1.12549 5.143 ## Diet3:Time 5.11572 1.12549 4.545 ## Diet4:Time 7.79434 1.13449 6.870 ## Diet1:I(Time^2) 0.10316 0.03995 2.583 ## Diet2:I(Time^2) 0.13111 0.05262 2.491 ## Diet3:I(Time^2) 0.29413 0.05262 5.589 ## Diet4:I(Time^2) 0.08703 0.05356 1.625 ## ## Correlation of Fixed Effects: ## (Intr) Dt1:Tm Dt2:Tm Dt3:Tm Dt4:Tm D1:I(T D2:I(T D3:I(T ## Diet1:Time -0.349 ## Diet2:Time -0.259 0.090 ## Diet3:Time -0.259 0.090 0.067 ## Diet4:Time -0.259 0.090 0.067 0.067 ## Dt1:I(Tm^2) 0.281 -0.961 -0.073 -0.073 -0.073 ## Dt2:I(Tm^2) 0.207 -0.072 -0.962 -0.053 -0.053 0.058 ## Dt3:I(Tm^2) 0.207 -0.072 -0.053 -0.962 -0.053 0.058 0.043 ## Dt4:I(Tm^2) 0.206 -0.072 -0.053 -0.053 -0.961 0.058 0.043 0.043 # lsmeans: projected mean for each Diet at given Time values lsmeans(model_4, specs=c(&quot;Diet&quot;), at=list(Time=c(5, 10, 20))) ## NOTE: Results may be misleading due to involvement in interactions ## Diet lsmean SE df lower.CL upper.CL ## 1 108.9524 4.349821 88.98 100.3094 117.5954 ## 2 128.4676 5.413575 131.34 117.7109 139.2243 ## 3 149.1506 5.413575 131.34 138.3939 159.9073 ## 4 144.1593 5.415879 131.77 133.3981 154.9206 ## ## Results are averaged over the levels of: Time ## Degrees-of-freedom method: satterthwaite ## Confidence level used: 0.95 Visualizing Model Fit Raw data # raw data plot base_plot &lt;- ChickWeight2 %&gt;% ggplot(aes( x = Time, y = weight, color = Diet)) + geom_jitter(size =.75, alpha=.5) base_plot Model 1 fit (approx.) # overlay model_1, approximated by y ~ x base_plot + geom_smooth( aes(x = Time, y = fitted.values(model_1), color = Diet), method=&#39;lm&#39;, formula = y ~ x, se=FALSE) Model 2 fit (approx.) # overlay model_2, approximated by loess base_plot + geom_smooth( aes(x = Time, y = fitted.values(model_2), color = Diet), se=FALSE) ## `geom_smooth()` using method = &#39;loess&#39; Model 3 fit (approx.) # overlay model_3, approximated by y ~ x base_plot + geom_smooth( aes(x = Time, y = fitted.values(model_3), color = Diet), method=&#39;lm&#39;, formula = y ~ x, se=FALSE) Model 4 fit (approx.) # overlay model_4, approximated by y ~ x + x^2 base_plot + geom_smooth( aes(x = Time, y = fitted.values(model_4), color = Diet), method=&#39;lm&#39;, formula = y ~ x + I(x^2), se=FALSE) "],
["5-resources.html", "5 Resources", " 5 Resources Here are more resources for learning R. Free Books Official CRAN R Manual Quick R The Art of R Programming by Norman Matloff ModernDive Impatient R Simple R by John Verzani Introduction to Probability and Statistics Using R By Jay Kerns R Wikibook Cookbook for R by Winston Chang OnePageR The R Inferno Videos Coursera’s four week course videos Workflow example video by Jermey Chacon Video on Youtube Tutorials Exploratory Data Analysis in R (Recommended) R Tutorial R Bootcamp - Jared Knowles Step-by-step (sequential) interactive tutorial- Try R Another step-by-step interactive tutorial - swirl With Small Fees: Tutorials from DataCamp Cleaning Data in R Data Manipulation in R with dplyr Data Visualization in R with ggvis Data Visualization with ggplot2 Introduction to Coding Coding Resources for Beginners by Tori Dykes Introduction to Shiny Excel Tools to Shiny (the author’s Shiny tutorial) "],
["references.html", "References", " References "]
]
