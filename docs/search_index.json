[
["index.html", "Piecemeal R Welcome", " Piecemeal R A Tutorial for Data Exploration with R Kota Minegishi Last updated: 2017-04-20 Welcome Welcome to a tutorial website for data analysis and visualization with R. This site provides a quick overview and topic-based tutorials in a piecemeal fashion. The site is organized based on two questions: How best to quickly introduce R to new audiences and showcase its data analytics tools? How best to provide tutorials for topic-based data applications with R? To answer the first question, Section 1 Introduction demonstrates a set of modern data analysis tools in R. Sections 2 describes essential concepts of R. For the second question, Section 3 provides topic-based tutorials. Additional resources are listed in Section 4. More content will be added when the author hosts a small workshop “Data Exploration with R” at his workplace. New Contents 2017-04-20: Test upload. VERY Preliminary! "],
["about.html", "About", " About Kota Minegishi is an assistant professor of Dairy Analytics at the University of Minnesota. He is an agricultural economist by training and works in the Department of Animal Science. Workshop dplyr and ggplot2 exercise 3.1: April 20, 5-6pm, Haecker 365. "],
["1-intro.html", "1 Introduction", " 1 Introduction 2017-04-20: VERY Preliminary! A Few Words from the Author R has come a long way in its evolution. Its download page looks pretty much unchanged from years ago but don’t be fooled by its archaic appearance. This piece of the past may be something to do with how the R developer community honors its legacy of turning an open-source project into one of the most popular data analytic tools of today. Please don’t mistake that archaic look as a sign of snobbishness–I hope you too will appreciate it some day. Welcome to the community. In what follows below, we assume that you have R and RStudio Desktop (free IDE) installed. It will be handy to have cheat sheets for base R, RStudio IDE, dplyr, and ggplot2 as well. If you find this introduction too technical, please start with ModernDive open-source textbook (say, up to Chapter 5). That book provided the initial inspiration for me to start this site. Also, more information on R is available in Section 2, as well as various sources listed in Section 4. "],
["1-1-materials.html", "1.1 Materials", " 1.1 Materials The power of R grows with each addition of user-contributed R packages, or a bundle of user-developed programs. Recent developments such as tidy, dplyr, and ggplot2 have greatly streamlined the coding for data manipulation and analysis, which is the starting point for learning R that is chosen for this site. With the R syntax system, you will learn the basic operations of data wrangling and visualization at a rapid pace given that it is an intuitive data operation language. Like any language, its grammar and framework provide a particular way of understanding the world. In this case, it will influence your thinking about data. Following the documentation of dplyr, let’s start with a sample dataset of airplane departures and arrivals. The dataset contains information on about 337,000 flights departed from New York City in 2013 (source: Bureau of Transportation Statistics). We load a built-in data frame by command library(nycflights13) where library(package_name) loads an R package named package_name in the current R session, or the computing environment. In the R console (i.e., the left bottom pane in RStudio), type install.packages(&quot;nycflights13&quot;) and hit enter. Generally, R packages are installed locally on your computer on an as-needed basis. To install several more packages that we will use, copy the following code and execute it in your R console. # Since we&#39;re just starting, don&#39;t worry about understanding the code here # &quot;#&quot;&quot; symbole is used to insert comments that are helpful to humans but are ignored by R required_pkgs &lt;- c(&quot;nycflights13&quot;, &quot;dplyr&quot;, &quot;ggplot2&quot;, &quot;lubridate&quot;, &quot;knitr&quot;, &quot;tidyr&quot;, &quot;broom&quot;) # creating a new object &quot;required_pkgs&quot; containing strings &quot;nycflights13&quot;, &quot;dplyr&quot;,.. # &quot;c()&quot; concatenates string names here. # &quot;&lt;-&quot; operator assigns from the object on the right to left new_pkgs &lt;- required_pkgs[!(required_pkgs %in% installed.packages())] # checking whether &quot;required_pkgs&quot; are already installed # &quot;[]&quot; of required_pkgs[ ] is extraction by logical TRUE or FALSE # &quot;%in%&quot; checks whether items on the left are members of the items on the right. # ! is a negation, so if the package is not installed, there will be an if (length(new_pkgs)) { install.packages(new_pkgs, repos = &quot;http://cran.rstudio.com&quot;) } Once packages are downloaded and installed on your computer, they become available for your libraries. In each R session, we load libraries we need (instead of all existing libraries). Here we load the following; suppressWarnings({ suppressMessages({ library(dplyr) # for data manipulation library(ggplot2) # for figures library(lubridate) # for date manipulation library(nycflights13) # sample data of NYC flights library(knitr) # for table formatting library(tidyr) # for table formatting library(broom) # for table formatting }) }) Let’s see the data. class(flights) # shows the class attribute dim(flights) # obtains dimention of rows and columns ## [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; ## [1] 336776 19 head(flights) # displays first seveal rows and columns ## # A tibble: 6 × 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 ## 2 2013 1 1 533 529 4 850 ## 3 2013 1 1 542 540 2 923 ## 4 2013 1 1 544 545 -1 1004 ## 5 2013 1 1 554 600 -6 812 ## 6 2013 1 1 554 558 -4 740 ## # ... with 12 more variables: sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, ## # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, ## # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, ## # time_hour &lt;dttm&gt; The dim() command returns the dimensions of a data frame, and the head() command returns the first several rows and columns. The flights dataset contains information on dates, actual departure and arrival times, scheduled departure and arrival times, carriers, origins, destinations, travel times, and distances. These variables are arranged in columns, and each row is an observation of flight. In R, we refer to a dataset as data frame, which is a class of R object. The data frame class is more general than the matrix class in that it can contain variables of more than one mode (numeric, character, factor etc). In case you want an overview of data types right away, here is a summary. "],
["1-2-arts-crafts.html", "1.2 Arts &amp; Crafts", " 1.2 Arts &amp; Crafts Crafts We will focus on six data wrangling functions in the dplyr package: filter(): extracts rows (e.g., observations) of a data frame. We put logical vectors in its arguments. select(): extracts columns (e.g., variables) of a data frame. We put column names in its arguments. arrange(): orders rows of a data frame. We put column names in its arguments. summarise(): collapses a data frame into summary statistics. We put summary functions (e.g., statistics functions) using column names in its arguments. mutate(): creates new variables and adds them to the existing columns. We put window functions (e.g., transforming operations) using column names in its arguments. group_by(): assigns rows into groups within a data frame. We put column names in its arguments. The very first argument in all these functions is a data frame, and by using this we can easily pipe a sequence of data wrangling operations through %&gt;% operator. The key is to start with a data frame and then formulate a sequence of data wrangling operations in plain English, which we can translate into code by replacing then in the sequence with the %&gt;% operator. Say, we want to find the average of delays in departures and arrivals from New York to the St. Paul-Minneapolis airport (MSP). We can construct the following sequence of instructions: take the flight data frame, apply filter() to extract the rows of flights to MSP, and then apply summarise() to calculate the mean. flights %&gt;% # take data frame &quot;flights&quot;, then filter(dest == &quot;MSP&quot;) %&gt;% # filter rows, then summarise( # summarise departure and arrival delays for their means # and call them mean_dep_delay and mean_arr_delay respectively mean_dep_delay = mean(dep_delay, na.rm = TRUE), mean_arr_delay = mean(arr_delay, na.rm = TRUE) ) # calculate the mean, while removing NA values ## # A tibble: 1 × 2 ## mean_dep_delay mean_arr_delay ## &lt;dbl&gt; &lt;dbl&gt; ## 1 13.32481 7.270169 In summarise(), one can use summary functions that takes a vector as an input and produces a scaler as an output. This includes functions like mean(), sd() (standard deviation), quantile(), min(), max(), and n() (observation count in the dplyr package). Each time we apply the %&gt;% operator above, we pass a modified data frame from one data operation to another through the first argument. The above code is equivalent to summarise( # data frame &quot;flights&quot; is inside filter(), which is inside summarise() filter(flights, dest == &quot;MSP&quot;), mean_dep_delay = mean(dep_delay, na.rm = TRUE), mean_arr_delay = mean(arr_delay, na.rm = TRUE) ) ## # A tibble: 1 × 2 ## mean_dep_delay mean_arr_delay ## &lt;dbl&gt; &lt;dbl&gt; ## 1 13.32481 7.270169 You will quickly discover that %&gt;% operator makes the code much easier to read, write, and edit and how that might inspire you want to play with the data more. Let’s add a few more lines to the previous example. Say, additionally we want to see the average delay by carrier and sort the results by the number of observations (e.g. flights) in descending order. Okay, what do we do? We make a sequence of data wrangling operations in plain English and translate that into code by replacing then with %&gt;% operator. For example, we say, “take the data frame flights; then (%&gt;%) filter() to extract the rows of flights to MSP; then (%&gt;%) group rows by carrier; then (%&gt;%) summarise() data for the number of observations and the means; then (%&gt;%) arrange() the results by the observation count in descending order.” flight_stats_MSP &lt;- flights %&gt;% # assign the results to an object named &quot;flight_stats&quot; filter(dest == &quot;MSP&quot;) %&gt;% group_by(carrier) %&gt;% # group rows by carrier summarise( n_obs = n(), # count number of rows mean_dep_delay = mean(dep_delay, na.rm = TRUE), mean_arr_delay = mean(arr_delay, na.rm = TRUE) ) %&gt;% arrange(desc(n_obs)) # sort by n_obs in descending order flight_stats_MSP # show flight_stats object ## # A tibble: 6 × 4 ## carrier n_obs mean_dep_delay mean_arr_delay ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 DL 2864 10.651392 4.035702 ## 2 EV 1773 17.093413 10.527995 ## 3 MQ 1293 8.255457 9.559350 ## 4 9E 1249 19.658113 8.089776 ## 5 OO 4 0.750000 -2.000000 ## 6 UA 2 -6.000000 -5.500000 The carrier variable is expressed in the International Air Transportation Association (IATA) code, so let’s add a column of carrier names by joining another data frame called airlines. In RStudio, you can find this data frame under the Environment tab (in the upper right corner); switch the display option from Global Environment to package:nycflights13. To inspect the data frame, type View(airlines) in the R console. Also, by typing data() you can see a list of all datasets that are loaded with libraries. left_join(flight_stats_MSP, airlines, by=&quot;carrier&quot;) %&gt;% # left_join(a,b, by=&quot;var&quot;) joins two data frames a, b by matching rows of b to a # by identifier variable &quot;var&quot;. kable(digits=2) # kable() prints a better-looking table here carrier n_obs mean_dep_delay mean_arr_delay name DL 2864 10.65 4.04 Delta Air Lines Inc. EV 1773 17.09 10.53 ExpressJet Airlines Inc. MQ 1293 8.26 9.56 Envoy Air 9E 1249 19.66 8.09 Endeavor Air Inc. OO 4 0.75 -2.00 SkyWest Airlines Inc. UA 2 -6.00 -5.50 United Air Lines Inc. In the next example, we add new variables to flights using mutate(). flights %&gt;% # keep only columns named &quot;dep_delay&quot; and &quot;arr_delay&quot; select(dep_delay, arr_delay) %&gt;% mutate( gain = arr_delay - dep_delay, gain_rank = round(percent_rank(gain), digits = 2) # Note: we can immediately use the &quot;gain&quot; variable we just defined. ) ## # A tibble: 336,776 × 4 ## dep_delay arr_delay gain gain_rank ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2 11 9 0.81 ## 2 4 20 16 0.88 ## 3 2 33 31 0.94 ## 4 -1 -18 -17 0.22 ## 5 -6 -25 -19 0.18 ## 6 -4 12 16 0.88 ## 7 -5 19 24 0.92 ## 8 -3 -14 -11 0.37 ## 9 -3 -8 -5 0.54 ## 10 -2 8 10 0.82 ## # ... with 336,766 more rows We extracted specific columns of flights by select() and added new columns defined in mutate(). mutate() differs from summarise() in that mutate() adds new columns to the data frame, while summarise() collapses the data frame into a summary table. There are roughly five types of window functions that are commonly used inside mutate(): (1) summary functions, which are interpreted as a vector of repeated values (e.g., a column of an identical mean value): (2) ranking or ordering functions (e.g., row_number(), min_rank(), dense_rank(), cume_dist(), percent_rank(), and ntile()): (3) offset functions, say defining a lagged variable in time series data (lead() and lag()): (4) cumulative aggregates (e.g., cumsum(), cummin(), cummax(), cumall(), cumany(), and cummean()): (5) fixed-window rolling aggregates such as a windowed mean, median, etc. To find help files for these function, for example, type ?cumsum. Before moving to the graphics, let’s quickly go over what a function is in R and how you can use a custom function inside summarise() or mutate(). In R, we use function() to create a function, which has its name, input arguments separated by comma, and a body (e.g., tasks to perform and what to return as an output). your_function_name &lt;- function(input arguments) { task1 task2 . . . output_to_return } For a function having only a single expression to execute, we can omit brackets { }. another_function &lt;- function(input args) task_and_output_in_a_single_expression Let’s go through a few examples. # generate a sequence from 1 to 10 (by the increment of 1) and name it &quot;vec1&quot;. vec1 &lt;- 1:10 vec1 ## [1] 1 2 3 4 5 6 7 8 9 10 # c() concatenates vec2 &lt;- c(vec1, NA, NA) vec2 ## [1] 1 2 3 4 5 6 7 8 9 10 NA NA my_mean_1 &lt;- function(x) mean(x, na.rm = TRUE) # Input arguments: x # Output: the calculation result of mean(x, na.rm = TRUE). # x is required by mean() (and implicitly assumed to be a vector of numeric values). # mean() is an existing function. The &quot;na.rm&quot; argument of mean() is set to be TRUE. my_mean_1(vec1) ## [1] 5.5 my_mean_2 &lt;- function(x, na.rm=TRUE) mean(x, na.rm = na.rm) # Input arguments: x and na.rm (optional with the default value of TRUE) # Output: the calculation result of mean(x, na.rm = na.rm). # The input argument &quot;na.rm&quot; is passed to the input argument &quot;na.rm&quot; of mean() my_mean_2(vec2) ## [1] 5.5 my_mean_2(vec2, na.rm=FALSE) # not removing NA returns NA for the mean calculation. ## [1] NA my_zscore &lt;- function(x, remove_na=TRUE) { (x - my_mean_2(x, na.rm = remove_na))/sd(x, na.rm = remove_na) } # Inputs: x and remove_na (optional: default = TRUE) # Output: z-score of vector x # my_mean2() and sd() return scalers but are interpreted # as a vector of repeated valuses that has the same length as x. my_zscore(vec1) %&gt;% round(2) ## [1] -1.49 -1.16 -0.83 -0.50 -0.17 0.17 0.50 0.83 1.16 1.49 Let’s apply functionsmy_mean_2() and my_zscore() in summarise() and mutate(). flights %&gt;% select(dep_delay) %&gt;% summarise( mean_dep_delay = my_mean_2(dep_delay), # using my_mean_2() mean_dep_delay_na = my_mean_2(dep_delay, na.rm = FALSE) # this returns NA ) %&gt;% kable(digits=2) mean_dep_delay mean_dep_delay_na 12.64 NA flights_gain &lt;- flights %&gt;% select(dep_delay, arr_delay) %&gt;% mutate( gain = arr_delay - dep_delay, gain_z = (gain - my_mean_2(gain))/sd(gain, na.rm=TRUE), # using my_mean_2() gain_z2 = my_zscore(gain_z) # using my_zscore() ) head(flights_gain) %&gt;% # show the first several rows kable(digits=2) dep_delay arr_delay gain gain_z gain_z2 2 11 9 0.81 0.81 4 20 16 1.20 1.20 2 33 31 2.03 2.03 -1 -18 -17 -0.63 -0.63 -6 -25 -19 -0.74 -0.74 -4 12 16 1.20 1.20 Creating a function spares us from writing similar codes in multiple places. While avoiding such repetition is important for making reading and editing code easier, it also reduces coding errors. A situation where you may consider using custom functions is inside functions like summarise_each() and mutate_each(). The two functions allow for applying summary functions like mean() or sd() to each column in a data frame. summarise_each() and mutate_each() work by calling a function by its name. They are very easy to use when an operation is to summarize a vector into a statistics without needing to specify additional arguments, say mean(var1). However, providing additional arguments into a function, say mean(var1, na.rm=TRUE), becomes somewhat cumbersome in terms of its syntax. One approach to get around this problem is to pre-process the data frame before getting to a summarise_each() or mutate_each() section. For example, if we want to test the argument na.rm=TRUE to mean(), we can first filter out rows that contain missing values (NA) and then apply summarise_each(). flights_gain %&gt;% select(dep_delay, arr_delay, gain) %&gt;% filter(!is.na(dep_delay) &amp; !is.na(arr_delay)) %&gt;% # filter out rows that have NA values in dep_delay or arr_deplay summarise_each(&quot;mean&quot;) %&gt;% kable(digits=2) dep_delay arr_delay gain 12.56 6.9 -5.66 The other approach is to use a custom function. For instance, my_mean_2() we defined above has the default argument na.rm=TRUE that gets passed into mean(), effectively overwriting the default argument na.rm=FALSE of mean(). A custom function (as well as any standard summary function) can be called in summarise_each() or mutate_each() using funs() flights_gain %&gt;% select(dep_delay, arr_delay, gain) %&gt;% summarise_each(funs(&quot;my_mean_2&quot;)) %&gt;% kable(digits=2) dep_delay arr_delay gain 12.64 6.9 -5.66 Being able to use your own functions in dplyr-style data wrangling operations will greatly enhance your ability to quickly analyze data in R. Arts Now we will cover the basics of data visualization via the ggplot2 package. The ggplot2 syntax has three essential components for generating graphics: data, aes, and geom. This implements the following philosophy (a quote mentioned in ModernDive); A statistical graphic is a mapping of data variables to aesthetic attributes of geometric objects. — (Wilkinson 2005) While coding complex graphics via ggplot() may appear intimidating at first, it boils down to the three primary components: data: a data frame e.g., the first argument in ggplot(data, ...). geom: geometric objects such as points, lines, bars, etc. with parameters given in the (), e.g., geom_point(), geom_line(), geom_histogram() aes: specifications for x-y variables, as well as variables to differentiate geom objects by color , shape, or size. e.g., aes(x = var_x, y = var_y, shape = var_z) One can refine a plot figure by adding secondary components or characteristics such as stat: data transformation, overlay of statistical inferences etc. scales: scaling data points etc. coord: Cartesian coordinates, polar coordinates, mapping projections etc. facet: laying out multiple plot panels in a grid etc. In what follows below, we will generate five common types of plots: scatter-plots, line-graphs, boxplots, histograms, and barplots. To provide a context, let’s use these plots to investigate what may explain patterns of flight departure delays. First, let’s consider the possibility of congestion at an airport during certain times of the day or certain seasons. We can use barplots to see whether there is any obvious pattern in the flight distribution across flight origins (i.e., airports) in New York City with St. Paul-Minneapolis airport (MSP) as a destination. A barplot shows observation counts (e.g., rows) by category. ggplot(data = flights, # the first argument is the data frame mapping = aes(x = origin)) + # the second argument is &quot;mapping&quot;, which is aes() geom_bar() # after &quot;+&quot; piping operator of ggplot(), we add geom_XXX() elements We can make the plot more informative and aesthetic. ggplot(data = flights, mapping = aes(x = origin, fill = origin)) + # here &quot;fill&quot; gives bars distinct colors geom_bar() + facet_wrap( ~ hour) # &quot;facet_wrap( ~ var)&quot; generates a grid of plots by var Another way to see the same information is a histogram. flights %&gt;% filter(hour &gt;= 5) %&gt;% # exclude hour earlier than 5 a.m. ggplot(aes(x = hour, fill = origin)) + geom_histogram(binwidth = 1, color = &quot;white&quot;) While mornings and late afternoons tend to get busy, there is not much difference in the number of flights across airports. Let’s see if there are distinct patters of departure delays over the course of a year. We do this by taking the average of departure delays for each day by flight origin and plot the data as a time series using line-graphs. delay_day &lt;- flights %&gt;% group_by(origin, year, month, day) %&gt;% summarise(dep_delay = mean(dep_delay, na.rm = TRUE)) %&gt;% mutate(date = as.Date(paste(year, month, day), format=&quot;%Y %m %d&quot;)) %&gt;% filter(!is.na(dep_delay)) # exclude rows with dep_delay == NA delay_day %&gt;% # &quot;facet_grid( var ~ .)&quot; is similar to &quot;facet_wrap( ~ var)&quot; ggplot(aes(x = date, y = dep_delay)) + geom_line() + facet_grid( origin ~ . ) The seasonal pattern seems similar across airports, and summer months appear to be busier on average. Across these airports, let’s see how closely these patterns are related to each other by focusing on a few summer months and making an overlap of the three line-graphs (EWR, JFK, and LGA). delay_day %&gt;% filter(&quot;2013-07-01&quot; &lt;= date, &quot;2013-08-31&quot; &gt;= date) %&gt;% ggplot(aes(x = date, y = dep_delay, color = origin)) + geom_line() We can see similar patterns of spikes across airports occurring on certain days, indicating a tendency for the three airports to get busy on the same days. Would this mean that the three airports tend to be congested at the same time? In the previous figure, there seems to be some cyclical pattern of delays. A good place to start would be comparing delays by day of the week. Here is a function to calculate day of the week for a given date. my_dow &lt;- function(date) { # as.POSIXlt(date)[[&#39;wday&#39;]] returns integers 0, 1, 2, .. 6, for Sun, Mon, ... Sat. # We extract one item from a vector (Sun, Mon, ..., Sat) by position numbered from 1 to 7. dow &lt;- as.POSIXlt(date)[[&#39;wday&#39;]] + 1 c(&quot;Sun&quot;, &quot;Mon&quot;, &quot;Tue&quot;, &quot;Wed&quot;, &quot;Thu&quot;, &quot;Fri&quot;, &quot;Sat&quot;)[dow] # extract &quot;dow&quot;-th element } # Input: date in the format as in &quot;2017-01-23&quot; # Output: day of week Sys.Date() # Sys.Date() returns the current date ## [1] &quot;2017-04-08&quot; my_dow(Sys.Date()) ## [1] &quot;Sat&quot; Now, let’s take a look at the mean delay by day of the week using boxplots. delay_day &lt;- flights %&gt;% group_by(year, month, day) %&gt;% summarise(dep_delay = mean(dep_delay, na.rm = TRUE)) %&gt;% mutate(date = as.Date(paste(year, month, day), format=&quot;%Y %m %d&quot;), # date defined by as.Data() function wday = my_dow(date), weekend = wday %in% c(&quot;Sat&quot;, &quot;Sun&quot;) # %in% operator: A %in% B returns TRUE/FALSE for whether each element of A is in B. ) # show the first 10 elements of &quot;wday&quot; variable in &quot;delay_day&quot; data frame delay_day$wday[1:10] ## [1] &quot;Tue&quot; &quot;Wed&quot; &quot;Thu&quot; &quot;Fri&quot; &quot;Sat&quot; &quot;Sun&quot; &quot;Mon&quot; &quot;Tue&quot; &quot;Wed&quot; &quot;Thu&quot; delay_day$wday &lt;- ordered(delay_day$wday, levels = c(&quot;Mon&quot;, &quot;Tue&quot;, &quot;Wed&quot;, &quot;Thu&quot;, &quot;Fri&quot;, &quot;Sat&quot;, &quot;Sun&quot;)) # adding a sorting order (Mon, Tue, ..., Sun) delay_day$wday[1:10] ## [1] Tue Wed Thu Fri Sat Sun Mon Tue Wed Thu ## Levels: Mon &lt; Tue &lt; Wed &lt; Thu &lt; Fri &lt; Sat &lt; Sun delay_day %&gt;% filter(!is.na(dep_delay)) %&gt;% ggplot(aes(x = wday, y = dep_delay, fill = weekend)) + geom_boxplot() It appears that delays are on average longer on Thursdays and Fridays and shorter on Saturdays. This is plausible if more people are traveling on Thursdays and Fridays before the weekend, and less are traveling on Saturdays to enjoy the weekend. Are Saturdays really less busy? Let’s find out. flights_wday &lt;- flights %&gt;% mutate(date = as.Date(paste(year, month, day), format=&quot;%Y %m %d&quot;), wday = ordered(my_dow(date), levels = c(&quot;Mon&quot;, &quot;Tue&quot;, &quot;Wed&quot;, &quot;Thu&quot;, &quot;Fri&quot;, &quot;Sat&quot;, &quot;Sun&quot;)), weekend = wday %in% c(&quot;Sat&quot;, &quot;Sun&quot;) ) flights_wday %&gt;% group_by(wday) %&gt;% summarise( nobs = n() ) ## # A tibble: 7 × 2 ## wday nobs ## &lt;ord&gt; &lt;int&gt; ## 1 Mon 50690 ## 2 Tue 50422 ## 3 Wed 50060 ## 4 Thu 50219 ## 5 Fri 50308 ## 6 Sat 38720 ## 7 Sun 46357 flights_wday %&gt;% ggplot(aes(x = wday)) + geom_bar() Yes, Saturdays are less busy for the airports in terms of flight numbers. Could we generalize this positive relationship between the number of flights and the average delays, which we find across days of the week? To investigate this, we can summarize the data into the average delays by date-hour and see if the busyness of a particular hour of a particular day is correlated with the mean delay. We visualize these data using a scatter plot. delay_day_hr &lt;- flights %&gt;% group_by(year, month, day, hour) %&gt;% # grouping by date-hour summarise( n_obs = n(), dep_delay = mean(dep_delay, na.rm = TRUE) ) %&gt;% mutate(date = as.Date(paste(year, month, day), format=&quot;%Y %m %d&quot;), wday = my_dow(date) ) plot_delay &lt;- delay_day_hr %&gt;% filter(!is.na(dep_delay)) %&gt;% ggplot(aes(x = n_obs, y = dep_delay)) + geom_point(alpha = 0.1) # plot of n_obs and the average dep_delay # where each point represents an date-hour average # &quot;alpha = 0.1&quot; controls the degree of transparency of points plot_delay Along the horizontal axis, we can see how the number of flights is distributed across date-hours. Some days are busy, and some hours busier still. It appears that there are two clusters in the number of flights, showing very slow date-hours (e.g., less than 10 flights flying out of New York city per hour) and normal date-hours (e.g., about 50 to 70 flights per hour). We could guess that the delays in the slow hours are caused by bad weather. On the other hand, we may wonder if the excess delays in the normal hours, compared to the slow hours, are caused by congestion at the airports. To see this, let’s fit a curve that captures the relationships between n_obs and dep_delay. Our hypothesis is that the delay would become more likely and longer as the number of flights increases. plot_delay + geom_smooth() # geom_smooth() addes a layer of fitted curve(s) ## `geom_smooth()` using method = &#39;gam&#39; We cannot see any clear pattern. How about fitting a curve by day of the week? plot_delay + # additional aes() argument for applying different colors to the day of the week geom_smooth(aes(color = wday), se=FALSE) ## `geom_smooth()` using method = &#39;gam&#39; Surprisingly, the delay does not seem to increase with the flights. There are more delays on Thursdays and Fridays and less delays on Saturdays, but we see no evidence of flight congestion as a cause of delay. Let’s take a closer look at the distribution of the delays. If it is not normally distributed, we may want to apply a transformation. delay_day_hr %&gt;% filter(!is.na(dep_delay)) %&gt;% ggplot(aes(x = dep_delay)) + geom_histogram(color = &quot;white&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. The distribution of the average delays are greatly skewed. In applying a logarithmic transformation, here we have to shift the variable so that its minimum is greater than zero. # define new column called &quot;dep_delay_shifted&quot; delay_day_hr$dep_delay_shifted &lt;- delay_day_hr %&gt;% with(dep_delay - min(dep_delay, na.rm = TRUE) + 1) # with() function takes a data frame in the first argument and allows for # referencing its variable names. delay_day_hr %&gt;% ungroup() %&gt;% # removing group_by() attribute select(dep_delay, dep_delay_shifted) %&gt;% with( apply(., 2, summary) # apply(data, num, fun) applies function &quot;fun&quot; for each item # in dimension &quot;num&quot; (1 = cows, 2= columns) of the data frame # Data referenced by &quot;.&quot; means all variables of the dataset inside with(). ) %&gt;% t() # transpose rows and columns ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## dep_delay -18 1.054 6.571 12.99 15.44 269 13 ## dep_delay_shifted 1 20.050 25.570 31.99 34.44 288 13 Now the transformed distribution; # Under the log of 10 transformation, the distribution looks closer to a normal distribution. delay_day_hr %&gt;% filter(!is.na(dep_delay_shifted)) %&gt;% ggplot(aes(x = dep_delay_shifted)) + scale_x_log10() + geom_histogram(color = &quot;white&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. # Alternatively, one can apply the natural logarithm to transform a variable. Histogram shows no difference here. delay_day_hr %&gt;% filter(!is.na(dep_delay_shifted)) %&gt;% ggplot(aes(x = dep_delay_shifted)) + scale_x_continuous(trans = &quot;log&quot;) + geom_histogram(color = &quot;white&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. The transformed distribution is much less skewed than the original. Now, let’s plot the relationship between delays and flights again. delay_day_hr %&gt;% filter(!is.na(dep_delay_shifted), dep_delay_shifted &gt; 5) %&gt;% ggplot(aes(x = n_obs, y = dep_delay_shifted)) + scale_y_log10() + # using transformation scale_y_log10() geom_point(alpha = 0.1) + geom_smooth() ## `geom_smooth()` using method = &#39;gam&#39; We still do not see a pattern that busier hours have more delays. This seems to suggest that the airports in New York City manage the fluctuating number of flights without causing congestion. References "],
["1-3-huning-down-numbers.html", "1.3 Huning down numbers", " 1.3 Huning down numbers This section is optional but contains more examples of dplyr and ggplot2 functions. Previously, we find that the congestion at the airports is unlikely the cause of delays. Then, what else may explain the patterns of delays? Are the airlines partly responsible? Recall that earlier we observe that some airlines have longer delays than others for NYC-MSP flights. Let’s take a look at the overall average delays by carrier. stat_carrier &lt;- flights %&gt;% group_by(carrier) %&gt;% summarise(n_obs = n(), dep_delay = mean(dep_delay, na.rm = TRUE), arr_delay = mean(arr_delay, na.rm = TRUE) ) %&gt;% left_join(airlines, by=&quot;carrier&quot;) %&gt;% arrange(desc(n_obs)) stat_carrier %&gt;% kable(digit=2) carrier n_obs dep_delay arr_delay name UA 58665 12.11 3.56 United Air Lines Inc. B6 54635 13.02 9.46 JetBlue Airways EV 54173 19.96 15.80 ExpressJet Airlines Inc. DL 48110 9.26 1.64 Delta Air Lines Inc. AA 32729 8.59 0.36 American Airlines Inc. MQ 26397 10.55 10.77 Envoy Air US 20536 3.78 2.13 US Airways Inc. 9E 18460 16.73 7.38 Endeavor Air Inc. WN 12275 17.71 9.65 Southwest Airlines Co. VX 5162 12.87 1.76 Virgin America FL 3260 18.73 20.12 AirTran Airways Corporation AS 714 5.80 -9.93 Alaska Airlines Inc. F9 685 20.22 21.92 Frontier Airlines Inc. YV 601 19.00 15.56 Mesa Airlines Inc. HA 342 4.90 -6.92 Hawaiian Airlines Inc. OO 32 12.59 11.93 SkyWest Airlines Inc. There could be some differences across carriers. However, the simple average of delays across various routes, days, and hours of flights may not be a good measure to compare the carriers. For example, some carriers may serve the routes and hours that tend to have more delays. Also, given that our dataset covers only the flights from New York City, the comparison may not be nationally representative since carriers use different airports around the country for their regional hubs. For our purposes, let’s compare the average air time among carriers, while accounting for flight’s destination and timing. The differences in air time are not the same as the differences in delays, but they may indicate some efficiency difference among carriers. Let’s first check how air time relates to flight distance. flights %&gt;% filter (month == 1, day == 1, !is.na(air_time)) %&gt;% ggplot(aes(x = distance, y = air_time)) + geom_point(alpha = 0.05) + geom_smooth() ## `geom_smooth()` using method = &#39;loess&#39; air_time and distance show a general linear relationship. We can better account for this relationship if we calculate the average air time for each flight destination from New York City. First, we will consider a simple approach to control for such average air time for each destination and compare the variation in air time among carriers. We can do this by fitting a linear regression model with fixed destination effects and comparing the residuals. This resembles the ANOVA for comparing the mean air times among carriers, but the fixed destination effects here difference out the average air time for each destination from the total variation. # a copy of flights data flights2 &lt;- flights # TRUE/FALSE vector showing whther air_time is not NA. idx0 &lt;- flights %&gt;% with(!is.na(air_time)) flights2$res &lt;- NA # prepare a column of residuals to be defined below flights2$res[idx0] &lt;- flights2 %&gt;% # replace rows with idx0 = TRUE filter(!is.na(air_time)) %&gt;% with( lm( air_time ~ as.factor(dest)) # lm() estimates a linear model. # &quot;y ~ x&quot;&quot; is the formula for regressing y on x. # as.factor() converts &quot;dest&quot; to a factor (categorical) class # which is used as a set of dummy variables in the regression. ) %&gt;% residuals() # obtains residuals of the lm() object stat_res &lt;- flights2 %&gt;% group_by(carrier) %&gt;% summarise( mean_res = mean(res, na.rm = TRUE), # mean residual by carrier sd_res = sd(res, na.rm = TRUE) ) left_join(stat_carrier, stat_res, by=&quot;carrier&quot;) %&gt;% kable(digit=2) carrier n_obs dep_delay arr_delay name mean_res sd_res UA 58665 12.11 3.56 United Air Lines Inc. -0.87 14.59 B6 54635 13.02 9.46 JetBlue Airways 0.28 11.55 EV 54173 19.96 15.80 ExpressJet Airlines Inc. -0.37 8.94 DL 48110 9.26 1.64 Delta Air Lines Inc. -0.20 12.32 AA 32729 8.59 0.36 American Airlines Inc. 0.68 13.86 MQ 26397 10.55 10.77 Envoy Air 0.45 8.87 US 20536 3.78 2.13 US Airways Inc. -0.42 9.43 9E 18460 16.73 7.38 Endeavor Air Inc. 0.84 8.76 WN 12275 17.71 9.65 Southwest Airlines Co. 0.16 12.55 VX 5162 12.87 1.76 Virgin America 3.26 17.58 FL 3260 18.73 20.12 AirTran Airways Corporation 1.16 8.75 AS 714 5.80 -9.93 Alaska Airlines Inc. -2.13 16.17 F9 685 20.22 21.92 Frontier Airlines Inc. 3.12 15.16 YV 601 19.00 15.56 Mesa Airlines Inc. -0.05 7.06 HA 342 4.90 -6.92 Hawaiian Airlines Inc. 5.64 20.69 OO 32 12.59 11.93 SkyWest Airlines Inc. 1.02 7.26 The differences in air time across carriers (“mean_res”) somewhat differ from the patterns of differences in the simple averages of delays (“dep_delay” and “arr_delay”). The patterns are different between “dep_delay” and “arr_delay” for that matter. To some extent, it appears to make sense that the average air time is longer for low-cost carriers such as Virgin America, Frontier Airlines, and Hawaiian Airlines. The differences across other carriers, on the other hand, are small, compared to the standard deviations. To get a sense of whether these differences have any statistical significance, let’s use t-test to compare the mean residual between United Airlines and American Airlines. # t-test comparing UA vs AA for the mean air time flights2 %&gt;% with({ idx_UA &lt;- carrier == &quot;UA&quot; idx_AA &lt;- carrier == &quot;AA&quot; t.test(res[idx_UA], res[idx_AA]) }) ## ## Welch Two Sample t-test ## ## data: res[idx_UA] and res[idx_AA] ## t = -15.722, df = 68826, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -1.741133 -1.355142 ## sample estimates: ## mean of x mean of y ## -0.8689523 0.6791852 With a large number of observations, a seemingly small difference in the means often turns out to be a statistically significant difference. Nonetheless, statistical significance is not sufficient for being an empirically significant difference that matters in the real world. The average difference of about 1.5 minute air time per flight appears very small. In fact, we can do this sort of pair-wise comparison all at once using a regression. Using carrier fixed effects in addition to destination fixed effects, we can directly compare the mean effects across carriers. We will set United Airlines to be a reference of the carrier fixed effects, so that the fixed effect for United Airlines is set to zero (i.e., omitted category), from which the fixed effects of all other airlines are estimated. flights2$carrier &lt;- relevel(factor(flights2$carrier), ref=&quot;UA&quot;) # reference level is United Airlines flights2$carrier %&gt;% table() ## . ## UA 9E AA AS B6 DL EV F9 FL HA MQ OO ## 58665 18460 32729 714 54635 48110 54173 685 3260 342 26397 32 ## US VX WN YV ## 20536 5162 12275 601 flights2 %&gt;% with({ n_carrier &lt;- unique(carrier) %&gt;% length() n_dest &lt;- unique(dest) %&gt;% length() print(paste(&#39;There are&#39;, n_carrier, &#39;distinct carriers and&#39;, n_dest,&#39;distinct destinations in the data.&#39; )) }) ## [1] &quot;There are 16 distinct carriers and 105 distinct destinations in the data.&quot; With 16 carriers and 105 destinations minus 2 reference levels for carriers and destinations, the total of 119 coefficients will be estimated for the fixed effects. f1 &lt;- flights2 %&gt;% with( lm( air_time ~ as.factor(carrier) + as.factor(dest) ) # fixed effects for carriers and destinations ) tidy(f1)[1:20,] # show the first 20 coefficients ## term estimate std.error statistic p.value ## 1 (Intercept) 247.9884874 0.75069658 330.3445016 0.000000e+00 ## 2 as.factor(carrier)9E 1.8015498 0.12723996 14.1586788 1.702649e-45 ## 3 as.factor(carrier)AA 1.9326712 0.09731105 19.8607572 1.002388e-87 ## 4 as.factor(carrier)AS -1.9071536 0.49596319 -3.8453531 1.204017e-04 ## 5 as.factor(carrier)B6 1.1808039 0.08495098 13.8998267 6.535025e-44 ## 6 as.factor(carrier)DL 0.7531812 0.08722600 8.6348244 5.907432e-18 ## 7 as.factor(carrier)EV 0.4174574 0.11044837 3.7796605 1.570702e-04 ## 8 as.factor(carrier)F9 3.8891981 0.48090201 8.0872985 6.120836e-16 ## 9 as.factor(carrier)FL 2.6434074 0.27600661 9.5773336 1.002386e-21 ## 10 as.factor(carrier)HA 11.0125104 0.89821710 12.2604106 1.503557e-34 ## 11 as.factor(carrier)MQ 1.4592669 0.11892133 12.2708590 1.321669e-34 ## 12 as.factor(carrier)OO 1.8091432 2.21222472 0.8177936 4.134757e-01 ## 13 as.factor(carrier)US 0.1319337 0.13826299 0.9542230 3.399715e-01 ## 14 as.factor(carrier)VX 4.5298528 0.18441295 24.5636378 4.086448e-133 ## 15 as.factor(carrier)WN 1.2226161 0.17520980 6.9780125 2.999500e-12 ## 16 as.factor(carrier)YV 0.5167461 0.52737831 0.9798395 3.271661e-01 ## 17 as.factor(dest)ACK -207.1011095 1.04478912 -198.2228803 0.000000e+00 ## 18 as.factor(dest)ALB -216.6188634 0.95130943 -227.7059972 0.000000e+00 ## 19 as.factor(dest)ANC 165.1365126 4.26930687 38.6799351 0.000000e+00 ## 20 as.factor(dest)ATL -136.1282095 0.75641976 -179.9638460 0.000000e+00 # a function to clean up the coefficient table above clean_lm_rlt &lt;- function(f) { # keep only rows for which column &quot;term&quot; contains &quot;carrier&quot; e.g., rows 2 to 16 above rlt &lt;- tidy(f) %&gt;% filter(grepl(&quot;carrier&quot;,term)) # create column named carrier rlt &lt;- rlt %&gt;% mutate(carrier = gsub(&#39;as.factor\\\\(carrier\\\\)&#39;,&#39;&#39;, term)) # drop column term rlt &lt;- rlt %&gt;% select(-term) # add columns of carrier, name, and n_obs from the stat_carrier data frame stat_carrier %&gt;% select(carrier, name, n_obs) %&gt;% left_join(rlt, by=&quot;carrier&quot;) } lm_rlt1 &lt;- clean_lm_rlt(f1) lm_rlt1 %&gt;% kable(digit=2) carrier name n_obs estimate std.error statistic p.value UA United Air Lines Inc. 58665 NA NA NA NA B6 JetBlue Airways 54635 1.18 0.08 13.90 0.00 EV ExpressJet Airlines Inc. 54173 0.42 0.11 3.78 0.00 DL Delta Air Lines Inc. 48110 0.75 0.09 8.63 0.00 AA American Airlines Inc. 32729 1.93 0.10 19.86 0.00 MQ Envoy Air 26397 1.46 0.12 12.27 0.00 US US Airways Inc. 20536 0.13 0.14 0.95 0.34 9E Endeavor Air Inc. 18460 1.80 0.13 14.16 0.00 WN Southwest Airlines Co. 12275 1.22 0.18 6.98 0.00 VX Virgin America 5162 4.53 0.18 24.56 0.00 FL AirTran Airways Corporation 3260 2.64 0.28 9.58 0.00 AS Alaska Airlines Inc. 714 -1.91 0.50 -3.85 0.00 F9 Frontier Airlines Inc. 685 3.89 0.48 8.09 0.00 YV Mesa Airlines Inc. 601 0.52 0.53 0.98 0.33 HA Hawaiian Airlines Inc. 342 11.01 0.90 12.26 0.00 OO SkyWest Airlines Inc. 32 1.81 2.21 0.82 0.41 The “estimate” column shows the mean difference in air time with United Airlines, accounting for the flight destination. The estimate tends to be more precise (i.e., smaller standard errors) for carriers with a larger number of observations. This time, we find that Virgin America, Air Tran, Frontier Airlines, and Hawaiian Airlines tend to show particularly longer air times than United Airlines. Next, let’s take a step further to account for flight timing as well. We can do this by adding fixed effects for flight dates and hours. flights2 &lt;- flights2 %&gt;% mutate( date_id = month*100 + day ) flights2$date_id %&gt;% unique() %&gt;% length() ## [1] 365 f2 &lt;- flights2 %&gt;% with( lm( air_time ~ as.factor(carrier) + as.factor(dest) + + as.factor(date_id) + as.factor(hour) ) ) lm_rlt2 &lt;- clean_lm_rlt(f2) lm_rlt2 %&gt;% kable(digit=2) carrier name n_obs estimate std.error statistic p.value UA United Air Lines Inc. 58665 NA NA NA NA B6 JetBlue Airways 54635 1.60 0.07 22.50 0.00 EV ExpressJet Airlines Inc. 54173 0.61 0.09 6.67 0.00 DL Delta Air Lines Inc. 48110 0.95 0.07 13.03 0.00 AA American Airlines Inc. 32729 1.84 0.08 22.81 0.00 MQ Envoy Air 26397 1.45 0.10 14.70 0.00 US US Airways Inc. 20536 0.17 0.11 1.51 0.13 9E Endeavor Air Inc. 18460 1.57 0.11 14.72 0.00 WN Southwest Airlines Co. 12275 1.14 0.15 7.82 0.00 VX Virgin America 5162 4.85 0.15 31.57 0.00 FL AirTran Airways Corporation 3260 2.19 0.23 9.58 0.00 AS Alaska Airlines Inc. 714 -2.55 0.41 -6.21 0.00 F9 Frontier Airlines Inc. 685 3.31 0.40 8.29 0.00 YV Mesa Airlines Inc. 601 0.32 0.44 0.73 0.46 HA Hawaiian Airlines Inc. 342 11.79 0.75 15.80 0.00 OO SkyWest Airlines Inc. 32 7.63 1.83 4.17 0.00 lm_rlt2 %&gt;% filter(carrier!=&#39;UA&#39;) %&gt;% ggplot(aes(x = carrier, y = estimate)) + geom_col() + labs(title = &quot;Mean Air Time Compared to United Airlines&quot;) The results are similar to the previous linear mode except that this time SkyWest Airlines shows much longer air time. Before wrapping up, our final model is a check for the robustness of the above results. We would like to replace the date and hour fixed effects in the previous model with date-hour fixed effects (i.e., the interaction between date and hour). We could add such fixed effects using time_hour variable defined above. However, that would mean adding nearly 7,000 dummy variables to our linear regression, which is computationally too intensive. To work around this issue, we approximate this estimation by pre-processing the dependent variable. Specifically, we calculate the average air time for each combination of time_hour and dest and define a new dependent variable by subtracting this average value from the original air time variable (i.e., the new variable is centered at zero-mean for each combination of time_hour and dest). Then, we estimate a linear model with carrier and destination fixed effects. ## Adding time_hour fixed effects is too computationally intensive # f1 &lt;- flights %&gt;% # with( # lm( air_time ~ as.factor(carrier) + as.factor(dest) + as.factor(time_hour)) # ) unique(flights2$time_hour) %&gt;% length() # 6,936 unique time_hour ## [1] 6936 flights2 &lt;- flights2 %&gt;% group_by(dest, time_hour) %&gt;% mutate( air_time_centered = air_time - mean(air_time, na.rm=TRUE) ) f3 &lt;- flights2 %&gt;% with( lm( air_time_centered ~ as.factor(carrier) + as.factor(dest) ) ) lm_rlt3 &lt;- clean_lm_rlt(f3) lm_rlt3 %&gt;% kable(digit=2) # Note: standard errors, t-stat, and p-val are incorrect carrier name n_obs estimate std.error statistic p.value UA United Air Lines Inc. 58665 NA NA NA NA B6 JetBlue Airways 54635 0.82 0.03 32.24 0.00 EV ExpressJet Airlines Inc. 54173 0.88 0.03 26.50 0.00 DL Delta Air Lines Inc. 48110 0.52 0.03 19.85 0.00 AA American Airlines Inc. 32729 1.20 0.03 41.06 0.00 MQ Envoy Air 26397 1.00 0.04 27.84 0.00 US US Airways Inc. 20536 -0.09 0.04 -2.21 0.03 9E Endeavor Air Inc. 18460 1.27 0.04 33.07 0.00 WN Southwest Airlines Co. 12275 1.30 0.05 24.70 0.00 VX Virgin America 5162 3.47 0.06 62.59 0.00 FL AirTran Airways Corporation 3260 1.78 0.08 21.48 0.00 AS Alaska Airlines Inc. 714 -2.86 0.15 -19.15 0.00 F9 Frontier Airlines Inc. 685 1.99 0.14 13.73 0.00 YV Mesa Airlines Inc. 601 0.78 0.16 4.89 0.00 HA Hawaiian Airlines Inc. 342 1.34 0.27 4.96 0.00 OO SkyWest Airlines Inc. 32 3.50 0.67 5.26 0.00 lm_rlt3 %&gt;% filter(carrier!=&#39;UA&#39;) %&gt;% ggplot(aes(x = carrier, y = estimate)) + geom_col() + labs(title = &quot;Mean Air Time Compared to United Airlines: Robustness Check&quot;) The point estimates should be approximately close to what we would obtain if we regress air_time on the fixed effects of carrier, dest, and time_hour. However, the standard errors are not correctly displayed in the table because the centered variable has a smaller total variation compared to the original air_time variable. (Correct standard errors can be obtained, for example, through a bootstrapping technique.) Overall, we see again a tendency that lower-cost carriers like Sky West Airlines, Virgin America, Frontier Airlines, and Air Tran show particularly longer air time than United Airlines. Jet Blue Airways, another low-cost carrier, shows a less obvious difference from United Airlines, possibly suggesting that their operation focused on the East Cost is efficient for the flights departing from New York City. Hawaiian Airlines and Alaskan Airlines appear to be somewhat different from other carriers perhaps because they are more specialized in particular flight times and destinations compared to their rivals. In particular, the flights to Hawaii may have distinct delay patterns that are concentrated on certain date-hours of the peak vacation seasons. "],
["1-4-reflections.html", "1.4 Reflections", " 1.4 Reflections In this introduction, we have reviewed the tools of deplyr and ggplot2 packages as a starting point for data analyses and visualization in R. This new generation of tools is a data exploration language as much as a set of functions to shortcut traditional data manipulation methods in R. This language provides an intuitive system of translating our inquiries to the data analysis in R. Using the flight dataset, we have also investigated flight delay patterns. We find that airport congestion is unlikely a major cause of delay in New York City. There are small differences in the air time (e.g. less than 5 minutes) across carriers for a given destination although it remains unclear how this relates to the delay patterns. In fact, the concept of “delay” is complicated because it is defined in reference to the scheduled departure and arrival times, which may differ by carrier. A delay would not include the time sitting in the airplane before taking off or after landing as long as it is within the schedule. It might be more interesting to compare scheduled flight duration instead of delays or air time. (Such an analysis would involve somewhat complicated manipulations of date and time with our flight data.) This leads us to the final point of this exercise; an interesting data analysis requires knowledge on the real-world process that generated the data and the ability to ask interesting questions. deplyr and ggplot2 packages can let you employ a variety of data analytics tools with ease, but the ultimate power of the analysis will always rest on your knowledge and creativity. -->"],
["2-essentials.html", "2 Essentials", " 2 Essentials 2017-04-20: VERY Preliminary! This section provides an overview of the essential concepts for manipulating data and programming in R. "],
["2-1-cheatsheets.html", "2.1 Cheatsheets", " 2.1 Cheatsheets Cheatsheets are useful for glancing at various functions. Base R RStudio IDE dplyr ggplot2 "],
["2-2-data-types.html", "2.2 Data types", " 2.2 Data types 2.2.1 Atomic In most cases, each atomic element has a type (mode) of numeric: number logical: TRUE or FALSE (T or F for shortcuts) character: character string factor: a level of categorical variable Other types include date and nonexistent NULL. The factor is also a class of its own, meaning that many R functions apply operations that are specific to the factor class. # assess objects 123, &quot;abc&quot;, and TRUE for their types str(123) # str() returns the structure ## num 123 str(&quot;abc&quot;) ## chr &quot;abc&quot; str(TRUE) ## logi TRUE c(is.numeric(123), is.numeric(&quot;abc&quot;), is.numeric(TRUE)) ## [1] TRUE FALSE FALSE c(is.logical(123), is.logical(&quot;abc&quot;), is.logical(TRUE)) ## [1] FALSE FALSE TRUE c(is.character(123), is.character(&quot;abc&quot;), is.character(TRUE)) ## [1] FALSE TRUE FALSE # &quot;&lt;-&quot; means an assignment from right to left factor1 &lt;- as.factor(c(1,2,3)) # Looks like numeric but not factor1 ## [1] 1 2 3 ## Levels: 1 2 3 factor2 &lt;- as.factor(c(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;)) # Looks like characters but not factor2 ## [1] a b c ## Levels: a b c factor3 &lt;- as.factor(c(TRUE,FALSE,T)) # Looks like logicals but not factor3 ## [1] TRUE FALSE TRUE ## Levels: FALSE TRUE c(is.factor(factor1[1]), is.factor(factor2[1]), is.factor(factor3[1])) ## [1] TRUE TRUE TRUE # Extract the first element (factor1[1] etc.) factor1[1] ## [1] 1 ## Levels: 1 2 3 factor2[2] ## [1] b ## Levels: a b c factor3[3] ## [1] TRUE ## Levels: FALSE TRUE NULL has zero-length. Also, empty numeric, logical, and character objects have zero-length. length(NULL) ## [1] 0 length(numeric(0)) # numeric(N) returns a vector of N zeros ## [1] 0 length(logical(0)) # logical(N) returns a vector of N FALSE objects ## [1] 0 length(character(0)) # character(N) returns a vector of N &quot;&quot; objects ## [1] 0 Each vector has a type of numeric, logical, character, or factor. Each matrix has a type of numeric, logical, or character. A data frame can contain mixed types across columns where each column (e.g., a variable) has a type of numeric, logical, character or factor. vector1 &lt;- c(1, NA, 2, 3) # read as numeric vector1 ## [1] 1 NA 2 3 vector2 &lt;- c(TRUE, FALSE, T, F) # read as logical vector2 ## [1] TRUE FALSE TRUE FALSE vector3 &lt;- c(1, NA, &quot;abc&quot;, TRUE, &quot;TRUE&quot;) # read as character vector3 ## [1] &quot;1&quot; NA &quot;abc&quot; &quot;TRUE&quot; &quot;TRUE&quot; vector4 &lt;- as.factor(c(1, NA, &quot;abc&quot;, TRUE, &quot;TRUE&quot;)) # read as factor vector4 ## [1] 1 &lt;NA&gt; abc TRUE TRUE ## Levels: 1 abc TRUE matrix1 &lt;- matrix(c(1:6), nrow = 3) # read as numeric matrix1 ## [,1] [,2] ## [1,] 1 4 ## [2,] 2 5 ## [3,] 3 6 matrix2 &lt;- matrix(c(TRUE,FALSE,rep(T,3),F), nrow = 3) # read as logical matrix2 ## [,1] [,2] ## [1,] TRUE TRUE ## [2,] FALSE TRUE ## [3,] TRUE FALSE matrix3 &lt;- matrix(c(1,2,3,&quot;a&quot;,&quot;b&quot;,&quot;abc&quot;), nrow = 3) # read as character matrix3 ## [,1] [,2] ## [1,] &quot;1&quot; &quot;a&quot; ## [2,] &quot;2&quot; &quot;b&quot; ## [3,] &quot;3&quot; &quot;abc&quot; df1 &lt;- data.frame( num = c(1,2,3), # read as numeric fac1 = c(&quot;a&quot;,&quot;b&quot;,&quot;abc&quot;), # read as factor logi = c(TRUE, FALSE, T), # read as logical fac2 = c(1,&quot;a&quot;,TRUE) # read as factor ) df1 ## num fac1 logi fac2 ## 1 1 a TRUE 1 ## 2 2 b FALSE a ## 3 3 abc TRUE TRUE df1$num # &quot;$&quot; symbol is used to extract a column ## [1] 1 2 3 df1$fac1 # character type is converted into a factor ## [1] a b abc ## Levels: a abc b df1$logi ## [1] TRUE FALSE TRUE df1$fac2 # mixed types within a column is converted into a factor ## [1] 1 a TRUE ## Levels: 1 a TRUE # additional argument &quot;stringsAsFactors = FALSE&quot; preserves character types. df2 &lt;- data.frame( num = c(1,2,3), # read as numeric char = c(&quot;a&quot;,&quot;b&quot;,&quot;abc&quot;), # read as character logi = c(TRUE, FALSE, T), # read as logical fac2 = as.factor(c(1,&quot;a&quot;,TRUE)), # read as factor stringsAsFactors = FALSE ) df2 ## num char logi fac2 ## 1 1 a TRUE 1 ## 2 2 b FALSE a ## 3 3 abc TRUE TRUE df2$num ## [1] 1 2 3 df2$char ## [1] &quot;a&quot; &quot;b&quot; &quot;abc&quot; df2$logi ## [1] TRUE FALSE TRUE df2$fac2 ## [1] 1 a TRUE ## Levels: 1 a TRUE 2.2.2 Factor A factor object is defined with a set of categorical levels, which may be labeled. The levels are either ordered (defined by ordered()) or unordered (defined by factor()). Ordered factor objects are treated in the specific order by certain statistical and graphical procedures. # We will convert the columns of df into factors df &lt;- data.frame( fac1 = c(0,1,1,4,4,2,2,3), fac2 = c(1,2,3,1,1,2,2,3), fac3 = c(4,2,3,4,4,2,2,3) ) # convert fac1 to ordered factors df$fac1 &lt;- ordered(df$fac1, levels = c(0,4,3,2,1) # defines the order ) df$fac1 ## [1] 0 1 1 4 4 2 2 3 ## Levels: 0 &lt; 4 &lt; 3 &lt; 2 &lt; 1 summary(df$fac1) # gives the table of counts for each level ## 0 4 3 2 1 ## 1 2 1 2 2 # convert fac2 to unordered factors with labels df$fac2 &lt;- factor(df$fac2, levels = c(1,2,3), # no particular order # attach labels to factors: 1=red, 2=blue, 3=green labels = c(&quot;red&quot;, &quot;blue&quot;, &quot;green&quot;) ) df$fac2 ## [1] red blue green red red blue blue green ## Levels: red blue green summary(df$fac2) ## red blue green ## 3 3 2 # convert fac3 to ordered factors with labels df$fac3 &lt;- ordered(df$fac3, levels = c(2,3,4), # attach labels to factors: 2=Low, 3=Medium, 4=High labels = c(&quot;Low&quot;, &quot;Medium&quot;, &quot;High&quot;) ) df$fac3 ## [1] High Low Medium High High Low Low Medium ## Levels: Low &lt; Medium &lt; High summary(df$fac3) ## Low Medium High ## 3 2 3 2.2.3 Matrix matrix() defines a matrix from a vector. The default is to arrange the vector by column (byrow = FALSE). # byrow = FALSE (the default) matrix(data = c(1:6), nrow = 2, ncol = 3, byrow = FALSE, dimnames = NULL) ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 2 4 6 # byrow = TRUE matrix(data = c(1:6), nrow = 2, ncol = 3, byrow = TRUE, dimnames = NULL) ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 4 5 6 # give row and column names to a matrix mat1 &lt;- matrix(data = c(1:6), nrow = 2, ncol = 3, byrow = FALSE, dimnames = list(c(&quot;r1&quot;,&quot;r2&quot;), c(&quot;c1&quot;,&quot;c2&quot;,&quot;c3&quot;))) mat1 ## c1 c2 c3 ## r1 1 3 5 ## r2 2 4 6 dim(mat1) # dimension: row by column ## [1] 2 3 colnames(mat1) ## [1] &quot;c1&quot; &quot;c2&quot; &quot;c3&quot; rownames(mat1) ## [1] &quot;r1&quot; &quot;r2&quot; colnames(mat1) &lt;- c(&quot;v1&quot;,&quot;v2&quot;,&quot;v3&quot;) # change column names by assignment &quot;&lt;-&quot; mat1 ## v1 v2 v3 ## r1 1 3 5 ## r2 2 4 6 # R makes a guess when only nrow or ncol is supplied matrix(data = c(1:6), nrow = 2) ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 2 4 6 matrix(data = c(1:6), ncol = 3) ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 2 4 6 # combine matrices by column via &quot;cbind()&quot; or by row via &quot;rbind()&quot; cbind(mat1,mat1) ## v1 v2 v3 v1 v2 v3 ## r1 1 3 5 1 3 5 ## r2 2 4 6 2 4 6 rbind(mat1,mat1) ## v1 v2 v3 ## r1 1 3 5 ## r2 2 4 6 ## r1 1 3 5 ## r2 2 4 6 There are recycling rules (which does/controls what?) in R. # the vector shorter than the length of all elements of a matrix matrix(data = c(1:4), nrow = 2, ncol= 3) ## Warning in matrix(data = c(1:4), nrow = 2, ncol = 3): data length [4] is ## not a sub-multiple or multiple of the number of columns [3] ## [,1] [,2] [,3] ## [1,] 1 3 1 ## [2,] 2 4 2 # R treats a scaler as a vector of length that conforms cbind() or rbind() cbind(mat1, colA = 1) ## v1 v2 v3 colA ## r1 1 3 5 1 ## r2 2 4 6 1 rbind(mat1, rowA= 1, rowB= 2, rowC= 3) ## v1 v2 v3 ## r1 1 3 5 ## r2 2 4 6 ## rowA 1 1 1 ## rowB 2 2 2 ## rowC 3 3 3 To replace elements of a matrix, we can use assignment operator &lt;-. mat1[1,1] &lt;- 10 mat1 ## v1 v2 v3 ## r1 10 3 5 ## r2 2 4 6 mat1[,2] &lt;- c(7,8) mat1 ## v1 v2 v3 ## r1 10 7 5 ## r2 2 8 6 mat1[,1] &lt;- 0 # recycling rule mat1 ## v1 v2 v3 ## r1 0 7 5 ## r2 0 8 6 Matrix allows for easy extraction for rows and columns separated by comma. mat1 ## v1 v2 v3 ## r1 0 7 5 ## r2 0 8 6 mat1[1, ] # row = 1 and all columns ## v1 v2 v3 ## 0 7 5 mat1[, 1] # all rows and col = 1 ## r1 r2 ## 0 0 mat1[c(TRUE,FALSE),] # by a logical vector ## v1 v2 v3 ## 0 7 5 mat1[, c(TRUE,FALSE)] ## v1 v3 ## r1 0 5 ## r2 0 6 mat1[2,3] # row = 2 and col = 3 ## [1] 6 mat1[1:2, 2:3] # row = 1:2 and col = 2:3 ## v2 v3 ## r1 7 5 ## r2 8 6 mat1[1:2, 2:3][2,2] # subset of a subset ## [1] 6 mat1[, 1][2] # vector extraction is done with one-dimensional index ## r2 ## 0 Important: when a single row or column is extracted, it gets converted to a vector with no dimension. mat1[1,] ## v1 v2 v3 ## 0 7 5 is.matrix(mat1[1, ]) ## [1] FALSE dim(mat1[1,]) ## NULL length(mat1[1, ]) ## [1] 3 # to keep a row or column vector structure, use drop = FALSE mat1[1,, drop = FALSE] ## v1 v2 v3 ## r1 0 7 5 is.matrix(mat1[1,,drop = FALSE]) ## [1] TRUE dim(mat1[1,,drop = FALSE]) ## [1] 1 3 length(mat1[1,,drop = FALSE]) ## [1] 3 mat1[,1, drop = FALSE] ## v1 ## r1 0 ## r2 0 is.matrix(mat1[,1,drop = FALSE]) ## [1] TRUE dim(mat1[,1,drop = FALSE]) ## [1] 2 1 length(mat1[,1,drop = FALSE]) ## [1] 2 Another way of extraction from a matrix is to use row or column names. mat1[,&#39;v1&#39;] ## r1 r2 ## 0 0 mat1[,c(&#39;v1&#39;,&#39;v3&#39;)] ## v1 v3 ## r1 0 5 ## r2 0 6 mat1[&#39;r2&#39;,,drop= FALSE] ## v1 v2 v3 ## r2 0 8 6 apply() applies a function for a specified margin (dimension index number) of the matrix. mat1 ## v1 v2 v3 ## r1 0 7 5 ## r2 0 8 6 apply(mat1,1,mean) # dimension 1 (across rows) ## r1 r2 ## 4.000000 4.666667 apply(mat1,2,mean) # dimension 2 (across columns) ## v1 v2 v3 ## 0.0 7.5 5.5 # one can write a custom function inside apply(). (called annonymous function) # Its argument corresponds to the row or column vector passed by apply(). apply(mat1,2, function(x) sum(x)/length(x) ) # x is the internal vector name ## v1 v2 v3 ## 0.0 7.5 5.5 ans1 &lt;- apply(mat1,2, function(x) { avg = mean(x) sd = sd(x) # return the results as a list list(Avg = avg, Sd = sd) } ) unlist(ans1[[2]]) # results for the second column ## Avg Sd ## 7.5000000 0.7071068 unlist(ans1[[3]]) # results for the third column ## Avg Sd ## 5.5000000 0.7071068 Arrays are a generalization of matrices and can have more than 2 dimensions. array(c(1:18), c(2,3,3)) # dimension 2 by 2 by 3 ## , , 1 ## ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 2 4 6 ## ## , , 2 ## ## [,1] [,2] [,3] ## [1,] 7 9 11 ## [2,] 8 10 12 ## ## , , 3 ## ## [,1] [,2] [,3] ## [1,] 13 15 17 ## [2,] 14 16 18 array(c(1:9), c(2,3,3)) # R recycles the vector ## , , 1 ## ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 2 4 6 ## ## , , 2 ## ## [,1] [,2] [,3] ## [1,] 7 9 2 ## [2,] 8 1 3 ## ## , , 3 ## ## [,1] [,2] [,3] ## [1,] 4 6 8 ## [2,] 5 7 9 2.2.4 Data Frame A data frame is similar to a matrix, but it accepts multiple types (modes) of variables across columns (e.g., a dataset in typical data analysis programs like SAS, SPSS, Stata etc.). In some cases matrices and data frames may be treated interchangeably, but generally they need to be distinguished. Data manipulation functions are often written for data frames, while some base R functions are written for matrices. mymat1 &lt;- matrix(data = c(1:6), nrow = 2, ncol = 3, dimnames = list(c(&quot;r1&quot;,&quot;r2&quot;), c(&quot;c1&quot;,&quot;c2&quot;,&quot;c3&quot;))) mymat1 ## c1 c2 c3 ## r1 1 3 5 ## r2 2 4 6 class(mymat1) ## [1] &quot;matrix&quot; colnames(mymat1) ## [1] &quot;c1&quot; &quot;c2&quot; &quot;c3&quot; names(mymat1) ## NULL mydf1 &lt;- data.frame( mymat1, num = c(1,2), fac1 = c(&quot;a&quot;,&quot;abc&quot;), logi = c(TRUE, FALSE), fac2 = c(1,&quot;a&quot;) ) mydf1 ## c1 c2 c3 num fac1 logi fac2 ## r1 1 3 5 1 a TRUE 1 ## r2 2 4 6 2 abc FALSE a class(mydf1) ## [1] &quot;data.frame&quot; colnames(mydf1) ## [1] &quot;c1&quot; &quot;c2&quot; &quot;c3&quot; &quot;num&quot; &quot;fac1&quot; &quot;logi&quot; &quot;fac2&quot; names(mydf1) # colnames and names are the same ## [1] &quot;c1&quot; &quot;c2&quot; &quot;c3&quot; &quot;num&quot; &quot;fac1&quot; &quot;logi&quot; &quot;fac2&quot; Extracting elements from a data frame is similar to extracting from a matrix, but there are a few additional methods. mydf1[1,] # row = 1 and all columns ## c1 c2 c3 num fac1 logi fac2 ## r1 1 3 5 1 a TRUE 1 mydf1[,1] # all rows and col = 1 ## [1] 1 2 # data frame preserves dimension while extracting a row but not a column dim(mydf1[1,]) ## [1] 1 7 dim(mydf1[,1]) ## NULL dim(mydf1[,1,drop=FALSE]) # use drop = FALSE to keep a column vector ## [1] 2 1 mydf1[,1,drop=FALSE] ## c1 ## r1 1 ## r2 2 mydf1[, c(&#39;c1&#39;,&#39;num&#39;,&#39;logi&#39;)] ## c1 num logi ## r1 1 1 TRUE ## r2 2 2 FALSE class(mydf1[, c(&#39;c1&#39;,&#39;num&#39;,&#39;logi&#39;)]) ## [1] &quot;data.frame&quot; # extraction by column name with &quot;$&quot; symbol: df$varname mydf1$c1 ## [1] 1 2 dim(mydf1$c1) ## NULL # one can use quote &#39; &#39; or &quot; &quot; as well mydf1$&#39;c1&#39; ## [1] 1 2 # similarly, extraction by column name with [[ ]]: df[[&#39;varname&#39;]] mydf1[[&#39;c1&#39;]] ## [1] 1 2 dim(mydf1[[&#39;c1&#39;]]) ## NULL # or by index mydf1[[1]] ## [1] 1 2 # [[ ]] method is useful when passing a variable name as a string set_to_na &lt;- function(df, var) { df[[var]] &lt;- NA df } mydf1 ## c1 c2 c3 num fac1 logi fac2 ## r1 1 3 5 1 a TRUE 1 ## r2 2 4 6 2 abc FALSE a mydf2 &lt;- set_to_na(mydf1, &quot;c2&quot;) mydf2 ## c1 c2 c3 num fac1 logi fac2 ## r1 1 NA 5 1 a TRUE 1 ## r2 2 NA 6 2 abc FALSE a # add a variable mydf1$newvar &lt;- c(4, 4) mydf1 ## c1 c2 c3 num fac1 logi fac2 newvar ## r1 1 3 5 1 a TRUE 1 4 ## r2 2 4 6 2 abc FALSE a 4 mydf1$newvar2 &lt;- mydf1$c2 + mydf1$c3 mydf1 ## c1 c2 c3 num fac1 logi fac2 newvar newvar2 ## r1 1 3 5 1 a TRUE 1 4 8 ## r2 2 4 6 2 abc FALSE a 4 10 apply() may not work well with data frames since data frames are not exactly matrices. We can use simplified apply sapply() or list apply lapply() instead. mydf1 ## c1 c2 c3 num fac1 logi fac2 newvar newvar2 ## r1 1 3 5 1 a TRUE 1 4 8 ## r2 2 4 6 2 abc FALSE a 4 10 # sapply() idx_num &lt;- sapply(mydf1, is.numeric) idx_num ## c1 c2 c3 num fac1 logi fac2 newvar newvar2 ## TRUE TRUE TRUE TRUE FALSE FALSE FALSE TRUE TRUE apply(mydf1[,idx_num], 2, mean) ## c1 c2 c3 num newvar newvar2 ## 1.5 3.5 5.5 1.5 4.0 9.0 sapply(mydf1[,idx_num], mean) ## c1 c2 c3 num newvar newvar2 ## 1.5 3.5 5.5 1.5 4.0 9.0 # lapply() idx_num2 &lt;- unlist(lapply(mydf1, is.numeric)) idx_num2 ## c1 c2 c3 num fac1 logi fac2 newvar newvar2 ## TRUE TRUE TRUE TRUE FALSE FALSE FALSE TRUE TRUE unlist(lapply(mydf1[,idx_num2], mean)) ## c1 c2 c3 num newvar newvar2 ## 1.5 3.5 5.5 1.5 4.0 9.0 2.2.5 List A list is an ordered collection of (possibly unrelated) objects. The objects in a list are referenced by [[1]], [[2]], …, or [[‘var1’]], [[‘var2’]], … etc. mylist1 &lt;- list(v1 = c(1,2,3), v2 = c(&quot;a&quot;,&quot;b&quot;), v3 = factor(c(&quot;blue&quot;,&quot;red&quot;,&quot;orange&quot;,&quot;yellow&quot;)), v4 = data.frame( u1 = c(1:3), u2 = c(&quot;p&quot;,&quot;q&quot;,&quot;r&quot;)) ) mylist1 ## $v1 ## [1] 1 2 3 ## ## $v2 ## [1] &quot;a&quot; &quot;b&quot; ## ## $v3 ## [1] blue red orange yellow ## Levels: blue orange red yellow ## ## $v4 ## u1 u2 ## 1 1 p ## 2 2 q ## 3 3 r # extraction mylist1[[1]] ## [1] 1 2 3 mylist1[[&quot;v2&quot;]] ## [1] &quot;a&quot; &quot;b&quot; mylist1$v3 ## [1] blue red orange yellow ## Levels: blue orange red yellow mylist1$v4$u2 ## [1] p q r ## Levels: p q r # assignment mylist1$v5 &lt;- c(&quot;a&quot;,NA) mylist1$v5 ## [1] &quot;a&quot; NA # a list can be nested mylist1$v6 &lt;- list(y1 = c(2,9), y2 = c(0,0,0,1)) mylist1$v6 ## $y1 ## [1] 2 9 ## ## $y2 ## [1] 0 0 0 1 lapply() is very versatile since the items in a list can be completely unrelated. unlist(lapply(mylist1, class)) ## v1 v2 v3 v4 v5 ## &quot;numeric&quot; &quot;character&quot; &quot;factor&quot; &quot;data.frame&quot; &quot;character&quot; ## v6 ## &quot;list&quot; unlist(lapply(mylist1, attributes)) # some variables have attributes ## v3.levels1 v3.levels2 v3.levels3 v3.levels4 v3.class ## &quot;blue&quot; &quot;orange&quot; &quot;red&quot; &quot;yellow&quot; &quot;factor&quot; ## v4.names1 v4.names2 v4.row.names1 v4.row.names2 v4.row.names3 ## &quot;u1&quot; &quot;u2&quot; &quot;1&quot; &quot;2&quot; &quot;3&quot; ## v4.class v6.names1 v6.names2 ## &quot;data.frame&quot; &quot;y1&quot; &quot;y2&quot; lapply(mylist1, function(x) { if (is.numeric(x)) return(summary(x)) if (is.character(x)) return(x) if (is.factor(x)) return(table(x)) if (is.data.frame(x)) return(head(x)) if (is.list(x)) return(unlist(lapply(x,class))) } ) ## $v1 ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.0 1.5 2.0 2.0 2.5 3.0 ## ## $v2 ## [1] &quot;a&quot; &quot;b&quot; ## ## $v3 ## x ## blue orange red yellow ## 1 1 1 1 ## ## $v4 ## u1 u2 ## 1 1 p ## 2 2 q ## 3 3 r ## ## $v5 ## [1] &quot;a&quot; NA ## ## $v6 ## y1 y2 ## &quot;numeric&quot; &quot;numeric&quot; "],
["2-3-programming.html", "2.3 Programming", " 2.3 Programming 2.3.1 Operator Table 2.1: Basic R Operators Operation Description x + y Addition x - y Subtraction x * y Multiplication x / y Division x ^ y Exponentiation x %% y Modular arithmatic x %/% y Integer division x == y Test for equality x &lt;= y Test for less than or equal to x &gt;= y Test for greater than or equal to x &amp;&amp; y Boolean AND for scalars x || y Boolean OR for scalers x &amp; y Boolean AND for vectors x | y Boolean OR for vectors !x Boolean negation source: (Matloff 2011) 2.3.2 If else if (1 &gt; 0) { print(&quot;result: if&quot;) } else { print(&quot;result: else&quot;) } ## [1] &quot;result: if&quot; # {} brackets can be used to combine multiple expressions # They can be skipped for a single-expression if-else statement. if (1 &gt; 2) print(&quot;result: if&quot;) else print(&quot;result: else&quot;) ## [1] &quot;result: else&quot; ifelse(c(1,2,3) &gt; 2, 1, -1) # return 1 if TRUE and -1 if else ## [1] -1 -1 1 Sys.time() ## [1] &quot;2017-04-20 16:36:21 CDT&quot; time &lt;- Sys.time() hour &lt;- as.integer(substr(time, 12,13)) # sequential if-else statements if (hour &gt; 8 &amp; hour &lt; 12) { print(&quot;morning&quot;) } else if (hour &lt; 18) { print(&quot;afternoon&quot;) } else { print(&quot;private time&quot;) } ## [1] &quot;afternoon&quot; 2.3.3 Loop for (i in 1:3) { print(i) } ## [1] 1 ## [1] 2 ## [1] 3 for (i in c(1,3,5)) print(i) ## [1] 1 ## [1] 3 ## [1] 5 i &lt;- 1 while (i &lt; 5) { print(i) i &lt;- i + 1 } ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 2.3.4 Function We can avoid repeating ourselves with writing similar lines of codes if we turn them into a function. Functions contain a series of tasks that can be applied to varying objects such as different vectors, matrices, characters, data frames, lists, and functions. A function consists of input arguments, tasks (R expressions), and output as an object (e.g. a vector, matrix, character, data frame, list, or function etc.). It can be named or remain anonymous (typically used inside a function like lapply()). ## name_to_be_assigned &lt;- function(input args) { ## tasks ## } The output of the function, aside from those that are printed, saved, or exported, is the very last task (expression). If variable result is created inside the function, having result at the very end will return this item as an output. When multiple objects are created, it is often convenient to return those as a list. Use return() to return a specific item in the middle of the function and skip the rest of the evaluations. For checking errors and halting evaluations, use stop() or stopifnot(). myfun1 &lt;- function() print(&quot;Hello world&quot;) # just returning &quot;Hello world&quot; myfun1() ## [1] &quot;Hello world&quot; myfun2 &lt;- function(var) var^2 myfun2(var = 3) ## [1] 9 myfun3 &lt;- function(var = 1) ifelse(var&gt;0, log(var), var) myfun3() # default argument is var = 1 ## [1] 0 myfun3(2) ## [1] 0.6931472 myfun4 &lt;- function(x1, x2) { if (!is.numeric(x1) | !is.numeric(x2)) stop(&#39;demo of error: numeric args needed&#39;) x1*x2 } # try(myfun4(1, &quot;a&quot;)) myfun4(4, 3) ## [1] 12 2.3.5 Environment A function, formally known as a closure, consists of its arguments (called formals), a body, and an environment. An environment is a collection of existing R objects at the time when the function is created. Functions created at the top level have .GlobalEnv as their environments (R may refer to it as R_GlobalEnv as well). environment() # .GlobalEnv (or R_GlobalEnv) is the top-level environment ## &lt;environment: R_GlobalEnv&gt; f1 &lt;- function(arg1) environment() formals(f1) # arguments of f1() ## $arg1 body(f1) # body of f1() ## environment() environment(f1) # environment of f1(), which is .GlobalEnv ## &lt;environment: R_GlobalEnv&gt; f1() # inside f1 has its own enviornment ## &lt;environment: 0x7f9ac55a5d78&gt; A function can access to the objects in its environment (i.e., global to the function) and those defined inside (i.e., local to the function) and generally cannot overwrite the global objects. It allows for using common names such as “x1”, “var1” etc. defined inside functions, but those objects are only accessible within the function. a &lt;- NULL # object named &quot;a&quot; in .GlobalEnv f2 &lt;- function() { a &lt;- 1 # object named &quot;a&quot; in an environment inside f2 print(a) environment() } f2() # one instance creating an environment ## [1] 1 ## &lt;environment: 0x7f9ac5ecf950&gt; f2() # another instance creating another environment ## [1] 1 ## &lt;environment: 0x7f9ac5efe038&gt; a # stays NULL ## NULL ls() # ls() shows all objects of an environment (here .GlobablEnv) ## [1] &quot;a&quot; &quot;ans1&quot; &quot;df&quot; &quot;df1&quot; ## [5] &quot;df2&quot; &quot;f1&quot; &quot;f2&quot; &quot;factor1&quot; ## [9] &quot;factor2&quot; &quot;factor3&quot; &quot;hour&quot; &quot;i&quot; ## [13] &quot;idx_num&quot; &quot;idx_num2&quot; &quot;mat1&quot; &quot;matrix1&quot; ## [17] &quot;matrix2&quot; &quot;matrix3&quot; &quot;mydf1&quot; &quot;mydf2&quot; ## [21] &quot;myfun1&quot; &quot;myfun2&quot; &quot;myfun3&quot; &quot;myfun4&quot; ## [25] &quot;mylist1&quot; &quot;mymat1&quot; &quot;set_to_na&quot; &quot;tbl_operator&quot; ## [29] &quot;time&quot; &quot;vector1&quot; &quot;vector2&quot; &quot;vector3&quot; ## [33] &quot;vector4&quot; rm(list = ls()) # rm() removes items of an environment (here .GlobablEnv) ls() # all gone in GlobalEnv ## character(0) Using global assignment &lt;&lt;- operator, one can bend this general rule of not affecting global objects. This can be useful when it is desirable to make certain objects accessible across multiple functions without explicitly passing them through arguments. a &lt;- NULL b &lt;- NULL f1 &lt;- function() { a &lt;&lt;- 1 # global assignment # another way to assign to GlobalEnv assign(&quot;b&quot;, 2, envir = .GlobalEnv) } f1() a ## [1] 1 b ## [1] 2 a &lt;- 2 f2 &lt;- function() { # Since there is no &quot;a&quot; local to f2, R looks for &quot;a&quot; # in a parent environment, or .GlobalEnv print(a) # g() assigns a number to &quot;a&quot; in g()&#39;s environment g &lt;- function() a &lt;&lt;- 5 a &lt;- 0 # object called &quot;a&quot; local to f2 print(a) # g() updates only the local &quot;a&quot; to f2(), but not &quot;a&quot; in GlobalEnv # R&#39;s scope hierarchy starts from local to its environment g() print(a) } a &lt;- 3 # the first &quot;a&quot; is in .GlobalEnv when f2() is called # the second &quot;a&quot; is local to an instace of f2() # the third &quot;a&quot; is the updated version of the local &quot;a&quot; by g() f2() ## [1] 3 ## [1] 0 ## [1] 5 a # object &quot;a&quot; in GlobalEnv: unchanged by g() ## [1] 3 It is convenient to use &lt;&lt;- if you are sure about which object to overwrite. Otherwise, the use of &lt;&lt;- should be avoided. 2.3.6 Debugging browser() and traceback() are common debugging tools. A debugging session starts where browser() is inserted and allows for a line-by-line execution onward. Putting browser() inside a loop or function is useful because it allows for accessing the objects at a particular moment of execution in its environment. After an error alert, executing traceback() shows at which process the error occurred. Other tools include debug(), debugger(), and stopifnot(). 2.3.7 Stat func. What is going on here? What do we see? Table 2.2: Common R Statistical Distribution Functions Distribution Density_pmf cdf Quantiles Random_draw Normal dnorm( ) pnorm( ) qnorm( ) rnorm( ) Chi square dchisq( ) pchisq( ) qchisq( ) rchisq( ) Binomial dbinom( ) pbinom( ) qbinom( ) rbinom() source: (Matloff 2011) 2.3.8 String func. R has built-in string manipulation functions. They are commonly used for; detecting a certain pattern in a vector (grep() returning a location index vector, grepl() returning a logical vector) replacing a certain pattern with another (gsub()) counting the length of a string (nchar()) concatenating characters and numbers as a string (paste(), paste0(), sprintf()) extracting a segment of a string by character position range (substr()) splitting a string with a particular pattern (strsplit()) finding a character position of a pattern in a string (regexpr()) oasis &lt;- c(&quot;Liam Gallagher&quot;, &quot;Noel Gallagher&quot;, &quot;Paul Arthurs&quot;, &quot;Paul McGuigan&quot;, &quot;Tony McCarroll&quot;) grep(pattern = &quot;Paul&quot;, oasis) ## [1] 3 4 grepl(pattern = &quot;Gall&quot;, oasis) ## [1] TRUE TRUE FALSE FALSE FALSE gsub(&quot;Gallagher&quot;, &quot;Gallag.&quot;, oasis) ## [1] &quot;Liam Gallag.&quot; &quot;Noel Gallag.&quot; &quot;Paul Arthurs&quot; &quot;Paul McGuigan&quot; ## [5] &quot;Tony McCarroll&quot; nchar(oasis) ## [1] 14 14 12 13 14 paste(oasis) ## [1] &quot;Liam Gallagher&quot; &quot;Noel Gallagher&quot; &quot;Paul Arthurs&quot; &quot;Paul McGuigan&quot; ## [5] &quot;Tony McCarroll&quot; paste(oasis, collapse=&quot;, &quot;) ## [1] &quot;Liam Gallagher, Noel Gallagher, Paul Arthurs, Paul McGuigan, Tony McCarroll&quot; sprintf(&quot;%s from %d to %d&quot;, &quot;Oasis&quot;, 1991, 2009) ## [1] &quot;Oasis from 1991 to 2009&quot; substr(oasis, 1, 6) ## [1] &quot;Liam G&quot; &quot;Noel G&quot; &quot;Paul A&quot; &quot;Paul M&quot; &quot;Tony M&quot; strsplit(oasis, split=&quot; &quot;) # split by a blank space ## [[1]] ## [1] &quot;Liam&quot; &quot;Gallagher&quot; ## ## [[2]] ## [1] &quot;Noel&quot; &quot;Gallagher&quot; ## ## [[3]] ## [1] &quot;Paul&quot; &quot;Arthurs&quot; ## ## [[4]] ## [1] &quot;Paul&quot; &quot;McGuigan&quot; ## ## [[5]] ## [1] &quot;Tony&quot; &quot;McCarroll&quot; regexpr(&quot;ll&quot;, oasis[1])[1] ## [1] 8 Common regular expressions used in R include; &quot;[char]&quot; (any string containing either “c”, “h”, “a”, or “r” ) &quot;a.c&quot; (any string containing “a” followed by any letter followed by “c”) &quot;\\\\.&quot; (any string containing symbol “.”). grepl(&quot;[is]&quot;, oasis) ## [1] TRUE FALSE TRUE TRUE FALSE grepl(&quot;P..l&quot;, oasis) ## [1] FALSE FALSE TRUE TRUE FALSE grepl(&quot;\\\\.&quot;, c(&quot;Liam&quot;, &quot;Noel&quot;, &quot;Paul A.&quot;, &quot;Paul M.&quot;, &quot;Tony&quot;)) ## [1] FALSE FALSE TRUE TRUE FALSE 2.3.9 Set func. The functions for common set operations include union(), intersect(), setdiff(), and setequal(). The most commonly used function is %in% operator; X %in% Y returns a logical vector indicating whether an each element of X is a member of Y. c(1,2,3,4,5) %in% c(3,2,5) c(&quot;a&quot;,&quot;b&quot;,&quot;t&quot;,&quot;s&quot;) %in% c(&quot;t&quot;,&quot;a&quot;,&quot;a&quot;) References "],
["2-4-housekeeping.html", "2.4 Housekeeping", " 2.4 Housekeeping 2.4.1 Working directory getwd() returns the current working directly. setwd(new_directory) sets a specified working directory. 2.4.2 R session sessionInfo() shows the current session information. In RStudio, .rs.restartR() restarts a session. 2.4.3 Save &amp; load R objects can be saved and loaded by save(object1, object2, ..., file=&quot;file_name.RData&quot;) and load(file=&quot;file_name.RData&quot;). A ggplot object can be save by ggsave(&quot;file_name.png&quot;). 2.4.4 Input &amp; Output A common method to read and write data files is read.csv(&quot;file_name.csv&quot;) and write.csv(data_frame, file = &quot;file_name.csv&quot;). scan() is a more general function to read data files and interact with user keyboard inputs. file() is also a general function for reading data through connections, which refer to R’s mechanism for various I/O operations. dir() returns the file names in your working directory. A useful function is cat(), which can print a cleaner output to the screen, compared to print(). print(&quot;example&quot;) ## [1] &quot;example&quot; cat(&quot;example\\n&quot;) # end with \\n ## example cat(&quot;some string&quot;, c(1:4), &quot;more string\\n&quot;) ## some string 1 2 3 4 more string cat(&quot;some string&quot;, c(1:4), &quot;more string\\n&quot;, sep=&quot;_&quot;) ## some string_1_2_3_4_more string 2.4.5 Updating R needs regular updates for R distribution, individual R packages, and RStudio. Generally, updating once or twice a year would suffice. For updating RStudio, go to Help and then Check for Updates. Also, RStudio also makes it easy to update packages; go to Tools and the Check for Package Updates. Do these updates when you have time or you know that you need to update a particular package; updating R and R packages can be trickier than it seems. # check R version getRversion() ## [1] &#39;3.3.1&#39; version ## _ ## platform x86_64-apple-darwin13.4.0 ## arch x86_64 ## os darwin13.4.0 ## system x86_64, darwin13.4.0 ## status ## major 3 ## minor 3.1 ## year 2016 ## month 06 ## day 21 ## svn rev 70800 ## language R ## version.string R version 3.3.1 (2016-06-21) ## nickname Bug in Your Hair # check installed packages ## installed.packages() # list all packages where an update is available ## old.packages() # update all available packages of installed packages ## update.packages() # update, without prompt ## update.packages(ask = FALSE) For windows users, one can automate the process using installr package. ## --- execute the following --- ## install.packages(&quot;installr&quot;) # install ## setInternet2(TRUE) # only for R versions older than 3.3.0 ## installr::updateR() # updating R. Sometimes, you can accidentally corrupt sample datasets that come with packages. To restore the original datasets, you have to remove the package by remove.packages() and then install it again. Use class(), attributes(), and str() to check for any unrecognized attributes attached to the dataset. Also, if you suspect that you have accidentally corrupted R itself, you should re-install the R distribution. -->"],
["3-piecemeal-top.html", "3 Piecemeal Topics", " 3 Piecemeal Topics Workshop materials will be added here. "],
["3-1-dplyr.html", "3.1 Unusual Deaths in Mexico", " 3.1 Unusual Deaths in Mexico 2017-04-20: VERY Preliminary! Materials This is a practice session of dplyr and ggplot2 using a case study related to tidyr package. The case is about investigating the causes of death in Mexico that have unusual temporal patterns within a day. The data on mortalities in 2008 have the following pattern by hour; Figure 3.1: Temporal pattern of all causes of death Do you find anything unusual or unexpected? The figure shows several peaks within a day, indicating some increased risk of death during certain times of the day. What could generate these patterns? Wickham, the author, finds; The causes of [unusual] death fall into three main groups: murder, drowning, and transportation related. Murder is more common at night, drowning in the afternoon, and transportation related deaths during commute times (Wickham 2014). Figure 3.2: Causes of death with unusual temporal courses. Hour of day (hod) on the x-axis and proportion (prop) on the y-axis. Overall hourly death rate shown in grey. Causes of death with more than 350 deaths over a year. We will use two datasets: deaths containing the timing and coded causes of deaths and codes containing the look-up table for the coded causes. The dataset deaths has over 53,000 records (rows), so we use head() to look at the first several rows. # &quot;deaths08b&quot; is a renamed dataset with easier-to-read column names head(deaths08b) ## Year of Death (yod) Month of Death (mod) Day of Death (dod) ## 1 2008 1 1 ## 2 2008 1 1 ## 3 2008 1 1 ## 4 2008 1 1 ## 5 2008 1 1 ## 6 2008 1 1 ## Hour of Death (hod) Cause of Death (cod) ## 1 1 B20 ## 2 1 B22 ## 3 1 C18 ## 4 1 C34 ## 5 1 C50 ## 6 1 C50 The dataset codes has 1851 records. This table is generated by DT and webshot packages. In the search box, you can type in key words like “bacteria”, “nutrition”, and “fever”, as well as “assault” and “exposure” to see what items are in the data. We will reproduce this case study and practice using functions of dplyr and ggplot2. Arts &amp; Crafts Let’s recap the key ingredients of dplyr and ggplot2 from the introduction in Section 1. The six important functions in dplyr are: filter(): extracts rows (e.g., observations) of a data frame. We put logical vectors in its arguments. select(): extracts columns (e.g., variables) of a data frame. We put column names in its arguments. arrange(): orders rows of a data frame. We put column names in its arguments. summarise(): collapses a data frame into summary statistics. We put summary functions (e.g., statistics functions) using column names in its arguments. mutate(): creates new variables and adds them to the existing columns. We put window functions (e.g., transforming operations) using column names in its arguments. group_by(): assigns rows into groups within a data frame. We put column names in its arguments. We use piping operator %&gt;% (read as then) to translate a sentence of sequential instructions. For example, take dataset deaths08, %&gt;% (then) group the data by month of death, and %&gt;% (then) summarize the grouped data for the number of observations. deaths08 %&gt;% group_by(mod) %&gt;% # mod: month of death summarise(nobs = n()) # n(): a dplyr funciton to count rows ## # A tibble: 12 × 2 ## mod nobs ## &lt;int&gt; &lt;int&gt; ## 1 1 49002 ## 2 2 41685 ## 3 3 44433 ## 4 4 39845 ## 5 5 41710 ## 6 6 38592 ## 7 7 40198 ## 8 8 40297 ## 9 9 39481 ## 10 10 41671 ## 11 11 43341 ## 12 12 42265 The graphics with ggplot2 consist of three components: data: a data frame e.g., the first argument in ggplot(data, ...). geom: geometric objects such as points, lines, bars, etc. with parameters in parenthesis; e.g., geom_point(), geom_line(), geom_histogram() aes: specifications for x-y variables, as well as variables to differentiate geom objects by color, shape, or size. e.g., aes(x = var_x, y = var_y, shape = var_z) We specify data and aes in ggplot() and then add geom objects followed by + symbol (read as add a layer of); e.g., ggplot(data = dataset, mapping = aes(x = ...)) + geom_point(). The order of layers added by + symbol is generally interchangeable. Combined with %&gt;% operator, we can think of the code as a sentence. For example, take dataset deaths08, %&gt;% (then) plot with gglpot() with aes() featuring hour of day on the x-axis, + (and add a player of) geom object geom_histogram(). # a histogram version of the line-graph for the total number of deaths above deaths08 %&gt;% ggplot(aes(x = hod)) + geom_histogram(binwidth = 1, color = &quot;white&quot;) # a summary by month of day and hour of day. # e.g, Jan-1am, ..,Jan-12pm, Feb-1am,..., Feb-12pm, ... n_month_hour &lt;- deaths08 %&gt;% group_by(mod, hod) %&gt;% summarise( nobs = n() ) n_month_hour %&gt;% ggplot(aes(x = hod, y = nobs, color = as.factor(mod))) + geom_point() # &quot;last_plot() + &quot; allows for adding more layers to the previous plot last_plot() + geom_line() Exercise Now it is your turn. The exercise is to reproduce the above results for the unusual causes of deaths. Download materials: case study paper and case study data Set working directly: setwd(your_directory) Load libraries: library(dplyr), library(ggplot2), library(MASS) (used for fitting data by robust regression) Note 1: There is a minor error in the case study where the author accidentally kept several records of data from years other than 2008. This has virtually no effect on the results, and we are seeking to produce the same results as that case study. Note 2: You could look at the code in the paper for hints. However, the code is written with the functions of plyr package, or the predecessor of dplyr. Do not load both plyr and dplyr libraries in the same R session; they do not seem to have good compatibility. Restart R if you accidentally loaded both. Part A. Display overall hourly deaths We will reproduce: Hints: Filter NA in the hour of day (hod) variable Use group_by(), summarise(), n() to obtain death counts by group Use ggplot() + geom_line() to produce plot Use + labs( x = &quot;x lable&quot;, y = &quot;y label&quot;) for axis labels see help file of scale_y_continous() for comma (use ?function_name for help) Part B. Count deaths per hour, per disease We will reproduce: --> Panel (a) of the table contains the frequency (i.e. the number of rows) for each combination of hour of day (hod) and cause of death (cod), supplemented by the disease description in panel (b). Panel (c) shows the proportion (prop) of each hod-cod combination in the total deaths by the cod. Panel (d) contains the frequency and proportion (freq_all and prop_all) of the deaths by hour of day. That is, panel (a) is the raw counts (e.g., frequency) of observations by each pair of hour of day (hod) and cause of death (cod), and panel (b) makes it easy to see the cause of death (cod). Panel (c) converts this frequency of hod-cod pair into the relative frequency within the total frequency of cod, so that we see at which hour a disproportionately large number of deaths occurs for cod. Panel (d), on the other hand, presents the overall hourly death rates; if every hour has the same probability of death, we would see prop_all \\(\\approx\\) 0.042 (i.e., 1/24). Here, we see the author’s idea of identifying “unusual deaths” by looking at how “prop” of each hod-cod pairs deviates from “prop_all” (see figure 3.2). Hints for creating panel (a) Use more than one variable in group_by() Use summarise() with n() to obtain death counts by group Hints for creating panel (b) Use left_join() to add the information from dataset codes Hints for creating panel (c) Use mutate() with sum() on the joined dataset Hints for creating panel (d) Create a new data frame by using summarise() on the joined and mutated data frame. summarise() will reduce the dimension of the data frame to its summary, which is the basis of panel (d). Once the desired summary is created, merge it to the data frame of panels (a)-(c). Before using summarise() above, use group_by() to specify new grouping First create freq_all variable via summarise() with n() and then create prop_all variable via mutate() with sum() (call this data frame overall_freq, which will be used again at the very end) Use left_join() to join panels (a)-(c) and panel (d) (overall_freq), which we refer to as master_hod data frame. Hints for extracting the same rows as in the Table 16 above Create a subset of the master_hod data under a new name Use filter() to select cod being either “I21”, “N18”, “E84”, or “B16” and hod being greater or equal to 8 and smaller or less than 11 Use select() to pick columns in a desired order and arrange() to sort Part C. Find outliers We will reproduce: --> We will create a deviation variable named dist by taking the mean of squared differences between prop and prop_all. The above figures show the the number of observations n and this distance measure dist by cause of death in the raw scale (left) and in the log scale (right). This is the author’s linear model that describes how each cause of death tends to exhibit a similar pattern of hourly death rates with the overall pattern. Once the model is defined, we can define “outliers”, which do not follow the model’s prediction. Hints Use group_by() and summarise() on the master_hod data frame to generate n with function sum() and dist by mean((prop - prop_all)^2) Filter this summary for n &gt; 50 and call it devi_cod (deviations by cause of death) Use ggplot() + geom_point() with data = devi_cod to produce the raw-scale figure Additionally use scale_x_log10(), scale_y_log10(), and geom_smooth(method = &quot;rlm&quot;, se = FALSE) to produce the log-scale figure See help for scale_x_log10() to adjust axis labels (look for “comma”) Technically speaking, we should change the axis labels to indicate the logarithmic transformation, but we skip it here. Let’s not worry about reproducing the exact grids as they appear in the paper Part D. Fit data by a regression and plot residuals We will reproduce: --> The figure is a plot of the regression residuals resid of log(dist) on log(n). By visual inspection, the points lying above the horizontal line at resid=1.5 are considered to be “unusual causes of deaths” by the author. Here the author used the robust linear model (rlm()) regression, but the syntax is mostly the same as that of the standard linear model regression (lm() ). Here is an example of regression by lm(). df &lt;- data.frame( x1 &lt;- c(1:10), y1 &lt;- c(1,3,2,4,6,5,7,5,7,8) ) df %&gt;% ggplot(aes(x = x1, y = y1)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) The geom_smooth() is estimating the following linear regression: \\[ y1 = intercept + coefficient * x1 + residual\\] The model is estimated by lm() as follows; f1 &lt;- lm(formula = y1 ~ x1, data = df) Now let’s see what we get out of the estimation results f1. class(f1) # class &quot;lm&quot; ## [1] &quot;lm&quot; summary(f1) # summary() knows how to summarise an object of class &quot;lm&quot; ## ## Call: ## lm(formula = y1 ~ x1, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.52727 -0.57273 -0.02727 0.52273 1.54545 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.0000 0.6924 1.444 0.186656 ## x1 0.6909 0.1116 6.192 0.000262 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.014 on 8 degrees of freedom ## Multiple R-squared: 0.8273, Adjusted R-squared: 0.8058 ## F-statistic: 38.34 on 1 and 8 DF, p-value: 0.0002618 coefficients(f1) # coefficient point estimate ## (Intercept) x1 ## 1.0000000 0.6909091 vcov(f1) # coefficient variance-covariance matrix ## (Intercept) x1 ## (Intercept) 0.47939394 -0.06848485 ## x1 -0.06848485 0.01245179 predict(f1) # predicted (fitted) values with the estimated coefficients ## 1 2 3 4 5 6 7 8 ## 1.690909 2.381818 3.072727 3.763636 4.454545 5.145455 5.836364 6.527273 ## 9 10 ## 7.218182 7.909091 resid(f1) # residuals: ## 1 2 3 4 5 6 ## -0.69090909 0.61818182 -1.07272727 0.23636364 1.54545455 -0.14545455 ## 7 8 9 10 ## 1.16363636 -1.52727273 -0.21818182 0.09090909 Now let’s get back to our exercise and reproduce the figure above. Hints Run a regression by rlm with formula = log(dist) ~ log(n) and store the residuals in devi_cod data. To read more about linear regressions, see the help file of lm() (type ?lm). For adding a column of residuals, you can use assignment devi_cod$resid &lt;- your_residuals. Plot the residual against log-scale n Note: Check the dataset devi_cod for missing values of dist and n before running a regression (you should not have missing values in this case). Most regression functions, including lm() and rlm(), drop any row with missing values from the estimation. This becomes an issue if we want to add a new column containing predicted values or residuals to the original dataset. (When rows containing missing values are dropped, the vector generated by predict() or resid() will be shorter than the number of rows in the original dataset.) Use ggplot() + geom_point() structure for the plot Add + scale_x_log10() and + geom_hline(yintercept = 1.5) Part E. Visualize unusual causes of death We will reproduce: --> The first figure is the unusual causes of deaths in devi_cod with a relatively large number of deaths (n &gt; 350) and the second is that of a relatively small number of deaths (n &lt;= 350). Hints Using the cut-toff value resid &gt; 1.5, filter devi_cod and call it unusual data frame. Join master_hod and unusual data frames. Then create two subsets of data with conditions n &gt; 350 and n &lt;= 350. Use ggplot() + geom_line() structure with + facet_warp(~ disease, ncol = 3) To include the overall hourly proportions of deaths (prop_all) representing the average of all causes of deaths in a given hour, add another layer by geom_line(aes(...), data = overall_freq) with a local aes() argument and a data argument. With the data argument, variables in another data frame can be combined (assuming the axes have the same measurements), and here we use the overall_freq data frame from the panel (d) portion of Table 16 above. last_plot() %+% another_data_frame reproduces a plot of the same structure with a different data frame The Key Click here Reflections Let’s recap. The author (Wickham) investigates the temporal pattern of death in Mexico to find the causes of death that have unusual temporal patterns within a day. Here are the five steps used in his approach. A. Visualize the overall hourly frequency of death within a day B. Construct variables to compare the relative frequency of death per hour per cause with the overall hourly death rate C. Plot the data to identify a general relationship among key variables D. Create a linear model (i.e., data point = model prediction + residual) E. Visualize the temporal pattern of the “unusual” cases, or the causes of death that have relatively large residuals In a typical application, it will be unlikely that these steps can be followed in order. Most likely, we will have to go back and forth between these steps to define variables, visualize data, and refine a model till we reach the final results. Also, the purpose of creating a model may vary from making predictions to identifying certain parameters or outliers. However, the thought process and techniques in the above exercise will be applicable to many situations of data analysis. References "],
["3-2-next.html", "3.2 Upcoming topics", " 3.2 Upcoming topics Statistical inferences with simulations Linear regressions -->"],
["4-resources.html", "4 Resources", " 4 Resources Here are more resources for learning R. Free Books Official CRAN R Manual Quick R The Art of R Programming by Norman Matloff ModernDive Impatient R Simple R by John Verzani Introduction to Probability and Statistics Using R By Jay Kerns R Wikibook Cookbook for R by Winston Chang OnePageR The R Inferno Videos Coursera’s four week course videos Workflow example video by Jermey Chacon Video on Youtube Tutorials R Tutorial R Bootcamp - Jared Knowles Step-by-step (sequential) interactive tutorial- Try R Another step-by-step interactive tutorial - swirl With Small Fees: Tutorials from DataCamp Cleaning Data in R Data Manipulation in R with dplyr Data Visualization in R with ggvis Data Visualization with ggplot2 Introduction to Coding Coding Resources for Beginners by Tori Dykes "],
["references.html", "References", " References "]
]
