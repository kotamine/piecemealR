[
["index.html", "Piecemeal R Welcome", " Piecemeal R A Tutorial for Data Exploration with R Kota Minegishi Last updated: 2017-06-14 Welcome Welcome to a tutorial website for data analysis and visualization with R. This site provides a quick overview and topic-based tutorials in a piecemeal fashion. The site is organized based on two questions: How best to quickly introduce R to new audiences and showcase its data analytics tools? How best to provide tutorials for topic-based data applications with R? To answer the first question, Section 1 Introduction demonstrates a set of modern data analysis tools in R. Sections 2 describes essential concepts of R. For the second question, Section 3 provides topic-based tutorials. Additional resources are listed in Section 4. More content will be added when the author hosts a small workshop “Data Exploration with R” at his workplace. New Contents 2017-06-14: Section 3.2 Action, Romance, and Chicks *Test upload. VERY Preliminary!* --? "],
["about.html", "About", " About Kota Minegishi is an assistant professor of Dairy Analytics at the University of Minnesota. He is an agricultural economist by training and works in the Department of Animal Science. Workshop random sampling, bootstrapping, and linear models 3.2: TBA dplyr and ggplot2 exercise 3.1: April 20, 5-6pm, Haecker 365. "],
["1-intro.html", "1 Introduction", " 1 Introduction 2017-06-14: VERY Preliminary! A Few Words from the Author R has come a long way in its evolution. Its download page looks pretty much unchanged from years ago but don’t be fooled by its archaic appearance. This piece of the past may be something to do with how the R developer community honors its legacy of turning an open-source project into one of the most popular data analytic tools of today. Please don’t mistake that archaic look as a sign of snobbishness–I hope you too will appreciate it some day. Welcome to the community. In what follows below, we assume that you have R and RStudio Desktop (free IDE) installed. It will be handy to have cheat sheets for base R, RStudio IDE, dplyr, and ggplot2 as well. If you find this introduction too technical, please start with ModernDive open-source textbook (say, up to Chapter 5). That book provided the initial inspiration for me to start this site. Also, more information on R is available in Section 2, as well as various sources listed in Section 4. "],
["1-1-materials.html", "1.1 Materials", " 1.1 Materials The power of R grows with each addition of user-contributed R packages, or a bundle of user-developed programs. Recent developments such as tidy, dplyr, and ggplot2 have greatly streamlined the coding for data manipulation and analysis, which is the starting point for learning R that is chosen for this site. With the R syntax system, you will learn the basic operations of data wrangling and visualization at a rapid pace given that it is an intuitive data operation language. Like any language, its grammar and framework provide a particular way of understanding the world. In this case, it will influence your thinking about data. Following the documentation of dplyr, let’s start with a sample dataset of airplane departures and arrivals. The dataset contains information on about 337,000 flights departed from New York City in 2013 (source: Bureau of Transportation Statistics). We load a built-in data frame by command library(nycflights13) where library(package_name) loads an R package named package_name in the current R session, or the computing environment. In the R console (i.e., the left bottom pane in RStudio), type install.packages(&quot;nycflights13&quot;) and hit enter. Generally, R packages are installed locally on your computer on an as-needed basis. To install several more packages that we will use, copy the following code and execute it in your R console. # Since we&#39;re just starting, don&#39;t worry about understanding the code here # &quot;#&quot;&quot; symbole is used to insert comments that are helpful to humans but are ignored by R required_pkgs &lt;- c(&quot;nycflights13&quot;, &quot;dplyr&quot;, &quot;ggplot2&quot;, &quot;lubridate&quot;, &quot;knitr&quot;, &quot;tidyr&quot;, &quot;broom&quot;) # creating a new object &quot;required_pkgs&quot; containing strings &quot;nycflights13&quot;, &quot;dplyr&quot;,.. # &quot;c()&quot; concatenates string names here. # &quot;&lt;-&quot; operator assigns from the object on the right to left new_pkgs &lt;- required_pkgs[!(required_pkgs %in% installed.packages())] # checking whether &quot;required_pkgs&quot; are already installed # &quot;[]&quot; of required_pkgs[ ] is extraction by logical TRUE or FALSE # &quot;%in%&quot; checks whether items on the left are members of the items on the right. # ! is a negation, so if the package is not installed, there will be an if (length(new_pkgs)) { install.packages(new_pkgs, repos = &quot;http://cran.rstudio.com&quot;) } Once packages are downloaded and installed on your computer, they become available for your libraries. In each R session, we load libraries we need (instead of all existing libraries). Here we load the following; suppressWarnings({ suppressMessages({ library(dplyr) # for data manipulation library(ggplot2) # for figures library(lubridate) # for date manipulation library(nycflights13) # sample data of NYC flights library(knitr) # for table formatting library(tidyr) # for table formatting library(broom) # for table formatting }) }) Let’s see the data. class(flights) # shows the class attribute dim(flights) # obtains dimention of rows and columns ## [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; ## [1] 336776 19 head(flights) # displays first seveal rows and columns ## # A tibble: 6 × 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 ## 2 2013 1 1 533 529 4 850 ## 3 2013 1 1 542 540 2 923 ## 4 2013 1 1 544 545 -1 1004 ## 5 2013 1 1 554 600 -6 812 ## 6 2013 1 1 554 558 -4 740 ## # ... with 12 more variables: sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, ## # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, ## # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, ## # time_hour &lt;dttm&gt; The dim() command returns the dimensions of a data frame, and the head() command returns the first several rows and columns. The flights dataset contains information on dates, actual departure and arrival times, scheduled departure and arrival times, carriers, origins, destinations, travel times, and distances. These variables are arranged in columns, and each row is an observation of flight. In R, we refer to a dataset as data frame, which is a class of R object. The data frame class is more general than the matrix class in that it can contain variables of more than one mode (numeric, character, factor etc). In case you want an overview of data types right away, here is a summary. "],
["1-2-arts-crafts.html", "1.2 Arts &amp; Crafts", " 1.2 Arts &amp; Crafts Crafts We will focus on six data wrangling functions in the dplyr package: filter(): extracts rows (e.g., observations) of a data frame. We put logical vectors in its arguments. select(): extracts columns (e.g., variables) of a data frame. We put column names in its arguments. arrange(): orders rows of a data frame. We put column names in its arguments. summarise(): collapses a data frame into summary statistics. We put summary functions (e.g., statistics functions) using column names in its arguments. mutate(): creates new variables and adds them to the existing columns. We put window functions (e.g., transforming operations) using column names in its arguments. group_by(): assigns rows into groups within a data frame. We put column names in its arguments. The very first argument in all these functions is a data frame, and by using this we can easily pipe a sequence of data wrangling operations through %&gt;% operator. The key is to start with a data frame and then formulate a sequence of data wrangling operations in plain English, which we can translate into code by replacing then in the sequence with the %&gt;% operator. Say, we want to find the average of delays in departures and arrivals from New York to the St. Paul-Minneapolis airport (MSP). We can construct the following sequence of instructions: take the flight data frame, apply filter() to extract the rows of flights to MSP, and then apply summarise() to calculate the mean. flights %&gt;% # take data frame &quot;flights&quot;, then filter(dest == &quot;MSP&quot;) %&gt;% # filter rows, then summarise( # summarise departure and arrival delays for their means # and call them mean_dep_delay and mean_arr_delay respectively mean_dep_delay = mean(dep_delay, na.rm = TRUE), mean_arr_delay = mean(arr_delay, na.rm = TRUE) ) # calculate the mean, while removing NA values ## # A tibble: 1 × 2 ## mean_dep_delay mean_arr_delay ## &lt;dbl&gt; &lt;dbl&gt; ## 1 13.32481 7.270169 In summarise(), one can use summary functions that takes a vector as an input and produces a scaler as an output. This includes functions like mean(), sd() (standard deviation), quantile(), min(), max(), and n() (observation count in the dplyr package). Each time we apply the %&gt;% operator above, we pass a modified data frame from one data operation to another through the first argument. The above code is equivalent to summarise( # data frame &quot;flights&quot; is inside filter(), which is inside summarise() filter(flights, dest == &quot;MSP&quot;), mean_dep_delay = mean(dep_delay, na.rm = TRUE), mean_arr_delay = mean(arr_delay, na.rm = TRUE) ) ## # A tibble: 1 × 2 ## mean_dep_delay mean_arr_delay ## &lt;dbl&gt; &lt;dbl&gt; ## 1 13.32481 7.270169 You will quickly discover that %&gt;% operator makes the code much easier to read, write, and edit and how that might inspire you want to play with the data more. Let’s add a few more lines to the previous example. Say, additionally we want to see the average delay by carrier and sort the results by the number of observations (e.g. flights) in descending order. Okay, what do we do? We make a sequence of data wrangling operations in plain English and translate that into code by replacing then with %&gt;% operator. For example, we say, “take the data frame flights; then (%&gt;%) filter() to extract the rows of flights to MSP; then (%&gt;%) group rows by carrier; then (%&gt;%) summarise() data for the number of observations and the means; then (%&gt;%) arrange() the results by the observation count in descending order.” flight_stats_MSP &lt;- flights %&gt;% # assign the results to an object named &quot;flight_stats&quot; filter(dest == &quot;MSP&quot;) %&gt;% group_by(carrier) %&gt;% # group rows by carrier summarise( n_obs = n(), # count number of rows mean_dep_delay = mean(dep_delay, na.rm = TRUE), mean_arr_delay = mean(arr_delay, na.rm = TRUE) ) %&gt;% arrange(desc(n_obs)) # sort by n_obs in descending order flight_stats_MSP # show flight_stats object ## # A tibble: 6 × 4 ## carrier n_obs mean_dep_delay mean_arr_delay ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 DL 2864 10.651392 4.035702 ## 2 EV 1773 17.093413 10.527995 ## 3 MQ 1293 8.255457 9.559350 ## 4 9E 1249 19.658113 8.089776 ## 5 OO 4 0.750000 -2.000000 ## 6 UA 2 -6.000000 -5.500000 The carrier variable is expressed in the International Air Transportation Association (IATA) code, so let’s add a column of carrier names by joining another data frame called airlines. In RStudio, you can find this data frame under the Environment tab (in the upper right corner); switch the display option from Global Environment to package:nycflights13. To inspect the data frame, type View(airlines) in the R console. Also, by typing data() you can see a list of all datasets that are loaded with libraries. left_join(flight_stats_MSP, airlines, by=&quot;carrier&quot;) %&gt;% # left_join(a,b, by=&quot;var&quot;) joins two data frames a, b by matching rows of b to a # by identifier variable &quot;var&quot;. kable(digits=2) # kable() prints a better-looking table here carrier n_obs mean_dep_delay mean_arr_delay name DL 2864 10.65 4.04 Delta Air Lines Inc. EV 1773 17.09 10.53 ExpressJet Airlines Inc. MQ 1293 8.26 9.56 Envoy Air 9E 1249 19.66 8.09 Endeavor Air Inc. OO 4 0.75 -2.00 SkyWest Airlines Inc. UA 2 -6.00 -5.50 United Air Lines Inc. In the next example, we add new variables to flights using mutate(). flights %&gt;% # keep only columns named &quot;dep_delay&quot; and &quot;arr_delay&quot; select(dep_delay, arr_delay) %&gt;% mutate( gain = arr_delay - dep_delay, gain_rank = round(percent_rank(gain), digits = 2) # Note: we can immediately use the &quot;gain&quot; variable we just defined. ) ## # A tibble: 336,776 × 4 ## dep_delay arr_delay gain gain_rank ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2 11 9 0.81 ## 2 4 20 16 0.88 ## 3 2 33 31 0.94 ## 4 -1 -18 -17 0.22 ## 5 -6 -25 -19 0.18 ## 6 -4 12 16 0.88 ## 7 -5 19 24 0.92 ## 8 -3 -14 -11 0.37 ## 9 -3 -8 -5 0.54 ## 10 -2 8 10 0.82 ## # ... with 336,766 more rows We extracted specific columns of flights by select() and added new columns defined in mutate(). mutate() differs from summarise() in that mutate() adds new columns to the data frame, while summarise() collapses the data frame into a summary table. There are roughly five types of window functions that are commonly used inside mutate(): (1) summary functions, which are interpreted as a vector of repeated values (e.g., a column of an identical mean value): (2) ranking or ordering functions (e.g., row_number(), min_rank(), dense_rank(), cume_dist(), percent_rank(), and ntile()): (3) offset functions, say defining a lagged variable in time series data (lead() and lag()): (4) cumulative aggregates (e.g., cumsum(), cummin(), cummax(), cumall(), cumany(), and cummean()): (5) fixed-window rolling aggregates such as a windowed mean, median, etc. To find help files for these function, for example, type ?cumsum. Before moving to the graphics, let’s quickly go over what a function is in R and how you can use a custom function inside summarise() or mutate(). In R, we use function() to create a function, which has its name, input arguments separated by comma, and a body (e.g., tasks to perform and what to return as an output). your_function_name &lt;- function(input arguments) { task1 task2 . . . output_to_return } For a function having only a single expression to execute, we can omit brackets { }. another_function &lt;- function(input args) task_and_output_in_a_single_expression Let’s go through a few examples. # generate a sequence from 1 to 10 (by the increment of 1) and name it &quot;vec1&quot;. vec1 &lt;- 1:10 vec1 ## [1] 1 2 3 4 5 6 7 8 9 10 # c() concatenates vec2 &lt;- c(vec1, NA, NA) vec2 ## [1] 1 2 3 4 5 6 7 8 9 10 NA NA my_mean_1 &lt;- function(x) mean(x, na.rm = TRUE) # Input arguments: x # Output: the calculation result of mean(x, na.rm = TRUE). # x is required by mean() (and implicitly assumed to be a vector of numeric values). # mean() is an existing function. The &quot;na.rm&quot; argument of mean() is set to be TRUE. my_mean_1(vec1) ## [1] 5.5 my_mean_2 &lt;- function(x, na.rm=TRUE) mean(x, na.rm = na.rm) # Input arguments: x and na.rm (optional with the default value of TRUE) # Output: the calculation result of mean(x, na.rm = na.rm). # The input argument &quot;na.rm&quot; is passed to the input argument &quot;na.rm&quot; of mean() my_mean_2(vec2) ## [1] 5.5 my_mean_2(vec2, na.rm=FALSE) # not removing NA returns NA for the mean calculation. ## [1] NA my_zscore &lt;- function(x, remove_na=TRUE) { (x - my_mean_2(x, na.rm = remove_na))/sd(x, na.rm = remove_na) } # Inputs: x and remove_na (optional: default = TRUE) # Output: z-score of vector x # my_mean2() and sd() return scalers but are interpreted # as a vector of repeated valuses that has the same length as x. my_zscore(vec1) %&gt;% round(2) ## [1] -1.49 -1.16 -0.83 -0.50 -0.17 0.17 0.50 0.83 1.16 1.49 Let’s apply functionsmy_mean_2() and my_zscore() in summarise() and mutate(). flights %&gt;% select(dep_delay) %&gt;% summarise( mean_dep_delay = my_mean_2(dep_delay), # using my_mean_2() mean_dep_delay_na = my_mean_2(dep_delay, na.rm = FALSE) # this returns NA ) %&gt;% kable(digits=2) mean_dep_delay mean_dep_delay_na 12.64 NA flights_gain &lt;- flights %&gt;% select(dep_delay, arr_delay) %&gt;% mutate( gain = arr_delay - dep_delay, gain_z = (gain - my_mean_2(gain))/sd(gain, na.rm=TRUE), # using my_mean_2() gain_z2 = my_zscore(gain_z) # using my_zscore() ) head(flights_gain) %&gt;% # show the first several rows kable(digits=2) dep_delay arr_delay gain gain_z gain_z2 2 11 9 0.81 0.81 4 20 16 1.20 1.20 2 33 31 2.03 2.03 -1 -18 -17 -0.63 -0.63 -6 -25 -19 -0.74 -0.74 -4 12 16 1.20 1.20 Creating a function spares us from writing similar codes in multiple places. While avoiding such repetition is important for making reading and editing code easier, it also reduces coding errors. A situation where you may consider using custom functions is inside functions like summarise_each() and mutate_each(). The two functions allow for applying summary functions like mean() or sd() to each column in a data frame. summarise_each() and mutate_each() work by calling a function by its name. They are very easy to use when an operation is to summarize a vector into a statistics without needing to specify additional arguments, say mean(var1). However, providing additional arguments into a function, say mean(var1, na.rm=TRUE), becomes somewhat cumbersome in terms of its syntax. One approach to get around this problem is to pre-process the data frame before getting to a summarise_each() or mutate_each() section. For example, if we want to test the argument na.rm=TRUE to mean(), we can first filter out rows that contain missing values (NA) and then apply summarise_each(). flights_gain %&gt;% select(dep_delay, arr_delay, gain) %&gt;% filter(!is.na(dep_delay) &amp; !is.na(arr_delay)) %&gt;% # filter out rows that have NA values in dep_delay or arr_deplay summarise_each(&quot;mean&quot;) %&gt;% kable(digits=2) dep_delay arr_delay gain 12.56 6.9 -5.66 The other approach is to use a custom function. For instance, my_mean_2() we defined above has the default argument na.rm=TRUE that gets passed into mean(), effectively overwriting the default argument na.rm=FALSE of mean(). A custom function (as well as any standard summary function) can be called in summarise_each() or mutate_each() using funs() flights_gain %&gt;% select(dep_delay, arr_delay, gain) %&gt;% summarise_each(funs(&quot;my_mean_2&quot;)) %&gt;% kable(digits=2) dep_delay arr_delay gain 12.64 6.9 -5.66 Being able to use your own functions in dplyr-style data wrangling operations will greatly enhance your ability to quickly analyze data in R. Arts Now we will cover the basics of data visualization via the ggplot2 package. The ggplot2 syntax has three essential components for generating graphics: data, aes, and geom. This implements the following philosophy (a quote mentioned in ModernDive); A statistical graphic is a mapping of data variables to aesthetic attributes of geometric objects. — (Wilkinson 2005) While coding complex graphics via ggplot() may appear intimidating at first, it boils down to the three primary components: data: a data frame e.g., the first argument in ggplot(data, ...). geom: geometric objects such as points, lines, bars, etc. with parameters given in the (), e.g., geom_point(), geom_line(), geom_histogram() aes: specifications for x-y variables, as well as variables to differentiate geom objects by color , shape, or size. e.g., aes(x = var_x, y = var_y, shape = var_z) One can refine a plot figure by adding secondary components or characteristics such as stat: data transformation, overlay of statistical inferences etc. scales: scaling data points etc. coord: Cartesian coordinates, polar coordinates, mapping projections etc. facet: laying out multiple plot panels in a grid etc. In what follows below, we will generate five common types of plots: scatter-plots, line-graphs, boxplots, histograms, and barplots. To provide a context, let’s use these plots to investigate what may explain patterns of flight departure delays. First, let’s consider the possibility of congestion at an airport during certain times of the day or certain seasons. We can use barplots to see whether there is any obvious pattern in the flight distribution across flight origins (i.e., airports) in New York City with St. Paul-Minneapolis airport (MSP) as a destination. A barplot shows observation counts (e.g., rows) by category. ggplot(data = flights, # the first argument is the data frame mapping = aes(x = origin)) + # the second argument is &quot;mapping&quot;, which is aes() geom_bar() # after &quot;+&quot; piping operator of ggplot(), we add geom_XXX() elements We can make the plot more informative and aesthetic. ggplot(data = flights, mapping = aes(x = origin, fill = origin)) + # here &quot;fill&quot; gives bars distinct colors geom_bar() + facet_wrap( ~ hour) # &quot;facet_wrap( ~ var)&quot; generates a grid of plots by var Another way to see the same information is a histogram. flights %&gt;% filter(hour &gt;= 5) %&gt;% # exclude hour earlier than 5 a.m. ggplot(aes(x = hour, fill = origin)) + geom_histogram(binwidth = 1, color = &quot;white&quot;) While mornings and late afternoons tend to get busy, there is not much difference in the number of flights across airports. Let’s see if there are distinct patters of departure delays over the course of a year. We do this by taking the average of departure delays for each day by flight origin and plot the data as a time series using line-graphs. delay_day &lt;- flights %&gt;% group_by(origin, year, month, day) %&gt;% summarise(dep_delay = mean(dep_delay, na.rm = TRUE)) %&gt;% mutate(date = as.Date(paste(year, month, day), format=&quot;%Y %m %d&quot;)) %&gt;% filter(!is.na(dep_delay)) # exclude rows with dep_delay == NA delay_day %&gt;% # &quot;facet_grid( var ~ .)&quot; is similar to &quot;facet_wrap( ~ var)&quot; ggplot(aes(x = date, y = dep_delay)) + geom_line() + facet_grid( origin ~ . ) The seasonal pattern seems similar across airports, and summer months appear to be busier on average. Across these airports, let’s see how closely these patterns are related to each other by focusing on a few summer months and making an overlap of the three line-graphs (EWR, JFK, and LGA). delay_day %&gt;% filter(&quot;2013-07-01&quot; &lt;= date, &quot;2013-08-31&quot; &gt;= date) %&gt;% ggplot(aes(x = date, y = dep_delay, color = origin)) + geom_line() We can see similar patterns of spikes across airports occurring on certain days, indicating a tendency for the three airports to get busy on the same days. Would this mean that the three airports tend to be congested at the same time? In the previous figure, there seems to be some cyclical pattern of delays. A good place to start would be comparing delays by day of the week. Here is a function to calculate day of the week for a given date. my_dow &lt;- function(date) { # as.POSIXlt(date)[[&#39;wday&#39;]] returns integers 0, 1, 2, .. 6, for Sun, Mon, ... Sat. # We extract one item from a vector (Sun, Mon, ..., Sat) by position numbered from 1 to 7. dow &lt;- as.POSIXlt(date)[[&#39;wday&#39;]] + 1 c(&quot;Sun&quot;, &quot;Mon&quot;, &quot;Tue&quot;, &quot;Wed&quot;, &quot;Thu&quot;, &quot;Fri&quot;, &quot;Sat&quot;)[dow] # extract &quot;dow&quot;-th element } # Input: date in the format as in &quot;2017-01-23&quot; # Output: day of week Sys.Date() # Sys.Date() returns the current date ## [1] &quot;2017-06-14&quot; my_dow(Sys.Date()) ## [1] &quot;Wed&quot; Now, let’s take a look at the mean delay by day of the week using boxplots. delay_day &lt;- flights %&gt;% group_by(year, month, day) %&gt;% summarise(dep_delay = mean(dep_delay, na.rm = TRUE)) %&gt;% mutate(date = as.Date(paste(year, month, day), format=&quot;%Y %m %d&quot;), # date defined by as.Data() function wday = my_dow(date), weekend = wday %in% c(&quot;Sat&quot;, &quot;Sun&quot;) # %in% operator: A %in% B returns TRUE/FALSE for whether each element of A is in B. ) # show the first 10 elements of &quot;wday&quot; variable in &quot;delay_day&quot; data frame delay_day$wday[1:10] ## [1] &quot;Tue&quot; &quot;Wed&quot; &quot;Thu&quot; &quot;Fri&quot; &quot;Sat&quot; &quot;Sun&quot; &quot;Mon&quot; &quot;Tue&quot; &quot;Wed&quot; &quot;Thu&quot; delay_day$wday &lt;- ordered(delay_day$wday, levels = c(&quot;Mon&quot;, &quot;Tue&quot;, &quot;Wed&quot;, &quot;Thu&quot;, &quot;Fri&quot;, &quot;Sat&quot;, &quot;Sun&quot;)) # adding a sorting order (Mon, Tue, ..., Sun) delay_day$wday[1:10] ## [1] Tue Wed Thu Fri Sat Sun Mon Tue Wed Thu ## Levels: Mon &lt; Tue &lt; Wed &lt; Thu &lt; Fri &lt; Sat &lt; Sun delay_day %&gt;% filter(!is.na(dep_delay)) %&gt;% ggplot(aes(x = wday, y = dep_delay, fill = weekend)) + geom_boxplot() It appears that delays are on average longer on Thursdays and Fridays and shorter on Saturdays. This is plausible if more people are traveling on Thursdays and Fridays before the weekend, and less are traveling on Saturdays to enjoy the weekend. Are Saturdays really less busy? Let’s find out. flights_wday &lt;- flights %&gt;% mutate(date = as.Date(paste(year, month, day), format=&quot;%Y %m %d&quot;), wday = ordered(my_dow(date), levels = c(&quot;Mon&quot;, &quot;Tue&quot;, &quot;Wed&quot;, &quot;Thu&quot;, &quot;Fri&quot;, &quot;Sat&quot;, &quot;Sun&quot;)), weekend = wday %in% c(&quot;Sat&quot;, &quot;Sun&quot;) ) flights_wday %&gt;% group_by(wday) %&gt;% summarise( nobs = n() ) ## # A tibble: 7 × 2 ## wday nobs ## &lt;ord&gt; &lt;int&gt; ## 1 Mon 50690 ## 2 Tue 50422 ## 3 Wed 50060 ## 4 Thu 50219 ## 5 Fri 50308 ## 6 Sat 38720 ## 7 Sun 46357 flights_wday %&gt;% ggplot(aes(x = wday)) + geom_bar() Yes, Saturdays are less busy for the airports in terms of flight numbers. Could we generalize this positive relationship between the number of flights and the average delays, which we find across days of the week? To investigate this, we can summarize the data into the average delays by date-hour and see if the busyness of a particular hour of a particular day is correlated with the mean delay. We visualize these data using a scatter plot. delay_day_hr &lt;- flights %&gt;% group_by(year, month, day, hour) %&gt;% # grouping by date-hour summarise( n_obs = n(), dep_delay = mean(dep_delay, na.rm = TRUE) ) %&gt;% mutate(date = as.Date(paste(year, month, day), format=&quot;%Y %m %d&quot;), wday = my_dow(date) ) plot_delay &lt;- delay_day_hr %&gt;% filter(!is.na(dep_delay)) %&gt;% ggplot(aes(x = n_obs, y = dep_delay)) + geom_point(alpha = 0.1) # plot of n_obs and the average dep_delay # where each point represents an date-hour average # &quot;alpha = 0.1&quot; controls the degree of transparency of points plot_delay Along the horizontal axis, we can see how the number of flights is distributed across date-hours. Some days are busy, and some hours busier still. It appears that there are two clusters in the number of flights, showing very slow date-hours (e.g., less than 10 flights flying out of New York city per hour) and normal date-hours (e.g., about 50 to 70 flights per hour). We could guess that the delays in the slow hours are caused by bad weather. On the other hand, we may wonder if the excess delays in the normal hours, compared to the slow hours, are caused by congestion at the airports. To see this, let’s fit a curve that captures the relationships between n_obs and dep_delay. Our hypothesis is that the delay would become more likely and longer as the number of flights increases. plot_delay + geom_smooth() # geom_smooth() addes a layer of fitted curve(s) ## `geom_smooth()` using method = &#39;gam&#39; We cannot see any clear pattern. How about fitting a curve by day of the week? plot_delay + # additional aes() argument for applying different colors to the day of the week geom_smooth(aes(color = wday), se=FALSE) ## `geom_smooth()` using method = &#39;gam&#39; Surprisingly, the delay does not seem to increase with the flights. There are more delays on Thursdays and Fridays and less delays on Saturdays, but we see no evidence of flight congestion as a cause of delay. Let’s take a closer look at the distribution of the delays. If it is not normally distributed, we may want to apply a transformation. delay_day_hr %&gt;% filter(!is.na(dep_delay)) %&gt;% ggplot(aes(x = dep_delay)) + geom_histogram(color = &quot;white&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. The distribution of the average delays are greatly skewed. In applying a logarithmic transformation, here we have to shift the variable so that its minimum is greater than zero. # define new column called &quot;dep_delay_shifted&quot; delay_day_hr$dep_delay_shifted &lt;- delay_day_hr %&gt;% with(dep_delay - min(dep_delay, na.rm = TRUE) + 1) # with() function takes a data frame in the first argument and allows for # referencing its variable names. delay_day_hr %&gt;% ungroup() %&gt;% # removing group_by() attribute select(dep_delay, dep_delay_shifted) %&gt;% with( apply(., 2, summary) # apply(data, num, fun) applies function &quot;fun&quot; for each item # in dimension &quot;num&quot; (1 = cows, 2= columns) of the data frame # Data referenced by &quot;.&quot; means all variables of the dataset inside with(). ) %&gt;% t() # transpose rows and columns ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## dep_delay -18 1.054 6.571 12.99 15.44 269 13 ## dep_delay_shifted 1 20.050 25.570 31.99 34.44 288 13 Now the transformed distribution; # Under the log of 10 transformation, the distribution looks closer to a normal distribution. delay_day_hr %&gt;% filter(!is.na(dep_delay_shifted)) %&gt;% ggplot(aes(x = dep_delay_shifted)) + scale_x_log10() + geom_histogram(color = &quot;white&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. # Alternatively, one can apply the natural logarithm to transform a variable. Histogram shows no difference here. delay_day_hr %&gt;% filter(!is.na(dep_delay_shifted)) %&gt;% ggplot(aes(x = dep_delay_shifted)) + scale_x_continuous(trans = &quot;log&quot;) + geom_histogram(color = &quot;white&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. The transformed distribution is much less skewed than the original. Now, let’s plot the relationship between delays and flights again. delay_day_hr %&gt;% filter(!is.na(dep_delay_shifted), dep_delay_shifted &gt; 5) %&gt;% ggplot(aes(x = n_obs, y = dep_delay_shifted)) + scale_y_log10() + # using transformation scale_y_log10() geom_point(alpha = 0.1) + geom_smooth() ## `geom_smooth()` using method = &#39;gam&#39; We still do not see a pattern that busier hours have more delays. This seems to suggest that the airports in New York City manage the fluctuating number of flights without causing congestion. References "],
["1-3-huning-down-numbers.html", "1.3 Huning down numbers", " 1.3 Huning down numbers This section is optional but contains more examples of dplyr and ggplot2 functions. Previously, we find that the congestion at the airports is unlikely the cause of delays. Then, what else may explain the patterns of delays? Are the airlines partly responsible? Recall that earlier we observe that some airlines have longer delays than others for NYC-MSP flights. Let’s take a look at the overall average delays by carrier. stat_carrier &lt;- flights %&gt;% group_by(carrier) %&gt;% summarise(n_obs = n(), dep_delay = mean(dep_delay, na.rm = TRUE), arr_delay = mean(arr_delay, na.rm = TRUE) ) %&gt;% left_join(airlines, by=&quot;carrier&quot;) %&gt;% arrange(desc(n_obs)) stat_carrier %&gt;% kable(digit=2) carrier n_obs dep_delay arr_delay name UA 58665 12.11 3.56 United Air Lines Inc. B6 54635 13.02 9.46 JetBlue Airways EV 54173 19.96 15.80 ExpressJet Airlines Inc. DL 48110 9.26 1.64 Delta Air Lines Inc. AA 32729 8.59 0.36 American Airlines Inc. MQ 26397 10.55 10.77 Envoy Air US 20536 3.78 2.13 US Airways Inc. 9E 18460 16.73 7.38 Endeavor Air Inc. WN 12275 17.71 9.65 Southwest Airlines Co. VX 5162 12.87 1.76 Virgin America FL 3260 18.73 20.12 AirTran Airways Corporation AS 714 5.80 -9.93 Alaska Airlines Inc. F9 685 20.22 21.92 Frontier Airlines Inc. YV 601 19.00 15.56 Mesa Airlines Inc. HA 342 4.90 -6.92 Hawaiian Airlines Inc. OO 32 12.59 11.93 SkyWest Airlines Inc. There could be some differences across carriers. However, the simple average of delays across various routes, days, and hours of flights may not be a good measure to compare the carriers. For example, some carriers may serve the routes and hours that tend to have more delays. Also, given that our dataset covers only the flights from New York City, the comparison may not be nationally representative since carriers use different airports around the country for their regional hubs. For our purposes, let’s compare the average air time among carriers, while accounting for flight’s destination and timing. The differences in air time are not the same as the differences in delays, but they may indicate some efficiency difference among carriers. Let’s first check how air time relates to flight distance. flights %&gt;% filter (month == 1, day == 1, !is.na(air_time)) %&gt;% ggplot(aes(x = distance, y = air_time)) + geom_point(alpha = 0.05) + geom_smooth() ## `geom_smooth()` using method = &#39;loess&#39; air_time and distance show a general linear relationship. We can better account for this relationship if we calculate the average air time for each flight destination from New York City. First, we will consider a simple approach to control for such average air time for each destination and compare the variation in air time among carriers. We can do this by fitting a linear regression model with fixed destination effects and comparing the residuals. This resembles the ANOVA for comparing the mean air times among carriers, but the fixed destination effects here difference out the average air time for each destination from the total variation. # a copy of flights data flights2 &lt;- flights # TRUE/FALSE vector showing whther air_time is not NA. idx0 &lt;- flights %&gt;% with(!is.na(air_time)) flights2$res &lt;- NA # prepare a column of residuals to be defined below flights2$res[idx0] &lt;- flights2 %&gt;% # replace rows with idx0 = TRUE filter(!is.na(air_time)) %&gt;% with( lm( air_time ~ as.factor(dest)) # lm() estimates a linear model. # &quot;y ~ x&quot;&quot; is the formula for regressing y on x. # as.factor() converts &quot;dest&quot; to a factor (categorical) class # which is used as a set of dummy variables in the regression. ) %&gt;% residuals() # obtains residuals of the lm() object stat_res &lt;- flights2 %&gt;% group_by(carrier) %&gt;% summarise( mean_res = mean(res, na.rm = TRUE), # mean residual by carrier sd_res = sd(res, na.rm = TRUE) ) left_join(stat_carrier, stat_res, by=&quot;carrier&quot;) %&gt;% kable(digit=2) carrier n_obs dep_delay arr_delay name mean_res sd_res UA 58665 12.11 3.56 United Air Lines Inc. -0.87 14.59 B6 54635 13.02 9.46 JetBlue Airways 0.28 11.55 EV 54173 19.96 15.80 ExpressJet Airlines Inc. -0.37 8.94 DL 48110 9.26 1.64 Delta Air Lines Inc. -0.20 12.32 AA 32729 8.59 0.36 American Airlines Inc. 0.68 13.86 MQ 26397 10.55 10.77 Envoy Air 0.45 8.87 US 20536 3.78 2.13 US Airways Inc. -0.42 9.43 9E 18460 16.73 7.38 Endeavor Air Inc. 0.84 8.76 WN 12275 17.71 9.65 Southwest Airlines Co. 0.16 12.55 VX 5162 12.87 1.76 Virgin America 3.26 17.58 FL 3260 18.73 20.12 AirTran Airways Corporation 1.16 8.75 AS 714 5.80 -9.93 Alaska Airlines Inc. -2.13 16.17 F9 685 20.22 21.92 Frontier Airlines Inc. 3.12 15.16 YV 601 19.00 15.56 Mesa Airlines Inc. -0.05 7.06 HA 342 4.90 -6.92 Hawaiian Airlines Inc. 5.64 20.69 OO 32 12.59 11.93 SkyWest Airlines Inc. 1.02 7.26 The differences in air time across carriers (“mean_res”) somewhat differ from the patterns of differences in the simple averages of delays (“dep_delay” and “arr_delay”). The patterns are different between “dep_delay” and “arr_delay” for that matter. To some extent, it appears to make sense that the average air time is longer for low-cost carriers such as Virgin America, Frontier Airlines, and Hawaiian Airlines. The differences across other carriers, on the other hand, are small, compared to the standard deviations. To get a sense of whether these differences have any statistical significance, let’s use t-test to compare the mean residual between United Airlines and American Airlines. # t-test comparing UA vs AA for the mean air time flights2 %&gt;% with({ idx_UA &lt;- carrier == &quot;UA&quot; idx_AA &lt;- carrier == &quot;AA&quot; t.test(res[idx_UA], res[idx_AA]) }) ## ## Welch Two Sample t-test ## ## data: res[idx_UA] and res[idx_AA] ## t = -15.722, df = 68826, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -1.741133 -1.355142 ## sample estimates: ## mean of x mean of y ## -0.8689523 0.6791852 With a large number of observations, a seemingly small difference in the means often turns out to be a statistically significant difference. Nonetheless, statistical significance is not sufficient for being an empirically significant difference that matters in the real world. The average difference of about 1.5 minute air time per flight appears very small. In fact, we can do this sort of pair-wise comparison all at once using a regression. Using carrier fixed effects in addition to destination fixed effects, we can directly compare the mean effects across carriers. We will set United Airlines to be a reference of the carrier fixed effects, so that the fixed effect for United Airlines is set to zero (i.e., omitted category), from which the fixed effects of all other airlines are estimated. flights2$carrier &lt;- relevel(factor(flights2$carrier), ref=&quot;UA&quot;) # reference level is United Airlines flights2$carrier %&gt;% table() ## . ## UA 9E AA AS B6 DL EV F9 FL HA MQ OO ## 58665 18460 32729 714 54635 48110 54173 685 3260 342 26397 32 ## US VX WN YV ## 20536 5162 12275 601 flights2 %&gt;% with({ n_carrier &lt;- unique(carrier) %&gt;% length() n_dest &lt;- unique(dest) %&gt;% length() print(paste(&#39;There are&#39;, n_carrier, &#39;distinct carriers and&#39;, n_dest,&#39;distinct destinations in the data.&#39; )) }) ## [1] &quot;There are 16 distinct carriers and 105 distinct destinations in the data.&quot; With 16 carriers and 105 destinations minus 2 reference levels for carriers and destinations, the total of 119 coefficients will be estimated for the fixed effects. f1 &lt;- flights2 %&gt;% with( lm( air_time ~ as.factor(carrier) + as.factor(dest) ) # fixed effects for carriers and destinations ) tidy(f1)[1:20,] # show the first 20 coefficients ## term estimate std.error statistic p.value ## 1 (Intercept) 247.9884874 0.75069658 330.3445016 0.000000e+00 ## 2 as.factor(carrier)9E 1.8015498 0.12723996 14.1586788 1.702649e-45 ## 3 as.factor(carrier)AA 1.9326712 0.09731105 19.8607572 1.002388e-87 ## 4 as.factor(carrier)AS -1.9071536 0.49596319 -3.8453531 1.204017e-04 ## 5 as.factor(carrier)B6 1.1808039 0.08495098 13.8998267 6.535025e-44 ## 6 as.factor(carrier)DL 0.7531812 0.08722600 8.6348244 5.907432e-18 ## 7 as.factor(carrier)EV 0.4174574 0.11044837 3.7796605 1.570702e-04 ## 8 as.factor(carrier)F9 3.8891981 0.48090201 8.0872985 6.120836e-16 ## 9 as.factor(carrier)FL 2.6434074 0.27600661 9.5773336 1.002386e-21 ## 10 as.factor(carrier)HA 11.0125104 0.89821710 12.2604106 1.503557e-34 ## 11 as.factor(carrier)MQ 1.4592669 0.11892133 12.2708590 1.321669e-34 ## 12 as.factor(carrier)OO 1.8091432 2.21222472 0.8177936 4.134757e-01 ## 13 as.factor(carrier)US 0.1319337 0.13826299 0.9542230 3.399715e-01 ## 14 as.factor(carrier)VX 4.5298528 0.18441295 24.5636378 4.086448e-133 ## 15 as.factor(carrier)WN 1.2226161 0.17520980 6.9780125 2.999500e-12 ## 16 as.factor(carrier)YV 0.5167461 0.52737831 0.9798395 3.271661e-01 ## 17 as.factor(dest)ACK -207.1011095 1.04478912 -198.2228803 0.000000e+00 ## 18 as.factor(dest)ALB -216.6188634 0.95130943 -227.7059972 0.000000e+00 ## 19 as.factor(dest)ANC 165.1365126 4.26930687 38.6799351 0.000000e+00 ## 20 as.factor(dest)ATL -136.1282095 0.75641976 -179.9638460 0.000000e+00 # a function to clean up the coefficient table above clean_lm_rlt &lt;- function(f) { # keep only rows for which column &quot;term&quot; contains &quot;carrier&quot; e.g., rows 2 to 16 above rlt &lt;- tidy(f) %&gt;% filter(grepl(&quot;carrier&quot;,term)) # create column named carrier rlt &lt;- rlt %&gt;% mutate(carrier = gsub(&#39;as.factor\\\\(carrier\\\\)&#39;,&#39;&#39;, term)) # drop column term rlt &lt;- rlt %&gt;% select(-term) # add columns of carrier, name, and n_obs from the stat_carrier data frame stat_carrier %&gt;% select(carrier, name, n_obs) %&gt;% left_join(rlt, by=&quot;carrier&quot;) } lm_rlt1 &lt;- clean_lm_rlt(f1) lm_rlt1 %&gt;% kable(digit=2) carrier name n_obs estimate std.error statistic p.value UA United Air Lines Inc. 58665 NA NA NA NA B6 JetBlue Airways 54635 1.18 0.08 13.90 0.00 EV ExpressJet Airlines Inc. 54173 0.42 0.11 3.78 0.00 DL Delta Air Lines Inc. 48110 0.75 0.09 8.63 0.00 AA American Airlines Inc. 32729 1.93 0.10 19.86 0.00 MQ Envoy Air 26397 1.46 0.12 12.27 0.00 US US Airways Inc. 20536 0.13 0.14 0.95 0.34 9E Endeavor Air Inc. 18460 1.80 0.13 14.16 0.00 WN Southwest Airlines Co. 12275 1.22 0.18 6.98 0.00 VX Virgin America 5162 4.53 0.18 24.56 0.00 FL AirTran Airways Corporation 3260 2.64 0.28 9.58 0.00 AS Alaska Airlines Inc. 714 -1.91 0.50 -3.85 0.00 F9 Frontier Airlines Inc. 685 3.89 0.48 8.09 0.00 YV Mesa Airlines Inc. 601 0.52 0.53 0.98 0.33 HA Hawaiian Airlines Inc. 342 11.01 0.90 12.26 0.00 OO SkyWest Airlines Inc. 32 1.81 2.21 0.82 0.41 The “estimate” column shows the mean difference in air time with United Airlines, accounting for the flight destination. The estimate tends to be more precise (i.e., smaller standard errors) for carriers with a larger number of observations. This time, we find that Virgin America, Air Tran, Frontier Airlines, and Hawaiian Airlines tend to show particularly longer air times than United Airlines. Next, let’s take a step further to account for flight timing as well. We can do this by adding fixed effects for flight dates and hours. flights2 &lt;- flights2 %&gt;% mutate( date_id = month*100 + day ) flights2$date_id %&gt;% unique() %&gt;% length() ## [1] 365 f2 &lt;- flights2 %&gt;% with( lm( air_time ~ as.factor(carrier) + as.factor(dest) + + as.factor(date_id) + as.factor(hour) ) ) lm_rlt2 &lt;- clean_lm_rlt(f2) lm_rlt2 %&gt;% kable(digit=2) carrier name n_obs estimate std.error statistic p.value UA United Air Lines Inc. 58665 NA NA NA NA B6 JetBlue Airways 54635 1.60 0.07 22.50 0.00 EV ExpressJet Airlines Inc. 54173 0.61 0.09 6.67 0.00 DL Delta Air Lines Inc. 48110 0.95 0.07 13.03 0.00 AA American Airlines Inc. 32729 1.84 0.08 22.81 0.00 MQ Envoy Air 26397 1.45 0.10 14.70 0.00 US US Airways Inc. 20536 0.17 0.11 1.51 0.13 9E Endeavor Air Inc. 18460 1.57 0.11 14.72 0.00 WN Southwest Airlines Co. 12275 1.14 0.15 7.82 0.00 VX Virgin America 5162 4.85 0.15 31.57 0.00 FL AirTran Airways Corporation 3260 2.19 0.23 9.58 0.00 AS Alaska Airlines Inc. 714 -2.55 0.41 -6.21 0.00 F9 Frontier Airlines Inc. 685 3.31 0.40 8.29 0.00 YV Mesa Airlines Inc. 601 0.32 0.44 0.73 0.46 HA Hawaiian Airlines Inc. 342 11.79 0.75 15.80 0.00 OO SkyWest Airlines Inc. 32 7.63 1.83 4.17 0.00 lm_rlt2 %&gt;% filter(carrier!=&#39;UA&#39;) %&gt;% ggplot(aes(x = carrier, y = estimate)) + geom_col() + labs(title = &quot;Mean Air Time Compared to United Airlines&quot;) The results are similar to the previous linear mode except that this time SkyWest Airlines shows much longer air time. Before wrapping up, our final model is a check for the robustness of the above results. We would like to replace the date and hour fixed effects in the previous model with date-hour fixed effects (i.e., the interaction between date and hour). We could add such fixed effects using time_hour variable defined above. However, that would mean adding nearly 7,000 dummy variables to our linear regression, which is computationally too intensive. To work around this issue, we approximate this estimation by pre-processing the dependent variable. Specifically, we calculate the average air time for each combination of time_hour and dest and define a new dependent variable by subtracting this average value from the original air time variable (i.e., the new variable is centered at zero-mean for each combination of time_hour and dest). Then, we estimate a linear model with carrier and destination fixed effects. ## Adding time_hour fixed effects is too computationally intensive # f1 &lt;- flights %&gt;% # with( # lm( air_time ~ as.factor(carrier) + as.factor(dest) + as.factor(time_hour)) # ) unique(flights2$time_hour) %&gt;% length() # 6,936 unique time_hour ## [1] 6936 flights2 &lt;- flights2 %&gt;% group_by(dest, time_hour) %&gt;% mutate( air_time_centered = air_time - mean(air_time, na.rm=TRUE) ) f3 &lt;- flights2 %&gt;% with( lm( air_time_centered ~ as.factor(carrier) + as.factor(dest) ) ) lm_rlt3 &lt;- clean_lm_rlt(f3) lm_rlt3 %&gt;% kable(digit=2) # Note: standard errors, t-stat, and p-val are incorrect carrier name n_obs estimate std.error statistic p.value UA United Air Lines Inc. 58665 NA NA NA NA B6 JetBlue Airways 54635 0.82 0.03 32.24 0.00 EV ExpressJet Airlines Inc. 54173 0.88 0.03 26.50 0.00 DL Delta Air Lines Inc. 48110 0.52 0.03 19.85 0.00 AA American Airlines Inc. 32729 1.20 0.03 41.06 0.00 MQ Envoy Air 26397 1.00 0.04 27.84 0.00 US US Airways Inc. 20536 -0.09 0.04 -2.21 0.03 9E Endeavor Air Inc. 18460 1.27 0.04 33.07 0.00 WN Southwest Airlines Co. 12275 1.30 0.05 24.70 0.00 VX Virgin America 5162 3.47 0.06 62.59 0.00 FL AirTran Airways Corporation 3260 1.78 0.08 21.48 0.00 AS Alaska Airlines Inc. 714 -2.86 0.15 -19.15 0.00 F9 Frontier Airlines Inc. 685 1.99 0.14 13.73 0.00 YV Mesa Airlines Inc. 601 0.78 0.16 4.89 0.00 HA Hawaiian Airlines Inc. 342 1.34 0.27 4.96 0.00 OO SkyWest Airlines Inc. 32 3.50 0.67 5.26 0.00 lm_rlt3 %&gt;% filter(carrier!=&#39;UA&#39;) %&gt;% ggplot(aes(x = carrier, y = estimate)) + geom_col() + labs(title = &quot;Mean Air Time Compared to United Airlines: Robustness Check&quot;) The point estimates should be approximately close to what we would obtain if we regress air_time on the fixed effects of carrier, dest, and time_hour. However, the standard errors are not correctly displayed in the table because the centered variable has a smaller total variation compared to the original air_time variable. (Correct standard errors can be obtained, for example, through a bootstrapping technique.) Overall, we see again a tendency that lower-cost carriers like Sky West Airlines, Virgin America, Frontier Airlines, and Air Tran show particularly longer air time than United Airlines. Jet Blue Airways, another low-cost carrier, shows a less obvious difference from United Airlines, possibly suggesting that their operation focused on the East Cost is efficient for the flights departing from New York City. Hawaiian Airlines and Alaskan Airlines appear to be somewhat different from other carriers perhaps because they are more specialized in particular flight times and destinations compared to their rivals. In particular, the flights to Hawaii may have distinct delay patterns that are concentrated on certain date-hours of the peak vacation seasons. "],
["1-4-reflections.html", "1.4 Reflections", " 1.4 Reflections In this introduction, we have reviewed the tools of deplyr and ggplot2 packages as a starting point for data analyses and visualization in R. This new generation of tools is a data exploration language as much as a set of functions to shortcut traditional data manipulation methods in R. This language provides an intuitive system of translating our inquiries to the data analysis in R. Using the flight dataset, we have also investigated flight delay patterns. We find that airport congestion is unlikely a major cause of delay in New York City. There are small differences in the air time (e.g. less than 5 minutes) across carriers for a given destination although it remains unclear how this relates to the delay patterns. In fact, the concept of “delay” is complicated because it is defined in reference to the scheduled departure and arrival times, which may differ by carrier. A delay would not include the time sitting in the airplane before taking off or after landing as long as it is within the schedule. It might be more interesting to compare scheduled flight duration instead of delays or air time. (Such an analysis would involve somewhat complicated manipulations of date and time with our flight data.) This leads us to the final point of this exercise; an interesting data analysis requires knowledge on the real-world process that generated the data and the ability to ask interesting questions. deplyr and ggplot2 packages can let you employ a variety of data analytics tools with ease, but the ultimate power of the analysis will always rest on your knowledge and creativity. -->"],
["2-essentials.html", "2 Essentials", " 2 Essentials 2017-06-14: VERY Preliminary! This section provides an overview of the essential concepts for manipulating data and programming in R. "],
["2-1-cheatsheets.html", "2.1 Cheatsheets", " 2.1 Cheatsheets Cheatsheets are useful for glancing at various functions. Base R RStudio IDE dplyr ggplot2 "],
["2-2-data-types.html", "2.2 Data types", " 2.2 Data types 2.2.1 Atomic In most cases, each atomic element has a type (mode) of numeric: number logical: TRUE or FALSE (T or F for shortcuts) character: character string factor: a level of categorical variable Other types include date and nonexistent NULL. The factor is also a class of its own, meaning that many R functions apply operations that are specific to the factor class. # assess objects 123, &quot;abc&quot;, and TRUE for their types str(123) # str() returns the structure ## num 123 str(&quot;abc&quot;) ## chr &quot;abc&quot; str(TRUE) ## logi TRUE c(is.numeric(123), is.numeric(&quot;abc&quot;), is.numeric(TRUE)) ## [1] TRUE FALSE FALSE c(is.logical(123), is.logical(&quot;abc&quot;), is.logical(TRUE)) ## [1] FALSE FALSE TRUE c(is.character(123), is.character(&quot;abc&quot;), is.character(TRUE)) ## [1] FALSE TRUE FALSE # &quot;&lt;-&quot; means an assignment from right to left factor1 &lt;- as.factor(c(1,2,3)) # Looks like numeric but not factor1 ## [1] 1 2 3 ## Levels: 1 2 3 factor2 &lt;- as.factor(c(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;)) # Looks like characters but not factor2 ## [1] a b c ## Levels: a b c factor3 &lt;- as.factor(c(TRUE,FALSE,T)) # Looks like logicals but not factor3 ## [1] TRUE FALSE TRUE ## Levels: FALSE TRUE c(is.factor(factor1[1]), is.factor(factor2[1]), is.factor(factor3[1])) ## [1] TRUE TRUE TRUE # Extract the first element (factor1[1] etc.) factor1[1] ## [1] 1 ## Levels: 1 2 3 factor2[2] ## [1] b ## Levels: a b c factor3[3] ## [1] TRUE ## Levels: FALSE TRUE NULL has zero-length. Also, empty numeric, logical, and character objects have zero-length. length(NULL) ## [1] 0 length(numeric(0)) # numeric(N) returns a vector of N zeros ## [1] 0 length(logical(0)) # logical(N) returns a vector of N FALSE objects ## [1] 0 length(character(0)) # character(N) returns a vector of N &quot;&quot; objects ## [1] 0 Each vector has a type of numeric, logical, character, or factor. Each matrix has a type of numeric, logical, or character. A data frame can contain mixed types across columns where each column (e.g., a variable) has a type of numeric, logical, character or factor. vector1 &lt;- c(1, NA, 2, 3) # read as numeric vector1 ## [1] 1 NA 2 3 vector2 &lt;- c(TRUE, FALSE, T, F) # read as logical vector2 ## [1] TRUE FALSE TRUE FALSE vector3 &lt;- c(1, NA, &quot;abc&quot;, TRUE, &quot;TRUE&quot;) # read as character vector3 ## [1] &quot;1&quot; NA &quot;abc&quot; &quot;TRUE&quot; &quot;TRUE&quot; vector4 &lt;- as.factor(c(1, NA, &quot;abc&quot;, TRUE, &quot;TRUE&quot;)) # read as factor vector4 ## [1] 1 &lt;NA&gt; abc TRUE TRUE ## Levels: 1 abc TRUE matrix1 &lt;- matrix(c(1:6), nrow = 3) # read as numeric matrix1 ## [,1] [,2] ## [1,] 1 4 ## [2,] 2 5 ## [3,] 3 6 matrix2 &lt;- matrix(c(TRUE,FALSE,rep(T,3),F), nrow = 3) # read as logical matrix2 ## [,1] [,2] ## [1,] TRUE TRUE ## [2,] FALSE TRUE ## [3,] TRUE FALSE matrix3 &lt;- matrix(c(1,2,3,&quot;a&quot;,&quot;b&quot;,&quot;abc&quot;), nrow = 3) # read as character matrix3 ## [,1] [,2] ## [1,] &quot;1&quot; &quot;a&quot; ## [2,] &quot;2&quot; &quot;b&quot; ## [3,] &quot;3&quot; &quot;abc&quot; df1 &lt;- data.frame( num = c(1,2,3), # read as numeric fac1 = c(&quot;a&quot;,&quot;b&quot;,&quot;abc&quot;), # read as factor logi = c(TRUE, FALSE, T), # read as logical fac2 = c(1,&quot;a&quot;,TRUE) # read as factor ) df1 ## num fac1 logi fac2 ## 1 1 a TRUE 1 ## 2 2 b FALSE a ## 3 3 abc TRUE TRUE df1$num # &quot;$&quot; symbol is used to extract a column ## [1] 1 2 3 df1$fac1 # character type is converted into a factor ## [1] a b abc ## Levels: a abc b df1$logi ## [1] TRUE FALSE TRUE df1$fac2 # mixed types within a column is converted into a factor ## [1] 1 a TRUE ## Levels: 1 a TRUE # additional argument &quot;stringsAsFactors = FALSE&quot; preserves character types. df2 &lt;- data.frame( num = c(1,2,3), # read as numeric char = c(&quot;a&quot;,&quot;b&quot;,&quot;abc&quot;), # read as character logi = c(TRUE, FALSE, T), # read as logical fac2 = as.factor(c(1,&quot;a&quot;,TRUE)), # read as factor stringsAsFactors = FALSE ) df2 ## num char logi fac2 ## 1 1 a TRUE 1 ## 2 2 b FALSE a ## 3 3 abc TRUE TRUE df2$num ## [1] 1 2 3 df2$char ## [1] &quot;a&quot; &quot;b&quot; &quot;abc&quot; df2$logi ## [1] TRUE FALSE TRUE df2$fac2 ## [1] 1 a TRUE ## Levels: 1 a TRUE 2.2.2 Factor A factor object is defined with a set of categorical levels, which may be labeled. The levels are either ordered (defined by ordered()) or unordered (defined by factor()). Ordered factor objects are treated in the specific order by certain statistical and graphical procedures. # We will convert the columns of df into factors df &lt;- data.frame( fac1 = c(0,1,1,4,4,2,2,3), fac2 = c(1,2,3,1,1,2,2,3), fac3 = c(4,2,3,4,4,2,2,3) ) # convert fac1 to ordered factors df$fac1 &lt;- ordered(df$fac1, levels = c(0,4,3,2,1) # defines the order ) df$fac1 ## [1] 0 1 1 4 4 2 2 3 ## Levels: 0 &lt; 4 &lt; 3 &lt; 2 &lt; 1 summary(df$fac1) # gives the table of counts for each level ## 0 4 3 2 1 ## 1 2 1 2 2 # convert fac2 to unordered factors with labels df$fac2 &lt;- factor(df$fac2, levels = c(1,2,3), # no particular order # attach labels to factors: 1=red, 2=blue, 3=green labels = c(&quot;red&quot;, &quot;blue&quot;, &quot;green&quot;) ) df$fac2 ## [1] red blue green red red blue blue green ## Levels: red blue green summary(df$fac2) ## red blue green ## 3 3 2 # convert fac3 to ordered factors with labels df$fac3 &lt;- ordered(df$fac3, levels = c(2,3,4), # attach labels to factors: 2=Low, 3=Medium, 4=High labels = c(&quot;Low&quot;, &quot;Medium&quot;, &quot;High&quot;) ) df$fac3 ## [1] High Low Medium High High Low Low Medium ## Levels: Low &lt; Medium &lt; High summary(df$fac3) ## Low Medium High ## 3 2 3 2.2.3 Matrix matrix() defines a matrix from a vector. The default is to arrange the vector by column (byrow = FALSE). # byrow = FALSE (the default) matrix(data = c(1:6), nrow = 2, ncol = 3, byrow = FALSE, dimnames = NULL) ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 2 4 6 # byrow = TRUE matrix(data = c(1:6), nrow = 2, ncol = 3, byrow = TRUE, dimnames = NULL) ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 4 5 6 # give row and column names to a matrix mat1 &lt;- matrix(data = c(1:6), nrow = 2, ncol = 3, byrow = FALSE, dimnames = list(c(&quot;r1&quot;,&quot;r2&quot;), c(&quot;c1&quot;,&quot;c2&quot;,&quot;c3&quot;))) mat1 ## c1 c2 c3 ## r1 1 3 5 ## r2 2 4 6 dim(mat1) # dimension: row by column ## [1] 2 3 colnames(mat1) ## [1] &quot;c1&quot; &quot;c2&quot; &quot;c3&quot; rownames(mat1) ## [1] &quot;r1&quot; &quot;r2&quot; colnames(mat1) &lt;- c(&quot;v1&quot;,&quot;v2&quot;,&quot;v3&quot;) # change column names by assignment &quot;&lt;-&quot; mat1 ## v1 v2 v3 ## r1 1 3 5 ## r2 2 4 6 # R makes a guess when only nrow or ncol is supplied matrix(data = c(1:6), nrow = 2) ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 2 4 6 matrix(data = c(1:6), ncol = 3) ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 2 4 6 # combine matrices by column via &quot;cbind()&quot; or by row via &quot;rbind()&quot; cbind(mat1,mat1) ## v1 v2 v3 v1 v2 v3 ## r1 1 3 5 1 3 5 ## r2 2 4 6 2 4 6 rbind(mat1,mat1) ## v1 v2 v3 ## r1 1 3 5 ## r2 2 4 6 ## r1 1 3 5 ## r2 2 4 6 There are recycling rules (which does/controls what?) in R. # the vector shorter than the length of all elements of a matrix matrix(data = c(1:4), nrow = 2, ncol= 3) ## Warning in matrix(data = c(1:4), nrow = 2, ncol = 3): data length [4] is ## not a sub-multiple or multiple of the number of columns [3] ## [,1] [,2] [,3] ## [1,] 1 3 1 ## [2,] 2 4 2 # R treats a scaler as a vector of length that conforms cbind() or rbind() cbind(mat1, colA = 1) ## v1 v2 v3 colA ## r1 1 3 5 1 ## r2 2 4 6 1 rbind(mat1, rowA= 1, rowB= 2, rowC= 3) ## v1 v2 v3 ## r1 1 3 5 ## r2 2 4 6 ## rowA 1 1 1 ## rowB 2 2 2 ## rowC 3 3 3 To replace elements of a matrix, we can use assignment operator &lt;-. mat1[1,1] &lt;- 10 mat1 ## v1 v2 v3 ## r1 10 3 5 ## r2 2 4 6 mat1[,2] &lt;- c(7,8) mat1 ## v1 v2 v3 ## r1 10 7 5 ## r2 2 8 6 mat1[,1] &lt;- 0 # recycling rule mat1 ## v1 v2 v3 ## r1 0 7 5 ## r2 0 8 6 Matrix allows for easy extraction for rows and columns separated by comma. mat1 ## v1 v2 v3 ## r1 0 7 5 ## r2 0 8 6 mat1[1, ] # row = 1 and all columns ## v1 v2 v3 ## 0 7 5 mat1[, 1] # all rows and col = 1 ## r1 r2 ## 0 0 mat1[c(TRUE,FALSE),] # by a logical vector ## v1 v2 v3 ## 0 7 5 mat1[, c(TRUE,FALSE)] ## v1 v3 ## r1 0 5 ## r2 0 6 mat1[2,3] # row = 2 and col = 3 ## [1] 6 mat1[1:2, 2:3] # row = 1:2 and col = 2:3 ## v2 v3 ## r1 7 5 ## r2 8 6 mat1[1:2, 2:3][2,2] # subset of a subset ## [1] 6 mat1[, 1][2] # vector extraction is done with one-dimensional index ## r2 ## 0 Important: when a single row or column is extracted, it gets converted to a vector with no dimension. mat1[1,] ## v1 v2 v3 ## 0 7 5 is.matrix(mat1[1, ]) ## [1] FALSE dim(mat1[1,]) ## NULL length(mat1[1, ]) ## [1] 3 # to keep a row or column vector structure, use drop = FALSE mat1[1,, drop = FALSE] ## v1 v2 v3 ## r1 0 7 5 is.matrix(mat1[1,,drop = FALSE]) ## [1] TRUE dim(mat1[1,,drop = FALSE]) ## [1] 1 3 length(mat1[1,,drop = FALSE]) ## [1] 3 mat1[,1, drop = FALSE] ## v1 ## r1 0 ## r2 0 is.matrix(mat1[,1,drop = FALSE]) ## [1] TRUE dim(mat1[,1,drop = FALSE]) ## [1] 2 1 length(mat1[,1,drop = FALSE]) ## [1] 2 Another way of extraction from a matrix is to use row or column names. mat1[,&#39;v1&#39;] ## r1 r2 ## 0 0 mat1[,c(&#39;v1&#39;,&#39;v3&#39;)] ## v1 v3 ## r1 0 5 ## r2 0 6 mat1[&#39;r2&#39;,,drop= FALSE] ## v1 v2 v3 ## r2 0 8 6 apply() applies a function for a specified margin (dimension index number) of the matrix. mat1 ## v1 v2 v3 ## r1 0 7 5 ## r2 0 8 6 apply(mat1,1,mean) # dimension 1 (across rows) ## r1 r2 ## 4.000000 4.666667 apply(mat1,2,mean) # dimension 2 (across columns) ## v1 v2 v3 ## 0.0 7.5 5.5 # one can write a custom function inside apply(). (called annonymous function) # Its argument corresponds to the row or column vector passed by apply(). apply(mat1,2, function(x) sum(x)/length(x) ) # x is the internal vector name ## v1 v2 v3 ## 0.0 7.5 5.5 ans1 &lt;- apply(mat1,2, function(x) { avg = mean(x) sd = sd(x) # return the results as a list list(Avg = avg, Sd = sd) } ) unlist(ans1[[2]]) # results for the second column ## Avg Sd ## 7.5000000 0.7071068 unlist(ans1[[3]]) # results for the third column ## Avg Sd ## 5.5000000 0.7071068 Arrays are a generalization of matrices and can have more than 2 dimensions. array(c(1:18), c(2,3,3)) # dimension 2 by 2 by 3 ## , , 1 ## ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 2 4 6 ## ## , , 2 ## ## [,1] [,2] [,3] ## [1,] 7 9 11 ## [2,] 8 10 12 ## ## , , 3 ## ## [,1] [,2] [,3] ## [1,] 13 15 17 ## [2,] 14 16 18 array(c(1:9), c(2,3,3)) # R recycles the vector ## , , 1 ## ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 2 4 6 ## ## , , 2 ## ## [,1] [,2] [,3] ## [1,] 7 9 2 ## [2,] 8 1 3 ## ## , , 3 ## ## [,1] [,2] [,3] ## [1,] 4 6 8 ## [2,] 5 7 9 2.2.4 Data Frame A data frame is similar to a matrix, but it accepts multiple types (modes) of variables across columns (e.g., a dataset in typical data analysis programs like SAS, SPSS, Stata etc.). In some cases matrices and data frames may be treated interchangeably, but generally they need to be distinguished. Data manipulation functions are often written for data frames, while some base R functions are written for matrices. mymat1 &lt;- matrix(data = c(1:6), nrow = 2, ncol = 3, dimnames = list(c(&quot;r1&quot;,&quot;r2&quot;), c(&quot;c1&quot;,&quot;c2&quot;,&quot;c3&quot;))) mymat1 ## c1 c2 c3 ## r1 1 3 5 ## r2 2 4 6 class(mymat1) ## [1] &quot;matrix&quot; colnames(mymat1) ## [1] &quot;c1&quot; &quot;c2&quot; &quot;c3&quot; names(mymat1) ## NULL mydf1 &lt;- data.frame( mymat1, num = c(1,2), fac1 = c(&quot;a&quot;,&quot;abc&quot;), logi = c(TRUE, FALSE), fac2 = c(1,&quot;a&quot;) ) mydf1 ## c1 c2 c3 num fac1 logi fac2 ## r1 1 3 5 1 a TRUE 1 ## r2 2 4 6 2 abc FALSE a class(mydf1) ## [1] &quot;data.frame&quot; colnames(mydf1) ## [1] &quot;c1&quot; &quot;c2&quot; &quot;c3&quot; &quot;num&quot; &quot;fac1&quot; &quot;logi&quot; &quot;fac2&quot; names(mydf1) # colnames and names are the same ## [1] &quot;c1&quot; &quot;c2&quot; &quot;c3&quot; &quot;num&quot; &quot;fac1&quot; &quot;logi&quot; &quot;fac2&quot; Extracting elements from a data frame is similar to extracting from a matrix, but there are a few additional methods. mydf1[1,] # row = 1 and all columns ## c1 c2 c3 num fac1 logi fac2 ## r1 1 3 5 1 a TRUE 1 mydf1[,1] # all rows and col = 1 ## [1] 1 2 # data frame preserves dimension while extracting a row but not a column dim(mydf1[1,]) ## [1] 1 7 dim(mydf1[,1]) ## NULL dim(mydf1[,1,drop=FALSE]) # use drop = FALSE to keep a column vector ## [1] 2 1 mydf1[,1,drop=FALSE] ## c1 ## r1 1 ## r2 2 mydf1[, c(&#39;c1&#39;,&#39;num&#39;,&#39;logi&#39;)] ## c1 num logi ## r1 1 1 TRUE ## r2 2 2 FALSE class(mydf1[, c(&#39;c1&#39;,&#39;num&#39;,&#39;logi&#39;)]) ## [1] &quot;data.frame&quot; # extraction by column name with &quot;$&quot; symbol: df$varname mydf1$c1 ## [1] 1 2 dim(mydf1$c1) ## NULL # one can use quote &#39; &#39; or &quot; &quot; as well mydf1$&#39;c1&#39; ## [1] 1 2 # similarly, extraction by column name with [[ ]]: df[[&#39;varname&#39;]] mydf1[[&#39;c1&#39;]] ## [1] 1 2 dim(mydf1[[&#39;c1&#39;]]) ## NULL # or by index mydf1[[1]] ## [1] 1 2 # [[ ]] method is useful when passing a variable name as a string set_to_na &lt;- function(df, var) { df[[var]] &lt;- NA df } mydf1 ## c1 c2 c3 num fac1 logi fac2 ## r1 1 3 5 1 a TRUE 1 ## r2 2 4 6 2 abc FALSE a mydf2 &lt;- set_to_na(mydf1, &quot;c2&quot;) mydf2 ## c1 c2 c3 num fac1 logi fac2 ## r1 1 NA 5 1 a TRUE 1 ## r2 2 NA 6 2 abc FALSE a # add a variable mydf1$newvar &lt;- c(4, 4) mydf1 ## c1 c2 c3 num fac1 logi fac2 newvar ## r1 1 3 5 1 a TRUE 1 4 ## r2 2 4 6 2 abc FALSE a 4 mydf1$newvar2 &lt;- mydf1$c2 + mydf1$c3 mydf1 ## c1 c2 c3 num fac1 logi fac2 newvar newvar2 ## r1 1 3 5 1 a TRUE 1 4 8 ## r2 2 4 6 2 abc FALSE a 4 10 apply() may not work well with data frames since data frames are not exactly matrices. We can use simplified apply sapply() or list apply lapply() instead. mydf1 ## c1 c2 c3 num fac1 logi fac2 newvar newvar2 ## r1 1 3 5 1 a TRUE 1 4 8 ## r2 2 4 6 2 abc FALSE a 4 10 # sapply() idx_num &lt;- sapply(mydf1, is.numeric) idx_num ## c1 c2 c3 num fac1 logi fac2 newvar newvar2 ## TRUE TRUE TRUE TRUE FALSE FALSE FALSE TRUE TRUE apply(mydf1[,idx_num], 2, mean) ## c1 c2 c3 num newvar newvar2 ## 1.5 3.5 5.5 1.5 4.0 9.0 sapply(mydf1[,idx_num], mean) ## c1 c2 c3 num newvar newvar2 ## 1.5 3.5 5.5 1.5 4.0 9.0 # lapply() idx_num2 &lt;- unlist(lapply(mydf1, is.numeric)) idx_num2 ## c1 c2 c3 num fac1 logi fac2 newvar newvar2 ## TRUE TRUE TRUE TRUE FALSE FALSE FALSE TRUE TRUE unlist(lapply(mydf1[,idx_num2], mean)) ## c1 c2 c3 num newvar newvar2 ## 1.5 3.5 5.5 1.5 4.0 9.0 2.2.5 List A list is an ordered collection of (possibly unrelated) objects. The objects in a list are referenced by [[1]], [[2]], …, or [[‘var1’]], [[‘var2’]], … etc. mylist1 &lt;- list(v1 = c(1,2,3), v2 = c(&quot;a&quot;,&quot;b&quot;), v3 = factor(c(&quot;blue&quot;,&quot;red&quot;,&quot;orange&quot;,&quot;yellow&quot;)), v4 = data.frame( u1 = c(1:3), u2 = c(&quot;p&quot;,&quot;q&quot;,&quot;r&quot;)) ) mylist1 ## $v1 ## [1] 1 2 3 ## ## $v2 ## [1] &quot;a&quot; &quot;b&quot; ## ## $v3 ## [1] blue red orange yellow ## Levels: blue orange red yellow ## ## $v4 ## u1 u2 ## 1 1 p ## 2 2 q ## 3 3 r # extraction mylist1[[1]] ## [1] 1 2 3 mylist1[[&quot;v2&quot;]] ## [1] &quot;a&quot; &quot;b&quot; mylist1$v3 ## [1] blue red orange yellow ## Levels: blue orange red yellow mylist1$v4$u2 ## [1] p q r ## Levels: p q r # assignment mylist1$v5 &lt;- c(&quot;a&quot;,NA) mylist1$v5 ## [1] &quot;a&quot; NA # a list can be nested mylist1$v6 &lt;- list(y1 = c(2,9), y2 = c(0,0,0,1)) mylist1$v6 ## $y1 ## [1] 2 9 ## ## $y2 ## [1] 0 0 0 1 lapply() is very versatile since the items in a list can be completely unrelated. unlist(lapply(mylist1, class)) ## v1 v2 v3 v4 v5 ## &quot;numeric&quot; &quot;character&quot; &quot;factor&quot; &quot;data.frame&quot; &quot;character&quot; ## v6 ## &quot;list&quot; unlist(lapply(mylist1, attributes)) # some variables have attributes ## v3.levels1 v3.levels2 v3.levels3 v3.levels4 v3.class ## &quot;blue&quot; &quot;orange&quot; &quot;red&quot; &quot;yellow&quot; &quot;factor&quot; ## v4.names1 v4.names2 v4.row.names1 v4.row.names2 v4.row.names3 ## &quot;u1&quot; &quot;u2&quot; &quot;1&quot; &quot;2&quot; &quot;3&quot; ## v4.class v6.names1 v6.names2 ## &quot;data.frame&quot; &quot;y1&quot; &quot;y2&quot; lapply(mylist1, function(x) { if (is.numeric(x)) return(summary(x)) if (is.character(x)) return(x) if (is.factor(x)) return(table(x)) if (is.data.frame(x)) return(head(x)) if (is.list(x)) return(unlist(lapply(x,class))) } ) ## $v1 ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.0 1.5 2.0 2.0 2.5 3.0 ## ## $v2 ## [1] &quot;a&quot; &quot;b&quot; ## ## $v3 ## x ## blue orange red yellow ## 1 1 1 1 ## ## $v4 ## u1 u2 ## 1 1 p ## 2 2 q ## 3 3 r ## ## $v5 ## [1] &quot;a&quot; NA ## ## $v6 ## y1 y2 ## &quot;numeric&quot; &quot;numeric&quot; "],
["2-3-programming.html", "2.3 Programming", " 2.3 Programming 2.3.1 Operator Table 2.1: Basic R Operators Operation Description x + y Addition x - y Subtraction x * y Multiplication x / y Division x ^ y Exponentiation x %% y Modular arithmatic x %/% y Integer division x == y Test for equality x &lt;= y Test for less than or equal to x &gt;= y Test for greater than or equal to x &amp;&amp; y Boolean AND for scalars x || y Boolean OR for scalers x &amp; y Boolean AND for vectors x | y Boolean OR for vectors !x Boolean negation source: (Matloff 2011) 2.3.2 If else if (1 &gt; 0) { print(&quot;result: if&quot;) } else { print(&quot;result: else&quot;) } ## [1] &quot;result: if&quot; # {} brackets can be used to combine multiple expressions # They can be skipped for a single-expression if-else statement. if (1 &gt; 2) print(&quot;result: if&quot;) else print(&quot;result: else&quot;) ## [1] &quot;result: else&quot; ifelse(c(1,2,3) &gt; 2, 1, -1) # return 1 if TRUE and -1 if else ## [1] -1 -1 1 Sys.time() ## [1] &quot;2017-06-14 11:36:27 CDT&quot; time &lt;- Sys.time() hour &lt;- as.integer(substr(time, 12,13)) # sequential if-else statements if (hour &gt; 8 &amp; hour &lt; 12) { print(&quot;morning&quot;) } else if (hour &lt; 18) { print(&quot;afternoon&quot;) } else { print(&quot;private time&quot;) } ## [1] &quot;morning&quot; 2.3.3 Loop for (i in 1:3) { print(i) } ## [1] 1 ## [1] 2 ## [1] 3 for (i in c(1,3,5)) print(i) ## [1] 1 ## [1] 3 ## [1] 5 i &lt;- 1 while (i &lt; 5) { print(i) i &lt;- i + 1 } ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 2.3.4 Function We can avoid repeating ourselves with writing similar lines of codes if we turn them into a function. Functions contain a series of tasks that can be applied to varying objects such as different vectors, matrices, characters, data frames, lists, and functions. A function consists of input arguments, tasks (R expressions), and output as an object (e.g. a vector, matrix, character, data frame, list, or function etc.). It can be named or remain anonymous (typically used inside a function like lapply()). ## name_to_be_assigned &lt;- function(input args) { ## tasks ## } The output of the function, aside from those that are printed, saved, or exported, is the very last task (expression). If variable result is created inside the function, having result at the very end will return this item as an output. When multiple objects are created, it is often convenient to return those as a list. Use return() to return a specific item in the middle of the function and skip the rest of the evaluations. For checking errors and halting evaluations, use stop() or stopifnot(). myfun1 &lt;- function() print(&quot;Hello world&quot;) # just returning &quot;Hello world&quot; myfun1() ## [1] &quot;Hello world&quot; myfun2 &lt;- function(var) var^2 myfun2(var = 3) ## [1] 9 myfun3 &lt;- function(var = 1) ifelse(var&gt;0, log(var), var) myfun3() # default argument is var = 1 ## [1] 0 myfun3(2) ## [1] 0.6931472 myfun4 &lt;- function(x1, x2) { if (!is.numeric(x1) | !is.numeric(x2)) stop(&#39;demo of error: numeric args needed&#39;) x1*x2 } # try(myfun4(1, &quot;a&quot;)) myfun4(4, 3) ## [1] 12 2.3.5 Environment A function, formally known as a closure, consists of its arguments (called formals), a body, and an environment. An environment is a collection of existing R objects at the time when the function is created. Functions created at the top level have .GlobalEnv as their environments (R may refer to it as R_GlobalEnv as well). environment() # .GlobalEnv (or R_GlobalEnv) is the top-level environment ## &lt;environment: R_GlobalEnv&gt; f1 &lt;- function(arg1) environment() formals(f1) # arguments of f1() ## $arg1 body(f1) # body of f1() ## environment() environment(f1) # environment of f1(), which is .GlobalEnv ## &lt;environment: R_GlobalEnv&gt; f1() # inside f1 has its own enviornment ## &lt;environment: 0x7fac116739c8&gt; A function can access to the objects in its environment (i.e., global to the function) and those defined inside (i.e., local to the function) and generally cannot overwrite the global objects. It allows for using common names such as “x1”, “var1” etc. defined inside functions, but those objects are only accessible within the function. a &lt;- NULL # object named &quot;a&quot; in .GlobalEnv f2 &lt;- function() { a &lt;- 1 # object named &quot;a&quot; in an environment inside f2 print(a) environment() } f2() # one instance creating an environment ## [1] 1 ## &lt;environment: 0x7fac12f07ea0&gt; f2() # another instance creating another environment ## [1] 1 ## &lt;environment: 0x7fac12f73ea0&gt; a # stays NULL ## NULL ls() # ls() shows all objects of an environment (here .GlobablEnv) ## [1] &quot;a&quot; &quot;ans1&quot; &quot;df&quot; &quot;df1&quot; ## [5] &quot;df2&quot; &quot;f1&quot; &quot;f2&quot; &quot;factor1&quot; ## [9] &quot;factor2&quot; &quot;factor3&quot; &quot;hour&quot; &quot;i&quot; ## [13] &quot;idx_num&quot; &quot;idx_num2&quot; &quot;mat1&quot; &quot;matrix1&quot; ## [17] &quot;matrix2&quot; &quot;matrix3&quot; &quot;mydf1&quot; &quot;mydf2&quot; ## [21] &quot;myfun1&quot; &quot;myfun2&quot; &quot;myfun3&quot; &quot;myfun4&quot; ## [25] &quot;mylist1&quot; &quot;mymat1&quot; &quot;set_to_na&quot; &quot;tbl_operator&quot; ## [29] &quot;time&quot; &quot;vector1&quot; &quot;vector2&quot; &quot;vector3&quot; ## [33] &quot;vector4&quot; rm(list = ls()) # rm() removes items of an environment (here .GlobablEnv) ls() # all gone in GlobalEnv ## character(0) Using global assignment &lt;&lt;- operator, one can bend this general rule of not affecting global objects. This can be useful when it is desirable to make certain objects accessible across multiple functions without explicitly passing them through arguments. a &lt;- NULL b &lt;- NULL f1 &lt;- function() { a &lt;&lt;- 1 # global assignment # another way to assign to GlobalEnv assign(&quot;b&quot;, 2, envir = .GlobalEnv) } f1() a ## [1] 1 b ## [1] 2 a &lt;- 2 f2 &lt;- function() { # Since there is no &quot;a&quot; local to f2, R looks for &quot;a&quot; # in a parent environment, or .GlobalEnv print(a) # g() assigns a number to &quot;a&quot; in g()&#39;s environment g &lt;- function() a &lt;&lt;- 5 a &lt;- 0 # object called &quot;a&quot; local to f2 print(a) # g() updates only the local &quot;a&quot; to f2(), but not &quot;a&quot; in GlobalEnv # R&#39;s scope hierarchy starts from local to its environment g() print(a) } a &lt;- 3 # the first &quot;a&quot; is in .GlobalEnv when f2() is called # the second &quot;a&quot; is local to an instace of f2() # the third &quot;a&quot; is the updated version of the local &quot;a&quot; by g() f2() ## [1] 3 ## [1] 0 ## [1] 5 a # object &quot;a&quot; in GlobalEnv: unchanged by g() ## [1] 3 It is convenient to use &lt;&lt;- if you are sure about which object to overwrite. Otherwise, the use of &lt;&lt;- should be avoided. 2.3.6 Debugging browser() and traceback() are common debugging tools. A debugging session starts where browser() is inserted and allows for a line-by-line execution onward. Putting browser() inside a loop or function is useful because it allows for accessing the objects at a particular moment of execution in its environment. After an error alert, executing traceback() shows at which process the error occurred. Other tools include debug(), debugger(), and stopifnot(). 2.3.7 Stat func. What is going on here? What do we see? Table 2.2: Common R Statistical Distribution Functions Distribution Density_pmf cdf Quantiles Random_draw Normal dnorm( ) pnorm( ) qnorm( ) rnorm( ) Chi square dchisq( ) pchisq( ) qchisq( ) rchisq( ) Binomial dbinom( ) pbinom( ) qbinom( ) rbinom() source: (Matloff 2011) 2.3.8 String func. R has built-in string manipulation functions. They are commonly used for; detecting a certain pattern in a vector (grep() returning a location index vector, grepl() returning a logical vector) replacing a certain pattern with another (gsub()) counting the length of a string (nchar()) concatenating characters and numbers as a string (paste(), paste0(), sprintf()) extracting a segment of a string by character position range (substr()) splitting a string with a particular pattern (strsplit()) finding a character position of a pattern in a string (regexpr()) oasis &lt;- c(&quot;Liam Gallagher&quot;, &quot;Noel Gallagher&quot;, &quot;Paul Arthurs&quot;, &quot;Paul McGuigan&quot;, &quot;Tony McCarroll&quot;) grep(pattern = &quot;Paul&quot;, oasis) ## [1] 3 4 grepl(pattern = &quot;Gall&quot;, oasis) ## [1] TRUE TRUE FALSE FALSE FALSE gsub(&quot;Gallagher&quot;, &quot;Gallag.&quot;, oasis) ## [1] &quot;Liam Gallag.&quot; &quot;Noel Gallag.&quot; &quot;Paul Arthurs&quot; &quot;Paul McGuigan&quot; ## [5] &quot;Tony McCarroll&quot; nchar(oasis) ## [1] 14 14 12 13 14 paste(oasis) ## [1] &quot;Liam Gallagher&quot; &quot;Noel Gallagher&quot; &quot;Paul Arthurs&quot; &quot;Paul McGuigan&quot; ## [5] &quot;Tony McCarroll&quot; paste(oasis, collapse=&quot;, &quot;) ## [1] &quot;Liam Gallagher, Noel Gallagher, Paul Arthurs, Paul McGuigan, Tony McCarroll&quot; sprintf(&quot;%s from %d to %d&quot;, &quot;Oasis&quot;, 1991, 2009) ## [1] &quot;Oasis from 1991 to 2009&quot; substr(oasis, 1, 6) ## [1] &quot;Liam G&quot; &quot;Noel G&quot; &quot;Paul A&quot; &quot;Paul M&quot; &quot;Tony M&quot; strsplit(oasis, split=&quot; &quot;) # split by a blank space ## [[1]] ## [1] &quot;Liam&quot; &quot;Gallagher&quot; ## ## [[2]] ## [1] &quot;Noel&quot; &quot;Gallagher&quot; ## ## [[3]] ## [1] &quot;Paul&quot; &quot;Arthurs&quot; ## ## [[4]] ## [1] &quot;Paul&quot; &quot;McGuigan&quot; ## ## [[5]] ## [1] &quot;Tony&quot; &quot;McCarroll&quot; regexpr(&quot;ll&quot;, oasis[1])[1] ## [1] 8 Common regular expressions used in R include; &quot;[char]&quot; (any string containing either “c”, “h”, “a”, or “r” ) &quot;a.c&quot; (any string containing “a” followed by any letter followed by “c”) &quot;\\\\.&quot; (any string containing symbol “.”). grepl(&quot;[is]&quot;, oasis) ## [1] TRUE FALSE TRUE TRUE FALSE grepl(&quot;P..l&quot;, oasis) ## [1] FALSE FALSE TRUE TRUE FALSE grepl(&quot;\\\\.&quot;, c(&quot;Liam&quot;, &quot;Noel&quot;, &quot;Paul A.&quot;, &quot;Paul M.&quot;, &quot;Tony&quot;)) ## [1] FALSE FALSE TRUE TRUE FALSE 2.3.9 Set func. The functions for common set operations include union(), intersect(), setdiff(), and setequal(). The most commonly used function is %in% operator; X %in% Y returns a logical vector indicating whether an each element of X is a member of Y. c(1,2,3,4,5) %in% c(3,2,5) c(&quot;a&quot;,&quot;b&quot;,&quot;t&quot;,&quot;s&quot;) %in% c(&quot;t&quot;,&quot;a&quot;,&quot;a&quot;) References "],
["2-4-housekeeping.html", "2.4 Housekeeping", " 2.4 Housekeeping 2.4.1 Working directory getwd() returns the current working directly. setwd(new_directory) sets a specified working directory. 2.4.2 R session sessionInfo() shows the current session information. In RStudio, .rs.restartR() restarts a session. 2.4.3 Save &amp; load R objects can be saved and loaded by save(object1, object2, ..., file=&quot;file_name.RData&quot;) and load(file=&quot;file_name.RData&quot;). A ggplot object can be save by ggsave(&quot;file_name.png&quot;). 2.4.4 Input &amp; Output A common method to read and write data files is read.csv(&quot;file_name.csv&quot;) and write.csv(data_frame, file = &quot;file_name.csv&quot;). scan() is a more general function to read data files and interact with user keyboard inputs. file() is also a general function for reading data through connections, which refer to R’s mechanism for various I/O operations. dir() returns the file names in your working directory. A useful function is cat(), which can print a cleaner output to the screen, compared to print(). print(&quot;example&quot;) ## [1] &quot;example&quot; cat(&quot;example\\n&quot;) # end with \\n ## example cat(&quot;some string&quot;, c(1:4), &quot;more string\\n&quot;) ## some string 1 2 3 4 more string cat(&quot;some string&quot;, c(1:4), &quot;more string\\n&quot;, sep=&quot;_&quot;) ## some string_1_2_3_4_more string 2.4.5 Updating R needs regular updates for R distribution, individual R packages, and RStudio. Generally, updating once or twice a year would suffice. For updating RStudio, go to Help and then Check for Updates. Also, RStudio also makes it easy to update packages; go to Tools and the Check for Package Updates. Do these updates when you have time or you know that you need to update a particular package; updating R and R packages can be trickier than it seems. # check R version getRversion() ## [1] &#39;3.3.3&#39; version ## _ ## platform x86_64-apple-darwin13.4.0 ## arch x86_64 ## os darwin13.4.0 ## system x86_64, darwin13.4.0 ## status ## major 3 ## minor 3.3 ## year 2017 ## month 03 ## day 06 ## svn rev 72310 ## language R ## version.string R version 3.3.3 (2017-03-06) ## nickname Another Canoe # check installed packages ## installed.packages() # list all packages where an update is available ## old.packages() # update all available packages of installed packages ## update.packages() # update, without prompt ## update.packages(ask = FALSE) For windows users, one can automate the process using installr package. ## --- execute the following --- ## install.packages(&quot;installr&quot;) # install ## setInternet2(TRUE) # only for R versions older than 3.3.0 ## installr::updateR() # updating R. Sometimes, you can accidentally corrupt sample datasets that come with packages. To restore the original datasets, you have to remove the package by remove.packages() and then install it again. Use class(), attributes(), and str() to check for any unrecognized attributes attached to the dataset. Also, if you suspect that you have accidentally corrupted R itself, you should re-install the R distribution. -->"],
["3-piecemeal-top.html", "3 Piecemeal Topics", " 3 Piecemeal Topics Workshop materials will be added here. "],
["3-1-dplyr.html", "3.1 Unusual Deaths in Mexico", " 3.1 Unusual Deaths in Mexico Materials This is a practice session of dplyr and ggplot2 using a case study related to tidyr package. The case is about investigating the causes of death in Mexico that have unusual temporal patterns within a day. The data on mortalities in 2008 have the following pattern by hour; Figure 3.1: Temporal pattern of all causes of death Do you find anything unusual or unexpected? The figure shows several peaks within a day, indicating some increased risk of death during certain times of the day. What could generate these patterns? Wickham, the author, finds; The causes of [unusual] death fall into three main groups: murder, drowning, and transportation related. Murder is more common at night, drowning in the afternoon, and transportation related deaths during commute times (Wickham 2014). Figure 3.2: Causes of death with unusual temporal courses. Hour of day (hod) on the x-axis and proportion (prop) on the y-axis. Overall hourly death rate shown in grey. Causes of death with more than 350 deaths over a year. We will use two datasets: deaths containing the timing and coded causes of deaths and codes containing the look-up table for the coded causes. The dataset deaths has over 53,000 records (rows), so we use head() to look at the first several rows. # &quot;deaths08b&quot; is a renamed dataset with easier-to-read column names head(deaths08b) ## Year of Death (yod) Month of Death (mod) Day of Death (dod) ## 1 2008 1 1 ## 2 2008 1 1 ## 3 2008 1 1 ## 4 2008 1 1 ## 5 2008 1 1 ## 6 2008 1 1 ## Hour of Death (hod) Cause of Death (cod) ## 1 1 B20 ## 2 1 B22 ## 3 1 C18 ## 4 1 C34 ## 5 1 C50 ## 6 1 C50 The dataset codes has 1851 records. This table is generated by DT and webshot packages. In the search box, you can type in key words like “bacteria”, “nutrition”, and “fever”, as well as “assault” and “exposure” to see what items are in the data. We will reproduce this case study and practice using functions of dplyr and ggplot2. Arts &amp; Crafts Let’s recap the key ingredients of dplyr and ggplot2 from the introduction in Section 1. The six important functions in dplyr are: filter(): extracts rows (e.g., observations) of a data frame. We put logical vectors in its arguments. select(): extracts columns (e.g., variables) of a data frame. We put column names in its arguments. arrange(): orders rows of a data frame. We put column names in its arguments. summarise(): collapses a data frame into summary statistics. We put summary functions (e.g., statistics functions) using column names in its arguments. mutate(): creates new variables and adds them to the existing columns. We put window functions (e.g., transforming operations) using column names in its arguments. group_by(): assigns rows into groups within a data frame. We put column names in its arguments. We use piping operator %&gt;% (read as then) to translate a sentence of sequential instructions. For example, take dataset deaths08, %&gt;% (then) group the data by month of death, and %&gt;% (then) summarize the grouped data for the number of observations. deaths08 %&gt;% group_by(mod) %&gt;% # mod: month of death summarise(nobs = n()) # n(): a dplyr funciton to count rows ## # A tibble: 12 × 2 ## mod nobs ## &lt;int&gt; &lt;int&gt; ## 1 1 49002 ## 2 2 41685 ## 3 3 44433 ## 4 4 39845 ## 5 5 41710 ## 6 6 38592 ## 7 7 40198 ## 8 8 40297 ## 9 9 39481 ## 10 10 41671 ## 11 11 43341 ## 12 12 42265 The graphics with ggplot2 consist of three components: data: a data frame e.g., the first argument in ggplot(data, ...). geom: geometric objects such as points, lines, bars, etc. with parameters in parenthesis; e.g., geom_point(), geom_line(), geom_histogram() aes: specifications for x-y variables, as well as variables to differentiate geom objects by color, shape, or size. e.g., aes(x = var_x, y = var_y, shape = var_z) We specify data and aes in ggplot() and then add geom objects followed by + symbol (read as add a layer of); e.g., ggplot(data = dataset, mapping = aes(x = ...)) + geom_point(). The order of layers added by + symbol is generally interchangeable. Combined with %&gt;% operator, we can think of the code as a sentence. For example, take dataset deaths08, %&gt;% (then) plot with gglpot() with aes() featuring hour of day on the x-axis, + (and add a player of) geom object geom_histogram(). # a histogram version of the line-graph for the total number of deaths above deaths08 %&gt;% ggplot(aes(x = hod)) + geom_histogram(binwidth = 1, color = &quot;white&quot;) # a summary by month of day and hour of day. # e.g, Jan-1am, ..,Jan-12pm, Feb-1am,..., Feb-12pm, ... n_month_hour &lt;- deaths08 %&gt;% group_by(mod, hod) %&gt;% summarise( nobs = n() ) n_month_hour %&gt;% ggplot(aes(x = hod, y = nobs, color = as.factor(mod))) + geom_point() # &quot;last_plot() + &quot; allows for adding more layers to the previous plot last_plot() + geom_line() Exercise Now it is your turn. The exercise is to reproduce the above results for the unusual causes of deaths. Download materials: case study paper and case study data Set working directly: setwd(your_directory) Load libraries: library(dplyr), library(ggplot2), library(MASS) (used for fitting data by robust regression) Note 1: There is a minor error in the case study where the author accidentally kept several records of data from years other than 2008. This has virtually no effect on the results, and we are seeking to produce the same results as that case study. Note 2: You could look at the code in the paper for hints. However, the code is written with the functions of plyr package, or the predecessor of dplyr. Do not load both plyr and dplyr libraries in the same R session; they do not seem to have good compatibility. Restart R if you accidentally loaded both. Part A. Display overall hourly deaths We will reproduce: Hints: Filter NA in the hour of day (hod) variable Use group_by(), summarise(), n() to obtain death counts by group Use ggplot() + geom_line() to produce plot Use + labs( x = &quot;x lable&quot;, y = &quot;y label&quot;) for axis labels see help file of scale_y_continous() for comma (use ?function_name for help) Part B. Count deaths per hour, per disease We will reproduce: --> Panel (a) of the table contains the frequency (i.e. the number of rows) for each combination of hour of day (hod) and cause of death (cod), supplemented by the disease description in panel (b). Panel (c) shows the proportion (prop) of each hod-cod combination in the total deaths by the cod. Panel (d) contains the frequency and proportion (freq_all and prop_all) of the deaths by hour of day. That is, panel (a) is the raw counts (e.g., frequency) of observations by each pair of hour of day (hod) and cause of death (cod), and panel (b) makes it easy to see the cause of death (cod). Panel (c) converts this frequency of hod-cod pair into the relative frequency within the total frequency of cod, so that we see at which hour a disproportionately large number of deaths occurs for cod. Panel (d), on the other hand, presents the overall hourly death rates; if every hour has the same probability of death, we would see prop_all \\(\\approx\\) 0.042 (i.e., 1/24). Here, we see the author’s idea of identifying “unusual deaths” by looking at how “prop” of each hod-cod pairs deviates from “prop_all” (see figure 3.2). Hints for creating panel (a) Use more than one variable in group_by() Use summarise() with n() to obtain death counts by group Hints for creating panel (b) Use left_join() to add the information from dataset codes Hints for creating panel (c) Use mutate() with sum() on the joined dataset Hints for creating panel (d) Create a new data frame by using summarise() on the joined and mutated data frame. summarise() will reduce the dimension of the data frame to its summary, which is the basis of panel (d). Once the desired summary is created, merge it to the data frame of panels (a)-(c). Before using summarise() above, use group_by() to specify new grouping First create freq_all variable via summarise() with n() and then create prop_all variable via mutate() with sum() (call this data frame overall_freq, which will be used again at the very end) Use left_join() to join panels (a)-(c) and panel (d) (overall_freq), which we refer to as master_hod data frame. Hints for extracting the same rows as in the Table 16 above Create a subset of the master_hod data under a new name Use filter() to select cod being either “I21”, “N18”, “E84”, or “B16” and hod being greater or equal to 8 and smaller or less than 11 Use select() to pick columns in a desired order and arrange() to sort Part C. Find outliers We will reproduce: --> We will create a deviation variable named dist by taking the mean of squared differences between prop and prop_all. The above figures show the the number of observations n and this distance measure dist by cause of death in the raw scale (left) and in the log scale (right). This is the author’s linear model that describes how each cause of death tends to exhibit a similar pattern of hourly death rates with the overall pattern. Once the model is defined, we can define “outliers”, which do not follow the model’s prediction. Hints Use group_by() and summarise() on the master_hod data frame to generate n with function sum() and dist by mean((prop - prop_all)^2) Filter this summary for n &gt; 50 and call it devi_cod (deviations by cause of death) Use ggplot() + geom_point() with data = devi_cod to produce the raw-scale figure Additionally use scale_x_log10(), scale_y_log10(), and geom_smooth(method = &quot;rlm&quot;, se = FALSE) to produce the log-scale figure See help for scale_x_log10() to adjust axis labels (look for “comma”) Technically speaking, we should change the axis labels to indicate the logarithmic transformation, but we skip it here. Let’s not worry about reproducing the exact grids as they appear in the paper Part D. Fit data by a regression and plot residuals We will reproduce: --> The figure is a plot of the regression residuals resid of log(dist) on log(n). By visual inspection, the points lying above the horizontal line at resid=1.5 are considered to be “unusual causes of deaths” by the author. Here the author used the robust linear model (rlm()) regression, but the syntax is mostly the same as that of the standard linear model regression (lm() ). Here is an example of regression by lm(). df &lt;- data.frame( x1 &lt;- c(1:10), y1 &lt;- c(1,3,2,4,6,5,7,5,7,8) ) df %&gt;% ggplot(aes(x = x1, y = y1)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) The geom_smooth() is estimating the following linear regression: \\[ y1 = intercept + coefficient * x1 + residual\\] The model is estimated by lm() as follows; f1 &lt;- lm(formula = y1 ~ x1, data = df) Now let’s see what we get out of the estimation results f1. class(f1) # class &quot;lm&quot; ## [1] &quot;lm&quot; summary(f1) # summary() knows how to summarise an object of class &quot;lm&quot; ## ## Call: ## lm(formula = y1 ~ x1, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.52727 -0.57273 -0.02727 0.52273 1.54545 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.0000 0.6924 1.444 0.186656 ## x1 0.6909 0.1116 6.192 0.000262 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.014 on 8 degrees of freedom ## Multiple R-squared: 0.8273, Adjusted R-squared: 0.8058 ## F-statistic: 38.34 on 1 and 8 DF, p-value: 0.0002618 coefficients(f1) # coefficient point estimate ## (Intercept) x1 ## 1.0000000 0.6909091 vcov(f1) # coefficient variance-covariance matrix ## (Intercept) x1 ## (Intercept) 0.47939394 -0.06848485 ## x1 -0.06848485 0.01245179 predict(f1) # predicted (fitted) values with the estimated coefficients ## 1 2 3 4 5 6 7 8 ## 1.690909 2.381818 3.072727 3.763636 4.454545 5.145455 5.836364 6.527273 ## 9 10 ## 7.218182 7.909091 resid(f1) # residuals: ## 1 2 3 4 5 6 ## -0.69090909 0.61818182 -1.07272727 0.23636364 1.54545455 -0.14545455 ## 7 8 9 10 ## 1.16363636 -1.52727273 -0.21818182 0.09090909 Now let’s get back to our exercise and reproduce the figure above. Hints Run a regression by rlm with formula = log(dist) ~ log(n) and store the residuals in devi_cod data. To read more about linear regressions, see the help file of lm() (type ?lm). For adding a column of residuals, you can use assignment devi_cod$resid &lt;- your_residuals. Plot the residual against log-scale n Note: Check the dataset devi_cod for missing values of dist and n before running a regression (you should not have missing values in this case). Most regression functions, including lm() and rlm(), drop any row with missing values from the estimation. This becomes an issue if we want to add a new column containing predicted values or residuals to the original dataset. (When rows containing missing values are dropped, the vector generated by predict() or resid() will be shorter than the number of rows in the original dataset.) Use ggplot() + geom_point() structure for the plot Add + scale_x_log10() and + geom_hline(yintercept = 1.5) Part E. Visualize unusual causes of death We will reproduce: --> The first figure is the unusual causes of deaths in devi_cod with a relatively large number of deaths (n &gt; 350) and the second is that of a relatively small number of deaths (n &lt;= 350). Hints Using the cut-toff value resid &gt; 1.5, filter devi_cod and call it unusual data frame. Join master_hod and unusual data frames. Then create two subsets of data with conditions n &gt; 350 and n &lt;= 350. Use ggplot() + geom_line() structure with + facet_warp(~ disease, ncol = 3) To include the overall hourly proportions of deaths (prop_all) representing the average of all causes of deaths in a given hour, add another layer by geom_line(aes(...), data = overall_freq) with a local aes() argument and a data argument. With the data argument, variables in another data frame can be combined (assuming the axes have the same measurements), and here we use the overall_freq data frame from the panel (d) portion of Table 16 above. last_plot() %+% another_data_frame reproduces a plot of the same structure with a different data frame The Key Click here Reflections Let’s recap. The author (Wickham) investigates the temporal pattern of death in Mexico to find the causes of death that have unusual temporal patterns within a day. Here are the five steps used in his approach. A. Visualize the overall hourly frequency of death within a day B. Construct variables to compare the relative frequency of death per hour per cause with the overall hourly death rate C. Plot the data to identify a general relationship among key variables D. Create a linear model (i.e., data point = model prediction + residual) E. Visualize the temporal pattern of the “unusual” cases, or the causes of death that have relatively large residuals In a typical application, it will be unlikely that these steps can be followed in order. Most likely, we will have to go back and forth between these steps to define variables, visualize data, and refine a model till we reach the final results. Also, the purpose of creating a model may vary from making predictions to identifying certain parameters or outliers. However, the thought process and techniques in the above exercise will be applicable to many situations of data analysis. Just in case you need to see more examples, here are additional dplyr and ggplot2 tutorials. R for data science 100 Free Tutorials for Learning R RPubs - Data Processing with dplyr &amp; tidyr Discovering Python &amp; R Aggregating and analyzing data with dplyr genomics class dplyr tutorial dplyr Tutorial (With 50 Examples) References "],
["3-2-boot.html", "3.2 Action, Romance, and Chicks", " 3.2 Action, Romance, and Chicks Materials This session covers brief introductions to random sampling, bootstrapping, and linear regressions. Learning these concepts in the same context will help you see how they are related to each other. We will use a dataset on movies. library(ggplot2movies) movies2 &lt;- movies %&gt;% dplyr::select(title, year, budget, rating, Action, Romance) %&gt;% filter((Action ==1 | Romance ==1), !( Action == 1 &amp; Romance == 1), budget &gt; 0, year &gt;= 1970) %&gt;% mutate(budget = budget/10^6) summary(movies2) ## title year budget rating ## Length:1206 Min. :1970 Min. : 0.001 Min. :1.500 ## Class :character 1st Qu.:1992 1st Qu.: 3.500 1st Qu.:5.025 ## Mode :character Median :1998 Median : 15.000 Median :6.000 ## Mean :1996 Mean : 27.549 Mean :5.902 ## 3rd Qu.:2002 3rd Qu.: 40.000 3rd Qu.:6.800 ## Max. :2005 Max. :200.000 Max. :9.800 ## Action Romance ## Min. :0.0000 Min. :0.0000 ## 1st Qu.:0.0000 1st Qu.:0.0000 ## Median :1.0000 Median :0.0000 ## Mean :0.5887 Mean :0.4113 ## 3rd Qu.:1.0000 3rd Qu.:1.0000 ## Max. :1.0000 Max. :1.0000 The extracted data movies2 contain IMDB ratings of Action and Romance movies (excluding those of both Action and Romance genres) that are released between 1970 and 2005 and have known budgets. Action and Romance movies are about 59% and 41% of the data respectively. The average rating is 5.9. Let’s look at the distribution of the release years and ratings in this dataset. movies2$year %&gt;% table ## . ## 1970 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984 ## 12 10 11 8 11 7 9 10 7 4 11 16 10 8 13 ## 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 ## 27 17 18 15 22 22 19 20 30 32 51 55 64 77 64 ## 2000 2001 2002 2003 2004 2005 ## 80 94 110 103 103 36 We see that more data are available for years 1999-2004 than other years. movies2 %&gt;% ggplot(aes(x=rating)) + geom_histogram(binwidth = 0.5, color = &quot;white&quot;, fill = &quot;dodgerblue&quot;) The distribution of rating is somewhat skewed to the left. Let’s see how Action and Romance movies compare. movies2 &lt;- movies2 %&gt;% mutate(genre = ifelse(Action==1, &quot;Action&quot;, &quot;Romance&quot;)) movies2 %&gt;% ggplot(aes(x=rating)) + geom_histogram(binwidth = 0.5, color = &quot;white&quot;, fill = &quot;dodgerblue&quot;) + facet_grid(genre ~ .) movies2 %&gt;% group_by(genre) %&gt;% summarise(mean = mean(rating), sd = sd(rating), n = n()) ## # A tibble: 2 × 4 ## genre mean sd n ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Action 5.729859 1.419404 710 ## 2 Romance 6.147581 1.262879 496 Romance genre gets a slightly higher average rating than the Action. For the sake of discussion, suppose that movies2 is the population (or the universe) of our movie data, meaning that it contains all possible observations (movies) that fit our criteria (i.e. Action or Romance movies released in 1970-2005 with known budgets). Then, the population mean ratings for Action and Romance movies are 5.73 and 6.15 respectively. Now consider a sampling world. In almost all situations, the researcher does not have population data and has to work with a sample drawn from the population. Knowing that what we have is only a sample, we make statistical inferences for the property of the population. For example, using a sample of Action and Romance movies, we can compare their average ratings to see if one genre has a higher rating than the other at a certain statistical significance. Let’s create our sampling world. Here we randomly draw 30 observations from each genre and calculate summary statistics. set.seed(2017) # Fix a starting point of random number generations for reproducibility movie_sample &lt;- movies2 %&gt;% group_by(genre) %&gt;% sample_n(30) movie_sample %&gt;% ggplot(aes(x=rating)) + geom_histogram(binwidth = 0.5, color = &quot;white&quot;, fill = &quot;dodgerblue&quot;) + facet_grid(genre ~ .) movie_sample %&gt;% group_by(genre) %&gt;% summarize(mean = mean(rating), std_dev = sd(rating), n = n()) %&gt;% kable() genre mean std_dev n Action 5.730000 1.248489 30 Romance 6.333333 1.208114 30 Here is an another view; movie_sample %&gt;% ggplot(aes( x = genre, y = rating)) + geom_point() + geom_boxplot() To compare the mean ratings between genres, a common practice is to test the equality of means, say \\(\\mu_A\\) and \\(\\mu_R\\) for Action and Romance movies respectively. The null and alternative hypotheses are: \\(H_0: \\mu_A = \\mu_R\\) (equivalently, \\(\\mu_A - \\mu_R = 0\\)) \\(H_A: \\mu_A \\neq \\mu_R\\) Central Limit Theorem Here are some fundamental building blocks of statistics. Let \\(y_{i}\\) is an independently and identically distributed (i.i.d.) random variable for observation \\(i = 1, .., N\\) drawn from some distribution with population mean \\(\\mu = E[y_i]\\) and standard deviation \\(\\sigma = \\sqrt{E[(y_i - \\mu)^2]}\\) where \\(E[.]\\) is an expectation operator over the random variable. The sample mean and standard deviation of \\(y_{i}\\) are defined as \\[\\bar{y} = \\frac{\\sum_i y_i}{N}, \\quad s =\\sqrt{\\frac{\\sum_i (y_{i} - \\bar{y})^2}{(N-1)}}.\\] So, we take the average \\(\\bar{y}\\), which serves as an unbiased estimate of \\(\\mu\\). Yet, how close is \\(\\bar{y}\\) to \\(\\mu\\)? The statistical theory gives us a probabilistic answer for the inferences about population mean \\(\\mu\\). For example, if we know that \\(y_i\\) is normally distributed with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), then it follows that the sample mean \\(\\bar{y}\\) is normally distributed with mean \\(\\mu\\) and standard deviation \\(\\sigma/\\sqrt{N}\\). The distribution of an estimate like this is called sampling distribution, and in special cases it is exactly known. With known \\(\\mu\\) and \\(\\sigma\\), we know how fast the sample mean \\(\\bar{y}\\) probabilistically approaches to the population mean \\(\\mu\\) as the sample size \\(N\\) increases. This is done by calculating the z-statistic \\[z = \\frac{\\bar{y} - \\mu}{\\sigma/\\sqrt N}\\] and comparing it to the standard normal distribution table. In other words, we don’t just look at \\(\\bar{y}\\); we look at the relationship of \\(\\bar{y}\\), \\(\\mu\\), \\(\\sigma\\), and \\(N\\) as described in the z-statistic, for which the shape of the distribution is known. This allows us to calculate the probability that the sample mean in the next random sample could be greater or smaller than observed \\(\\bar{y}\\). To put it differently, we can infer how representative observed \\(\\bar{y}\\) would be if we were to repeat random samples of size \\(N\\) and calculate the mean many times. In most situations, we do not know \\(\\mu\\) or \\(\\sigma\\) of the population, and we would not be sure whether the underlying distribution is really normal. But, that’s okay. We can still make statistical inferences for the population mean \\(\\mu\\). Under some regularity conditions (e.g. the existence of a finite mean and variance of the random variable), the Central Limit Theorem tells us that regardless the underlying distribution, the sampling distribution of \\(\\bar{y}\\) is approximately normal. In a world with unknown \\(\\mu\\) or \\(\\sigma\\), we approximate the standard normal distribution with a Student’s t distribution. The t-statistic is calculated as \\[t = \\frac{\\bar{y} - \\mu}{s/\\sqrt N}\\] where \\(s\\) is the consistent estimate of \\(\\sigma\\), and we compare it to the t distribution table at \\(N-1\\) degrees of freedom (one degree of freedom is reduced for the estimate \\(s\\)). The Student’s t distribution is fatter-tailed than the standard normal distribution (due to the estimated standard error on the denominator), but it approaches to the standard normal distribution as the sample size \\(N\\) increases. For given significance level \\(\\alpha\\), the \\(1-\\alpha\\) confidence interval is \\[t_{N, 1-\\alpha/2} \\le \\frac{\\bar{y} - \\mu}{s/\\sqrt N} \\le t_{N, \\alpha/2}\\] where \\(t_{N, 1-\\alpha/2}\\) and \\(t_{N, 1-\\alpha/2}\\) are the lower and upper bounds of the t-statistic and are found in the t-distribution table. Since the t-distribution is symmetric, \\(- t_{N, 1-\\alpha/2} = t_{N, 1-\\alpha/2}\\). For example, at \\(\\alpha=0.05\\) and \\(N&gt;1000\\), we have \\(t_{N, 1-\\alpha/2} \\approx -1.96\\) and \\(t_{N, 1-\\alpha/2}=1.96\\). Thus, for a large \\(N\\), the confidence interval of \\(\\mu\\) is given by \\[ \\bar{y} - 1.96 \\:\\: s/\\sqrt{N} \\le \\mu \\le \\bar{y} + 1.96 \\:\\: s/\\sqrt{N}.\\] Let’s get back to the comparison of ratings between genres. How do we test our hypothesis \\(\\mu_A = \\mu_R\\)? Intuitively, we can make estimates of \\(\\mu_A\\) and \\(\\mu_R\\) by the corresponding sample means \\(\\bar{y}_A\\) and \\(\\bar{y}_R\\). Then, it’s a matter of making statistical inferences about \\(\\bar{y}_A - \\bar{y}_R\\) for how close they would be to \\(\\mu_A - \\mu_R\\). We calculate the t-statistic of \\(\\bar{y}_A - \\bar{y}_R\\) and infer the probability of rejecting \\(H_0: \\mu_A - \\mu_R = 0\\). Let \\(\\bar{y}_{A}\\) and \\(s_A\\) be the sample mean and standard deviation of ratings for Action movies and \\(\\bar{y}_{R}\\) and \\(s_R\\) be those for Romance movies. A typical approach called Welch’s t-test statistic uses \\[t = \\frac{\\bar{y}_A - \\bar{y}_R}{s_\\Delta}\\] where \\[s_\\Delta = \\sqrt{\\frac{s_A^2}{N_A} + \\frac{s^2_R}{N_R}}\\] is sort of a joint standard deviation of \\(y_{iA} - y_{iR}\\). Its degree of freedom has a somewhat complicated form but is approximately \\((N_A-1) + (N_R-1)\\) in many situations. For your information (not need to memorize), it is formally given as \\[d.f. = \\frac{s^2_\\Delta}{(s_A^2/N_A)^2/(N_A - 1) + (s_R^2/N_R)^2/(N_R - 1) }.\\] The Welch’s t-test statistic can be manually calculated as follows. # Welch&#39;s t-stat for the mean difference of two groups mean_ratings &lt;- movie_sample %&gt;% group_by(genre) %&gt;% summarize(mean = mean(rating), sd = sd(rating)) sample_diff &lt;- mean_ratings$mean[1] - mean_ratings$mean[2] sample_diff_sd &lt;- sqrt(mean_ratings$sd[1]^2/30 + mean_ratings$sd[2]^2/30) # N = 30 sample_t &lt;- sample_diff/sample_diff_sd c(sample_diff, sample_diff_sd, sample_t) ## [1] -0.6033333 0.3171889 -1.9021261 The observed mean difference is -0.603, for which the t-statistic is -1.902 at approximately 58 degrees of freedom. Let’s visualize this t-statistic against its theoretical distribution, which can be approximated by many random draws from the Student’s t distribution. many_t_df58 &lt;- data.frame(t = rt(10^6, 58)) # one million random draws from t-dist with 58 d.f. many_t_df58 %&gt;% ggplot(aes(x=t)) + geom_histogram(color=&#39;white&#39;, binwidth = 0.1) + geom_vline(xintercept = sample_t) # add vertical line at -1.902 The t distribution is centered at zero and has symmetric tails. We can think of the area of each bar representing the probability that a random draw of t-stat falls in that bin, and the total area of bars on the left of our t-statistic (-1.902) represents the probability that a random draw of t-stat is smaller than -1.902. By applying the two-tail t test, we can calculate the probability that a random draw of t-stat is more extreme than our t-statistic (i.e., being located toward either of the tails); nrow(subset(many_t_df58, abs(t)&gt;= abs(sample_t)))/nrow(many_t_df58) ## [1] 0.061964 Let’s visualize this probability; many_t_df58 %&gt;% ggplot(aes(x=t)) + geom_histogram(color=&#39;white&#39;, binwidth = 0.1) + geom_histogram(data = subset(many_t_df58, abs(t)&gt;= abs(sample_t)), color=&#39;white&#39;, fill=&quot;red&quot;, binwidth = 0.1) + geom_vline(xintercept = sample_t) where the highlighted area represents the probability of type I error, or the rejection of the null hypothesis \\(H_0\\) when it is in fact true, and represents the p-value (0.062 in this case). If we set our tolerance level for making a type I error at the probability of 10% or less (\\(\\alpha = 0.1\\)), we conclude that we reject the null hypothesis \\(H_0\\) (i.e., a finding of a statistically significant difference in ratings between Action and Romance genres) since the p-value is smaller than \\(\\alpha\\). If we set our tolerance level at \\(\\alpha = 0.05\\), we fail to reject \\(H_0\\) (no statistically significant effect). We can visualize how the probability of rejections (called rejection regions) associated with \\(\\alpha = 0.1\\) (orange below) and \\(\\alpha = 0.05\\) (green) compare to our t-statistic; many_t_df58 %&gt;% ggplot(aes(x=t)) + geom_histogram(color=&#39;white&#39;, binwidth = 0.1) + # rejection regions with critical val for a = 0.1 geom_histogram(data = subset(many_t_df58, abs(t)&gt;= abs(qt(.95,58))), color=&#39;white&#39;, fill=&quot;orange&quot;, binwidth = 0.1) + # rejection regions with critical val for a = 0.05 geom_histogram(data = subset(many_t_df58, abs(t)&gt;= abs(qt(.975,58))), color=&#39;white&#39;, fill=&quot;green&quot;, binwidth = 0.1) + geom_vline(xintercept = sample_t) Now that we know what the Welch’s t-test does, we can simply use R’s function to conduct a t-test; movie_sample %&gt;% with(t.test( rating ~ genre )) # using &quot;formula&quot; input ## ## Welch Two Sample t-test ## ## data: rating by genre ## t = -1.9021, df = 57.937, p-value = 0.06213 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -1.23827054 0.03160388 ## sample estimates: ## mean in group Action mean in group Romance ## 5.730000 6.333333 Recall that we started with knowing that the population mean rating is slightly higher for Romance than for Action. Here, it seems reasonable that a random sample of 30 observations from each genre leads us to find a statistically significant difference at the 10% level. Bootstrapping Now we will introduce a concept of bootstrapping. Recall that our statistic, say \\(\\bar{y}\\), conceptually has its distribution, depending on the version of random sample that we happen to draw. The idea here is to mimic the process of having many versions of random samples through simulations, so that we generate a simulated distribution of the statistic. Then, we can make statistical inferences without invoking the statistical theory of the approximate distribution via the Central Limit Theorem. There are many ways to conduct bootstrapping. For simplicity, we will use some of the most common practices. First, we make many random draws of \\(y_{iA}\\) and \\(y_{iR}\\) from our sample movie_sample with replacement (i.e., each time the drawn observation is put back into the pool). We can use do() from mosaic package and sample_n() from dplyr package. set.seed(2017) boot1 &lt;- mosaic::do(5000) * # repeat the following expression ({...}) for 5000 times ({ movie_sample %&gt;% group_by(genre) %&gt;% # sample 30 obs from each genre with replacement sample_n(30, replace=TRUE) %&gt;% summarize(mean = mean(rating), sd = sd(rating), n =n()) %&gt;% data.frame() }) head(boot1) ## genre mean sd n .row .index ## 1 Action 5.536667 1.0473228 30 1 1 ## 2 Romance 6.173333 1.3284508 30 2 1 ## 3 Action 5.683333 1.3164957 30 1 2 ## 4 Romance 6.760000 1.0682244 30 2 2 ## 5 Action 5.630000 0.8317617 30 1 3 ## 6 Romance 6.333333 1.1931799 30 2 3 The column .index shows the index of bootstrap replications, and the column .row nested in .index gives the indicator of calculated results by genre within each replication. Next, we calculate the bootstrap version of our estimates such as \\(\\bar{y}_A - \\bar{y}_R\\) and Welch’s t statistic. boot1_w &lt;- boot1 %&gt;% dplyr::select(-.row) %&gt;% # get rid of .row column reshape(idvar= &quot;.index&quot;, timevar=&quot;genre&quot;, # reshape into a &quot;wide-form&quot; dataset direction=&quot;wide&quot;) head(boot1_w) ## .index mean.Action sd.Action n.Action mean.Romance sd.Romance n.Romance ## 1 1 5.536667 1.0473228 30 6.173333 1.328451 30 ## 3 2 5.683333 1.3164957 30 6.760000 1.068224 30 ## 5 3 5.630000 0.8317617 30 6.333333 1.193180 30 ## 7 4 5.480000 1.0768408 30 6.170000 1.303087 30 ## 9 5 5.513333 1.1340295 30 6.293333 1.235937 30 ## 11 6 5.950000 1.2601998 30 6.440000 1.131554 30 boot1_w &lt;- boot1_w %&gt;% mutate( bt_diff = (mean.Action - mean.Romance), # difference bt_sd = sqrt(sd.Action^2/30 + sd.Romance^2/30), bt_t = bt_diff/bt_sd # Welch&#39;s t-stat ) Here is how sample estimate sample_diff compares with the histogram of its bootstrap counterpart bt_diff. boot1_w %&gt;% ggplot(aes(x = bt_diff, fill = bt_diff &gt; 0)) + geom_histogram(color = &quot;white&quot;, bins = 40) + geom_vline(xintercept = sample_diff) + theme(legend.position=&quot;top&quot;) Here the bt_diff values that are greater than zero are marked by a different color since they suggest the opposite conclusion that the mean rating is higher for Action than for Romance. Depending on the random draw of a bootstrap replication, one could have the opposite result in some of the times. The question is how often that happens. Using the bootstrap estimates, we can estimate the confidence interval in a few ways. For example, to estimate a 95% confidence interval, one can take the 2.5th and 97.5th percentiles of the distribution shown above in the histogram. # confidence interval quantile(boot1_w$bt_diff, c(0.025, 0.975)) # version 1 ## 2.5% 97.5% ## -1.20333333 0.01333333 Another approach is to calculate a bootstrap standard deviation and apply \\(t_{df,\\alpha/2} \\le (\\bar{y}_A - \\bar{y}_R)/s_{bt} \\le t_{df,1-\\alpha/2}\\) where \\(\\bar{y}_A - \\bar{y}_R\\) is the mean difference in ratings between Action and Romance movies, \\(s_{bt}\\) is an estimate of the standard deviation of \\(\\bar{y}_A - \\bar{y}_R\\), and \\(- t_{df,\\alpha/2}=t_{df,\\alpha/2}=2.00\\) for the t distribution with 58 degrees of freedom. Note that here we do not need \\(\\sqrt{N}\\) in the confidence interval calculation (unlike the above discussion where \\(s\\) was the standard deviation of rating \\(y_i\\) instead of the standard deviation of the average). sample_diff_sd_boot &lt;- sd(boot1_w$bt_diff) sample_diff_sd_boot ## [1] 0.307742 # version 2 c(sample_diff - 2.00 * sample_diff_sd_boot, sample_diff + 2.00 * sample_diff_sd_boot) ## [1] -1.21881724 0.01215058 Recall that in the case of two-tail test, we would look for extreme values on both tails of the distribution. We can visualize this by centering the above graph at sample_diff and identifying the values that are farther from the center than sample estimate sample_diff is. In the previous plot, we drew a histogram of bt_diff with fill code bt_diff &gt; 0. By subtracting sample_diff and taking the absolute values, we have bt_diff - sample_diff with fill code abs(bt_diff - sample_diff) &gt; abs(sample_diff). Essentially, we are approximating the distribution of \\(\\bar{y}_A-\\bar{y}_R - (\\mu_A -\\mu_R)\\) (sample_diff compared to the population difference) by the distribution of \\(\\bar{y}^*_A-\\bar{y}^*_R - (\\bar{y}_A-\\bar{y}_R )\\) ( bootstrap estimate \\(\\bar{y}^*_A-\\bar{y}^*_R =\\) bt_diff compared to sample_diff. boot1_w %&gt;% ggplot(aes(x = bt_diff - sample_diff, fill = abs(bt_diff - sample_diff) &gt; abs(sample_diff))) + geom_histogram(color = &quot;white&quot;, bins = 40) + theme(legend.position=&quot;top&quot;) The centered histogram makes it easier to see whether our sample estimate sample_diff is representative; how closely are the bootstrap estimates centered around sample_diff and how do they spread? In this sense, the areas of the extreme values correspond to the rejection regions for the two-tail t-test, and the middle part corresponds to the confidence interval. By taking the area of the rejection regions, we can estimate the p-value; # p-value boot1_w %&gt;% with(sum(abs(bt_diff - sample_diff) &gt; abs(sample_diff))/length(bt_diff)) ## [1] 0.0512 which represents the probability that a bootstrap estimate falls in either of the colored tails. Previously, we noted there are many ways to do bootstrapping. Another version here is to bootstrap Welch’s t-statistic instead of the mean difference. An estimate in the form of t-statistic has a pivotal quality, meaning that the distribution of the statistic does not depend on unknown parameters. Welch’s t-statistic follows the Student’s t distribution with a given degree of freedom, which does not depend on any unknown parameter such as the population mean or variance. It may be recommended to use a pivotal statistic for bootstrapping. Here are the parallel results for bootstrapping Welch’s t-statistic. boot1_w %&gt;% ggplot(aes(x = bt_t, fill = bt_t &gt; 0)) + geom_histogram(color = &quot;white&quot;, bins = 40) + geom_vline(xintercept = sample_t) + theme(legend.position=&quot;top&quot;) To convert a confidence interval of Welch’s t-statistic \\((\\bar{y}_A - \\bar{y}_R)/s_\\Delta\\) into a confidence interval of the difference in mean ratings \\(\\bar{y}_A - \\bar{y}_R\\), we multiply the former by \\(s_\\Delta=\\) sample_diff_sd; # confidence interval quantile(boot1_w$bt_t, c(0.025, 0.975)) * sample_diff_sd # version 1 ## 2.5% 97.5% ## -1.31094136 0.01345958 sample_t_sd_boot &lt;- boot1_w %&gt;% with(sd(bt_t - mean(bt_t))) # version 2 c(sample_t - 2.00 * sample_t_sd_boot, sample_t + 2.00 * sample_t_sd_boot) * sample_diff_sd ## [1] -1.27547455 0.06880788 # centered at observed Welch&#39;s t-statistic boot1_w %&gt;% ggplot(aes(x = bt_t - sample_t, fill = abs(bt_t - sample_t) &gt; abs(sample_t))) + geom_histogram(color = &quot;white&quot;, bins = 40) + theme(legend.position=&quot;top&quot;) # p-value boot1_w %&gt;% with(sum(abs(bt_t - sample_t) &gt; abs(sample_t))/length(bt_t)) ## [1] 0.0716 Linear Models Let’s extend our bootstrapping experiment to a regression framework. While we have not formally covered regressions in this site yet, we can develop an intuitive understanding on how they works. A simple form of linear regression is \\[ y_i = a_0 + b_1 x_{i} + \\varepsilon_i\\] where \\(y_i\\) is the dependent variable of observation \\(i\\), \\(x_i\\) is an independent variable, \\(a_0\\) and \\(b_1\\) are parameters for the intercept and the slope of \\(x\\) to be estimated, and \\(\\varepsilon_i\\) is the residual error term. Depending on the assumption on the error term \\(\\varepsilon_i\\), we can run different models on the same equation. Regardless, we are required to assume that the error \\(\\varepsilon_i\\) is uncorrelated with the independent variable \\(x_i\\). Here is an example. By default, lm() assumes a set of the standard assumptions for the Ordinary Least Squares (OLS). d0 &lt;- data.frame( y = c(1, 3, 5, 2, 6, 7, 8, 3), x = c(0, 2, 8, 1, 6, 8, 10, 4)) lm( y ~ x, data = d0) %&gt;% summary() ## ## Call: ## lm(formula = y ~ x, data = d0) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.3966 -0.3683 0.2207 0.5145 0.8972 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.2213 0.5109 2.390 0.053994 . ## x 0.6469 0.0856 7.557 0.000279 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8338 on 6 degrees of freedom ## Multiple R-squared: 0.9049, Adjusted R-squared: 0.8891 ## F-statistic: 57.11 on 1 and 6 DF, p-value: 0.0002787 Coefficient estimates are \\(a_0 = 1.2213\\) and \\(b_1 = 0.6469\\) with the standard errors of \\(sd(a_0) = 0.5109\\) and \\(sd(b1) = 0.0856\\), suggesting that the t-values of \\(2.390\\) and \\(7.557\\) for testing whether these coefficients are statistically different from zero, or \\(H_0: a_0=0\\) and \\(H_0: b_1 =0\\). The standard error for the residual is estimated with the assumption that \\(\\varepsilon_i\\) is i.i.d. normal with mean zero and some standard deviation \\(\\sigma\\). “Pr(&gt;|t|)” shows the p-values of the coefficient estimates, and the statistical significance is indicated with symbols “***”, “**” etc. Let’s not worry about other metrics here. Let’s visualize the above regression; d0 %&gt;% ggplot(aes( x = x, y = y)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se=FALSE) With only two variables, the plot like this gives us a clear picture of how the regression relates to the data points. The straight line is the fitted regression equation \\(\\widehat{a}_0 + \\widehat{b}_1 x_i\\) where hat \\(\\widehat{}\\) notation represents an estimate, and the vertical distance (and its direction) from this line is the predicted residual \\(\\widehat{\\varepsilon_i}=y_i - \\widehat{a}_0 + \\widehat{b}_1 x_i\\). The OLS estimates are the best linear unbiased estimators (BLUE). Now, let \\(y_{ij}\\) be the rating of movie \\(i\\) in genre \\(j\\) where \\(j\\) is either \\(A\\) (Action) or \\(R\\) (Romance). Also, let \\(1(A)\\) be the indicator function that takes a value of one if condition \\(A\\) is true and zero otherwise. Then, we can formulate a linear regression equation to test the mean difference in movie ratings between Action and Romance as follows. \\[ y_{ij} = b_A \\: 1(j=A) + b_R \\: 1(j=R) + \\varepsilon_{ij}\\] where the means of Action and Romance movies are estimated as \\(b_A\\) and \\(b_R\\) respectively. Under the standard OLS assumptions, \\(b_A\\) and \\(b_R\\) are equivalent to the sample means (calculated by the usual summing and dividing by the number of observations), and the estimates of the variances for \\(\\hat{b}_A\\) and \\(\\hat{b}_R\\) are obtained by matrix algebra (which we will cover in a future session). For rotational brevity, this equation may be written as \\[ y_{ij} = \\alpha_j + \\varepsilon_{ij}\\] where \\(\\alpha_j\\) is \\(\\alpha_A = b_A\\) for \\(j=A\\) and \\(\\alpha_R = b_R\\) for \\(j=R\\). This model yields the following; # &quot;0 +&quot; eliminates the intercept lm(rating ~ 0 + genre, data = movie_sample) %&gt;% summary() ## ## Call: ## lm(formula = rating ~ 0 + genre, data = movie_sample) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.3333 -0.6583 -0.1300 0.7942 3.2700 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## genreAction 5.7300 0.2243 25.55 &lt;2e-16 *** ## genreRomance 6.3333 0.2243 28.24 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.228 on 58 degrees of freedom ## Multiple R-squared: 0.9615, Adjusted R-squared: 0.9602 ## F-statistic: 725 on 2 and 58 DF, p-value: &lt; 2.2e-16 Note that the t-statistics test hypotheses \\(H_0: b_A = 0\\) and \\(H_0: b_R = 0\\), which differ from our interest, \\(H_0: b_A = b_R\\). We can rewrite the above equation as \\[ y_{ij} = a_0 + \\beta_R \\: 1(j=R) + \\varepsilon_{ij}\\] where \\(\\beta_R = b_R - b_A\\). Alternatively, \\[ y_{ij} = a_0 + \\alpha_j + \\varepsilon_{ij}\\] where \\(\\alpha_A\\) serves as a reference group and hence is excluded from the coefficient estimates. This yields; ols1 &lt;- lm( rating ~ genre, data = movie_sample) summary(ols1) ## ## Call: ## lm(formula = rating ~ genre, data = movie_sample) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.3333 -0.6583 -0.1300 0.7942 3.2700 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.7300 0.2243 25.548 &lt;2e-16 *** ## genreRomance 0.6033 0.3172 1.902 0.0621 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.228 on 58 degrees of freedom ## Multiple R-squared: 0.05872, Adjusted R-squared: 0.04249 ## F-statistic: 3.618 on 1 and 58 DF, p-value: 0.06212 where the estimate of \\(\\alpha_R\\) is equivalent with the calculated difference sample_diff, and its t-statistic is approximately the same as sample_t above. The sign is switched since our estimate here is \\(b_R - b_A\\) instead of \\(b_A - b_R\\). The OLS output provides the estimate of standard errors for the coefficients based on the statistical theory like the output from Welch’s t-statistic above. Now we derive our estimate of standard errors by applying bootstrapping to the OLS model. In doing so, we create several functions here. The first two functions extract “formula” and the dependent variable from a lm class object. getFormula &lt;- function(model) gsub(&quot;()&quot;,&quot;&quot;, model$call[2]) # gsub() substitues characters getFormula(ols1) ## [1] &quot;rating ~ genre&quot; getDependentVar &lt;- function(model) { str &lt;- getFormula(model) gsub(&quot; &quot;,&quot;&quot;, substr(str, 1, (regexpr(&quot;~&quot;,str)[1]-1))) # substr() takes a substring } getDependentVar(ols1) ## [1] &quot;rating&quot; The next function takes a lm class object (i.e., a result by lm()) and the number of bootstrap replications for its inputs and produces bootstrap versions of the coefficient estimates as an output. We assume that the distribution of \\(\\varepsilon_i\\) is a normal distribution with mean \\(0\\) and variance \\(\\hat{\\sigma}^2\\) where \\(\\hat{\\sigma} = \\sum\\widehat{\\varepsilon}^2_i/(N-k)\\) is the OLS estimate of \\(\\sigma\\) with \\(N-k\\) degrees of freedom (Num. obs minus num. of parameters). Then, we can generate a bootstrapped dependent variable by combining the predicted part of the linear model $ _0 + _j$ and an random draw of bootstrap error term \\(\\varepsilon^b_i\\), or \\(y^b_{ij} = \\widehat{a}_0 + \\widehat{\\alpha}_j + \\varepsilon^b_i\\). In each bootstrap replication \\(b=1, .., B\\), we replace \\(y_{ij}\\) with its bootstrap counterpart \\(y^b_{ij}\\) and run the OLS estimation, and we repeat this process for \\(B\\) times (we set \\(B=5000\\)). run_ols_boot &lt;- function(lm_rlt, num_do = 5000) { # calculate the standard deviation of the residuals N &lt;- length(lm_rlt$residuals) sd_res &lt;- (sum(lm_rlt$residuals^2)/lm_rlt$df.residual) %&gt;% sqrt() dep_var &lt;- getDependentVar(lm_rlt) do(num_do) * ({ data_bt &lt;- lm_rlt$model # replace the dependent variable with its bootstrap counterpart data_bt[[dep_var]] &lt;- lm_rlt$fitted.values + # the predicted component + rnorm(N, mean = 0, sd = sd_res) # random draws from the error distribution # run the OLS model with the same formula but with a new, bootstrap dataset ols_bt &lt;- lm(as.formula(getFormula(lm_rlt)), data = data_bt) coef(ols_bt) # get coefficients }) } set.seed(2007) # run bootstrap with our function bt_est_ols1 &lt;- run_ols_boot(ols1, 5000) Let’s compare the estimates of standard errors between the default OLS and our bootstrap results. sample_ols1 &lt;- tidy(ols1) # summary of the original OLS estimates bt_sd_ols1 &lt;- apply(bt_est_ols1, 2, sd) # calculate bootstrap standard errors bt_ols1 &lt;- cbind(coeff = sample_ols1$estimate, # copy the coeff from the OLS result sd = bt_sd_ols1, # use bootstrap standard errors tstat = sample_ols1$estimate/bt_sd_ols1) # OLS estimates with statistical inferences by statistic theory sample_ols1 ## term estimate std.error statistic p.value ## 1 (Intercept) 5.7300000 0.2242864 25.547688 2.999063e-33 ## 2 genreRomance 0.6033333 0.3171889 1.902126 6.212427e-02 # OLS estimates with statistical inferences by bootstrapping bt_ols1 ## coeff sd tstat ## Intercept 5.7300000 0.2262160 25.329774 ## genreRomance 0.6033333 0.3231541 1.867014 In this case they are pretty close. Let’s visualize this. sample_Romance &lt;- sample_ols1$estimate[2] bt_est_ols1 %&gt;% ggplot(aes(x = genreRomance - sample_Romance, fill = (abs(genreRomance - sample_Romance) &gt;= abs(sample_Romance)))) + geom_histogram(color = &quot;white&quot;, bins = 40) + theme(legend.position=&quot;top&quot;) Here are estimates of the confidence interval and the p-value by bootstrapping. # confidence interval by bootstrapping quantile(bt_est_ols1$genreRomance, c(0.025, 0.975)) # version 1 ## 2.5% 97.5% ## -0.01984114 1.24606817 c(&#39;2.5%&#39; = sample_Romance - 2.00 * bt_sd_ols1[2], &#39;97.5%&#39; = sample_Romance + 2.00 * bt_sd_ols1[2]) # version 2 ## 2.5%.genreRomance 97.5%.genreRomance ## -0.04297487 1.24964154 # p-value bt_est_ols1 %&gt;% with( sum(abs(genreRomance - sample_Romance) &gt; abs(sample_Romance))/length(genreRomance) ) ## [1] 0.0604 These results are also very close to what we saw for the Welch’s t-statistic and its bootstrap estimates above. Now let’s get a bit deeper into the modeling aspect of the linear models. The regression allows us to utilize additional variables in the model. Let’s try adding a linear effect of movie budget. It seems reasonable to hypothesize that the higher the budget, the better a movie can be since the director can hire famous actors and actresses and use expensive movie settings and computer graphics. Then, our estimation equation becomes \\[ y_{ij} = a_0 + \\alpha_j + \\beta_1 \\:budget_i + \\varepsilon_{ij}.\\] ols2 &lt;- lm( rating ~ genre + budget, data = movie_sample) summary(ols2) ## ## Call: ## lm(formula = rating ~ genre + budget, data = movie_sample) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.20205 -0.69226 -0.07884 0.71999 2.92299 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.424309 0.304038 17.841 &lt;2e-16 *** ## genreRomance 0.753435 0.330186 2.282 0.0263 * ## budget 0.006944 0.004717 1.472 0.1465 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.216 on 57 degrees of freedom ## Multiple R-squared: 0.09319, Adjusted R-squared: 0.06137 ## F-statistic: 2.929 on 2 and 57 DF, p-value: 0.06155 The new estimate of \\(\\alpha_R\\) is 0.753 with standard error 0.330. A common interpretation goes like this; the relative effect of Romance to Action genre on the movie rating is 0.753 at the 5% significance level, while controlling for the effect of movie budget. Note that the effect of budget itself is not statistically significant even at the 10% level, but having this variable allows for more robust results. Recall that an important assumption in regression analysis is that the error term is uncorrelated with independent variables. Here our model is using the assumption that \\[E[\\varepsilon_{ij} \\:| \\: genre_j,\\: budget_i] = E[\\varepsilon_{ij} ] = 0\\] where \\(E[v | u]\\) denotes the conditional mean of \\(v\\) given \\(u\\). It says that the information of \\(genre_j\\) and \\(budget_i\\) does not affect the mean of the error distribution. There is no definitive way to test this assumption, so that has to hold at least conceptually. The assumption may be violated in several ways. The most relevant case here is what is known as omitted variable bias; some unobserved attributes of the movie (which are conceptually a part of the error \\(\\varepsilon_{ij}\\)) may be correlated with both \\(y_{ij}\\) and \\(budget_i\\). For example, many people think that the use of explosions make action movies more exciting and romance movies more dramatic. Then, suppose that the story taking place in the World War setting can increase the movie rating and also inflate the movie budget. In such a case, the OLS estimates could be biased via the omitted variable (an indicator for having a World War setting). In a future session, we will talk more about the potential sources of bias. Let’s not worry about them here. The bottom line: every model is incorrect when applied to some real-world data, and the “error term” captures the difference. The correctness is always a matter of degree, and that’s why we care about the error term; how big are its dispersion and its tails? is it systematically varying with independent variables? and so forth. Predicted error can be diagnosed for certain properties, yet the true error is essentially conceptual, for which some judgement on the side of the analyst is necessarily involved. This is why developing sound intuitions is crucial. It is no secret that the statistical modeling of real-world data is an art as much as a science. Okay, let’s get back to our topic. Now how about adding another variable to our model? The movies in the dataset were released between 1970 and 2005, during which movie-goers’ preferences or movie production costs may have shifted. By accounting for a quadratic time trend, we estimate the following \\[ y_{ij} = a_0 + \\alpha_j + \\beta_1 \\:budget_i + \\beta_2 \\: year_i + \\beta_3 \\: year^2_i + \\varepsilon_{ij}\\] ols3 &lt;- lm( rating ~ genre + budget + year + I(year^2), data = movie_sample) # I() allows the user to construct a new variable on the fly. summary(ols3) ## ## Call: ## lm(formula = rating ~ genre + budget + year + I(year^2), data = movie_sample) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.02921 -0.70194 0.03311 0.60606 3.00738 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.108e+03 7.024e+03 0.727 0.4701 ## genreRomance 7.918e-01 3.254e-01 2.434 0.0182 * ## budget 8.452e-03 4.700e-03 1.798 0.0776 . ## year -5.087e+00 7.058e+00 -0.721 0.4741 ## I(year^2) 1.268e-03 1.773e-03 0.715 0.4776 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.196 on 55 degrees of freedom ## Multiple R-squared: 0.1544, Adjusted R-squared: 0.09288 ## F-statistic: 2.51 on 4 and 55 DF, p-value: 0.05214 When accounting for the time trend, the coefficient for budget is statistically significant at the 10% level, and the residual standard error is slightly reduced from 1.216 to 1.196. That seems like an improvement, and the model equation looks sensible. Now do we feel confident in our results? Let’s see if we can do more to check the results. We assumed a common quadratic time trend for Action and Romance movies. Let’s take a step back and visualize the time trend in the data using ggplot() + geom_jitter() + geom_smooth(). By default geom_smooth() uses a flexible form to fit the relationship between x and y variables. movie_sample %&gt;% ggplot(aes( x = year, y = rating, color = genre)) + geom_jitter() + geom_smooth(se =F) ## `geom_smooth()` using method = &#39;loess&#39; We immediately spot that in our sample, we only have a few movies from the 1970-1984 period. Also, we see that the time trend seems to be shifted around year 2000. Let’s set aside those older movies in our model. Also, we can use our estimate \\(\\hat{\\beta_1}\\) to control for the varying impacts of movie budget (\\(\\hat{\\beta_1} \\: budget_i\\)), which we replace with the sample average (\\(\\hat{\\beta_1} \\: E[budget_i]\\)). And here is an update; movie_sample %&gt;% filter(year &gt;= 1985) %&gt;% ggplot(aes( x = year, y = rating - ols3$coefficients[&#39;budget&#39;]*(budget - mean(budget)), color = genre)) + geom_jitter() + geom_smooth(se =FALSE) ## `geom_smooth()` using method = &#39;loess&#39; We still see the shift in time trend around 2000, at which the trends between Action and Romance movies start to diverge. Year 2000 may be the beginning of increasing computer graphics usage due to its decreasing costs. That could be a turning point, especially for Action movies. Now what can we do? Assigning different time trends for Action and Romance would be a possibility if our objective were to simply find the model that best fits the data. But, that may not be a good idea if we are interested in comparing the average ratings of the two genres. After all, the genre-specific time trend is a part of the difference between genres that we want to compare. The best we could do seems that we let the time trend vary before and after 2000, while assuming the common trend for both genres. Here is a relatively simple solution. \\[ \\begin{align} \\nonumber y_{ij} &amp;= a_0 + \\alpha_{j} + \\beta_1 \\:budget_i + \\beta_2 \\: year_i + \\beta_3 \\: year^2_i \\\\ \\nonumber &amp;+ (a_{0,M} + \\alpha_{j, M} + \\beta_{1,M} \\:budget_i + \\beta_{2,M} \\: year_i + \\beta_{3,M} \\: year^2_i) \\:M_i + \\varepsilon_{ij} \\end{align} \\] where \\(M_i = 1(year_i\\ge2000)\\) is an indicator variable for post-millennium years. The items in parenthesis multiplied by \\(M_i\\) are the interaction terms between the baseline variables and the millennium indicator. These additional terms captures the additional effects that only apply to post-millennium movies. The interaction terms can be constructed with * symbol in lm(). ols4 &lt;- lm( rating ~ genre*I(year&gt;=2000) + budget*I(year&gt;=2000) + year*I(year&gt;=2000) + I(year^2)*I(year&gt;=2000), data = subset(movie_sample, year&gt;=1985)) summary(ols4) ## ## Call: ## lm(formula = rating ~ genre * I(year &gt;= 2000) + budget * I(year &gt;= ## 2000) + year * I(year &gt;= 2000) + I(year^2) * I(year &gt;= 2000), ## data = subset(movie_sample, year &gt;= 1985)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.8226 -0.6920 -0.1258 0.6058 3.5089 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.649e+05 5.796e+04 -2.845 0.006515 ## genreRomance 1.801e+00 4.946e-01 3.642 0.000662 ## I(year &gt;= 2000)TRUE -8.242e+04 4.076e+05 -0.202 0.840627 ## budget 1.448e-02 8.152e-03 1.776 0.082040 ## year 1.656e+02 5.820e+01 2.846 0.006498 ## I(year^2) -4.159e-02 1.461e-02 -2.847 0.006482 ## genreRomance:I(year &gt;= 2000)TRUE -1.497e+00 7.143e-01 -2.096 0.041371 ## I(year &gt;= 2000)TRUE:budget -4.906e-03 1.100e-02 -0.446 0.657487 ## I(year &gt;= 2000)TRUE:year 8.120e+01 4.072e+02 0.199 0.842791 ## I(year &gt;= 2000)TRUE:I(year^2) -2.000e-02 1.017e-01 -0.197 0.844965 ## ## (Intercept) ** ## genreRomance *** ## I(year &gt;= 2000)TRUE ## budget . ## year ** ## I(year^2) ** ## genreRomance:I(year &gt;= 2000)TRUE * ## I(year &gt;= 2000)TRUE:budget ## I(year &gt;= 2000)TRUE:year ## I(year &gt;= 2000)TRUE:I(year^2) ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.119 on 48 degrees of freedom ## Multiple R-squared: 0.3259, Adjusted R-squared: 0.1995 ## F-statistic: 2.578 on 9 and 48 DF, p-value: 0.01649 The results show that accounting for the effects of budget and distinct time trends for two time spans 1985-1999 and 2000-2005, on average the Romance movie has a 1.801 (\\(\\alpha \\le 0.001\\)) higher rating during 1985-1999 and 0.304 ( = 1.801 - 1.497) higher rating during 2000-2005, compared to the Action movie. To see whether the total effect for the latter period (\\(\\alpha_R + \\alpha_{R,M}\\)) is statistically different from zero, we can use Wald test (using a function from aod package). The standard notation is \\[H_0: \\Gamma \\beta = r\\] where \\(\\beta\\) is the coefficients of the linear model, \\(\\Gamma\\) is a matrix that specifies linear combinations of \\(\\beta\\), and \\(r\\) is a column vector of constants. In this case, we only have a single equation for \\(H_0\\) (i.e. a single row), namely \\(H_0: \\alpha_R + \\alpha_{R, M} = 0\\). This corresponds to \\(\\Gamma = [0\\: 1\\: 0\\: 0\\: 0\\: 0\\: 1\\: 0\\: 0\\: 0]\\) (the second and sixth coefficients corresponding to \\(\\alpha_R\\) and \\(\\alpha_{R, M}\\)) and \\(r = [0\\: 0\\: 0\\: 0\\: 0\\: 0\\: \\: 0\\: 0\\: 0]&#39;\\). The test statistic \\(\\alpha_R + \\alpha_{R, M}\\) approximately follows the chi-square distribution. gamma &lt;- matrix(c(0, 1, 0, 0, 0, 0, 1, 0, 0, 0), nrow=1) wald.test(Sigma = vcov(ols4), b=coef(ols4), L=gamma) ## Wald test: ## ---------- ## ## Chi-squared test: ## X2 = 0.35, df = 1, P(&gt; X2) = 0.56 which shows the p-value of 0.56. Thus, we fail to reject \\(H_0: \\alpha_R + \\alpha_{R, M} = 0\\) at the 10% significance level. This means that there is not significant difference between genres for the 2000-2005 period. We can visualize the results in a plot. We plot fitted values as data points and their time trends that are approximated by geom_smooth() with a polynomial fit (i.e., having two quadratic time trends is akin to a polynomial of degree 4). We are looking to see whether these predicted trends resemble the raw-data time trends we saw above. movie_sample$ols4_fitted &lt;- NA movie_sample$ols4_fitted[movie_sample$year&gt;=1985] &lt;- ols4$fitted.values movie_sample %&gt;% filter(year &gt;= 1985) %&gt;% ggplot(aes( x = year, y = ols4_fitted, color = genre)) + geom_jitter() + geom_smooth(formula = y ~ x + x^2 + x^3 + x^4, se =FALSE) ## `geom_smooth()` using method = &#39;loess&#39; It appears that the model captured the central features of the original time trends. Using the predicted parameters \\(\\widehat{\\beta}_{1}\\) and \\(\\widehat{\\beta}_{1,M}\\), we can further account for the varying effects of budget (\\(\\widehat{\\beta}_1 \\:budget_i + \\widehat{\\beta}_{1,M} \\:budget_i \\: M_i\\)) by replacing them with the sample average (\\(\\widehat{\\beta}_1 \\:E[budget_i] + \\widehat{\\beta}_{1,M} \\:E[budget_i | M_i] \\: M_i\\)). This shows the prediction net the effect of budgets. repl_avg_budget &lt;- movie_sample %&gt;% filter(year&gt;=1985) %&gt;% mutate(budget_after_2000 = budget*(year&gt;=2000)) %&gt;% with( ols4$coefficients[&#39;budget&#39;]*(budget - mean(budget)) + + ols4$coefficients[&#39;I(year &gt;= 2000)TRUE:budget&#39;]* (budget_after_2000 - mean(budget_after_2000)) ) movie_sample$ols4_fitted2 &lt;- NA movie_sample$ols4_fitted2[movie_sample$year&gt;=1985] &lt;- ols4$fitted.values - repl_avg_budget movie_sample %&gt;% filter(year &gt;= 1985) %&gt;% ggplot(aes( x = year, y = ols4_fitted2, color = genre)) + geom_jitter() + # geom_smooth( formula = y ~ (x + x^2)*I(x &gt;= 2000), se =FALSE) # alternative geom_smooth( formula = y ~ x + x^2 + x^3 + x^4, se =FALSE) ## `geom_smooth()` using method = &#39;loess&#39; In summary, we find that accounting for the time trends and movie budgets, the Romance movie has a 1.801 (\\(\\alpha ≤0.001\\)) higher rating than the Action movie for 1985-1999, but the difference is insignificant for 2000-2005. The results from the two sample t-test and various regression models may be correct in their own light, but we see that the conclusions may differ depending on what question we ask and how we execute the analysis. Rarely we ask the right question in our first try, do we need the exploratory data analysis and the tools and intuitions to help us with that. Exercise Now it is your turn. Download materials: We will use the same movie2 data data as shown above Set working directly: setwd(your_directory) Load libraries: library(dplyr), library(ggplot2), library(broom), library(tidyr), library(mosaic), library(lme4) Part A: Observe the Central Limit Theorem (CLT) Load movies2 data, which we consider as the population of interest. Let rating \\(y_{ij}\\) for observation \\(i\\) in genre \\(j\\) where \\(j\\) is either \\(A\\) for Action or \\(R\\) for Romance. Calculate population means \\(\\mu_{A}\\) and \\(\\mu_{R}\\) and standard errors \\(\\sigma_{A}\\) and \\(\\sigma_{R}\\) for Action and Romance movies. Calculate the population mean and standard deviation of difference \\(y_{iA} - y_{iR}\\). Hint: the variance of the sum of uncorrelated variables is \\(Var[a + b] = Var[a]\\) and \\(Var[b]\\) for variables \\(a\\) and \\(b\\). What is the predicted distribution of the sample mean difference \\(\\bar{y}_{A} - \\bar{y}_{R}\\) by the CLT? Draw a random sample of 30 observations from each genre and summarize them for stats (mean, sd, and number of observations). The sampling function in dplyr is sample_n(). Hint: Look back to see what we did above and copy the procedure; having the same format is important for the later part of the exercise. Turn the previous step d into a function, for which the input argument is a sample size and the output is the summary stats by genre. Apply this function to generate a set of 100 bootstrap replications using mosaic::do(100) * { function(N=30) }. Reshape the bootstrap results, plot its density distribution, and calculate summary statistics using the following functions; reshape_movie_samples &lt;- function(bt_samples) { bt_samples %&gt;% data.frame() %&gt;% # don&#39;t forget to use data.frame() dplyr::select(-.row) %&gt;% reshape(idvar= &quot;.index&quot;, timevar=&quot;genre&quot;, direction=&quot;wide&quot;) %&gt;% mutate(bt_diff = (mean.Action - mean.Romance)) } density_sample_movies &lt;- function(rehsaped_samples, N, B) { rehsaped_samples %&gt;% ggplot(aes(x = bt_diff)) + geom_density(fill = &quot;steelblue&quot;, adjust = 2, alpha = .75) + xlim(c(-2, 2) + pop_diff) + geom_vline(xintercept = mean(rehsaped_samples$bt_diff), color = &quot;steelblue&quot;, size = 1) + geom_vline(xintercept = pop_diff, color = &quot;yellow&quot;, size = 1) + # CTL prediction mean stat_function(fun = dnorm, colour = &quot;yellow&quot;, size =1, # CTL prediction distribution args = list(mean = pop_diff, sd = pop_sigma/sqrt(rehsaped_samples$n.Action[1]))) + labs(title = paste0(&quot;Bootstrop: &quot;, B, &quot;, Num observations:&quot;, N )) } stats_sample_movies &lt;- function(reshaped_samples) { reshaped_samples %&gt;% summarize( diff_mean = mean(bt_diff), diff_sd = sd(bt_diff), p_val = sum(bt_diff&gt;0)/length(bt_diff)*2, theory_mean = pop_diff, theory_sd = pop_sigma/sqrt(length(bt_diff)), abs_error_mean = abs(diff_mean - theory_mean), abs_error_sd = abs(diff_sd - theory_sd) ) } Review the above functions to understand each line. Use ?function_name for look-up. Observe how p_val in stats_sample_movies() relates to the area of the density generated by density_sample_movies(). Also, check what theoretical sd in stats_sample_movies() calculates (sd of what?). Change N and B several times to observe how they affect the results. Part B: Analyze the performance of CLT Pick 6 values between 0 and 120 for the number of observations N and store them as a vector named loc_N: i.e., loc_N &lt;- c(20, 30, ...). Pick 5 values between 100 and 5000 for the number of bootstrap replications B and store them as a vector named loc_B. Conduct 30 experiments of bootstrapping for each combination of N and B from loc_N and loc_B and store the results of density plots and summary stats in nested lists using the following code; list_density &lt;- list() list_stats &lt;- list() # This will take some time for (idx_N in 1:length(loc_N)) { list_density[[idx_N]] &lt;- list() list_stats[[idx_N]] &lt;- list() for (idx_B in 1:length(loc_B)) { print(paste0(&#39;N =&#39;, loc_N[idx_N],&#39;, B = &#39;, loc_B[idx_B])) my_boot1 &lt;- mosaic::do(loc_B[idx_B]) * { my_movie_samples(loc_N[idx_N]) } reshaped_my_boot1 &lt;- reshape_movie_samples(my_boot1) list_density[[idx_N]][[idx_B]] &lt;- density_sample_movies(reshaped_my_boot1, loc_N[idx_N], loc_B[idx_B]) list_stats[[idx_N]][[idx_B]] &lt;- stats_sample_movies(reshaped_my_boot1) } } Print the density plots and observe how they vary with \\(N\\). Do this for the largest \\(B\\) first, then the smallest \\(B\\). You can use the following code and use the arrows (&lt;-, -&gt;) in the Plots Pane of Rstudio. How would you characterize the results? # Use Plots Pane in RStudio &lt;- -&gt; to observe the influence of N for (idx_N in 1:length(loc_N)) print(list_density[[idx_N]][[which(loc_B==max(loc_B))]]) # dispersion decreases with N for (idx_N in 1:length(loc_N)) print(list_density[[idx_N]][[which(loc_B==min(loc_B))]]) Use the following code to extract the results from the nested lists. extract_list_stats_N &lt;- function(seq, idx_B, stat) { lapply(c(1:length(seq)), function (idx_N) list_stats[[idx_N]][[idx_B]][[stat]]) %&gt;% unlist() } extract_list_stats_B &lt;- function(seq, idx_N, stat) { lapply(c(1:length(seq)), function (idx_B) list_stats[[idx_N]][[idx_B]][[stat]]) %&gt;% unlist() } max_B &lt;- which(loc_B==max(loc_B)) # index of max B max_N &lt;- which(loc_N==max(loc_N)) # index of max N results_N &lt;- data.frame( N = loc_N, p_val = extract_list_stats_N(loc_N, max_B, &quot;p_val&quot;), abs_error_mean = extract_list_stats_N(loc_N, max_B, &quot;abs_error_mean&quot;), abs_error_sd = extract_list_stats_N(loc_N, max_B, &quot;abs_error_sd&quot;) ) results_B &lt;- data.frame( B = loc_B, p_val = extract_list_stats_B(loc_B, max_N, &quot;p_val&quot;), abs_error_mean = extract_list_stats_B(loc_B, max_N, &quot;abs_error_mean&quot;), abs_error_sd = extract_list_stats_B(loc_B, max_N, &quot;abs_error_sd&quot;) ) Use ggplot() on results_N to characterize the relationships between sample size N and p_val, between N and abs_error_mean, and between N and abs_error_sd. Which relationship shows a clear pattern? Why? Hint: use geom_point() and geom_smooth(). How does this relate to the CLT? [Not essential.] Use ggplot() on results_B to characterize the relationships between bootstrap size B and p_val, between B and abs_error_mean, and between B and abs_error_sd. Which relationship shows a clear pattern? Why? Part C: Analyze data with linear models You will analyze ChickWeight data that is a part of the sample datasets automatically loaded when you start R. You will run linear models and get some practice on fixed effects (FE) and random effects (RE) models. The ChickWeight dataset contains data of a diet experiment on early growth of chicks. There are four variables: weight (gm), Time (days), Chick (id), and Diet (1 through 4 types). Run the following code to observe the basic structure of the data. ?ChickWeight # description shows up in the Help pane ChickWeight2 &lt;- ChickWeight # make a copy that we may modify head(ChickWeight2) table(ChickWeight2$Chick) table(ChickWeight2$Diet) table(ChickWeight2$Chick, ChickWeight2$Diet) ChickWeight2 %&gt;% ggplot(aes(x = Time, y = weight, color = Diet)) + geom_point(size = .25, alpha=.5) + facet_wrap(~Chick) How would you go about analyzing the effect of Diets on the weight growth? Let \\(weight_{ijt}\\) be the weight of chick \\(i\\) in Diet group \\(j\\) observed in time \\(t\\). Compose the following linear models with lm(), see the summary via summary(), and interpret the effects of four Diet types. Let’ start with a model of diet-specific intercepts and a quadratic time trend given by \\[ weight_{ijt} = \\alpha_j + \\beta_1\\: time_t + \\beta_2 \\: time^2_t + \\varepsilon_{ijt}. \\] Hint: Use I(Time^2) in the formula. Next try a FE model given by \\[ weight_{ijt} = \\alpha_j + \\beta_1\\: time_t + \\beta_2 \\: time^2_t + \\alpha_i + \\varepsilon_{ijt} \\] where \\(\\alpha_i\\) is a fixed effect representing a fixed intercept for each Chick. You may be surprised by the result. Next try a RE model given by \\[ weight_{ijt} = \\alpha_j + \\beta_1\\: time_t + \\beta_2 \\: time^2_t + v_{it}, \\quad v_{it} = \\alpha_i + \\varepsilon_{ijt}\\] where \\(\\alpha_i\\) is a random effect representing a random intercept for each Chick assumed to be normally distributed. Use lmer() from lme4 package instead of lm() and replace variable Chick with random intercept (1 | Chick) in the formula. Now try another RE model given by \\[ weight_{ijt} = \\alpha_0 + \\beta_1\\: time_t + \\beta_2 \\: time^2_t + v_{ijt}, \\quad v_{ijt} =\\alpha_j + \\alpha_i + \\varepsilon_{ijt}\\] where \\(\\alpha_j\\) and \\(\\alpha_i\\) are a Diet random effect and a Chick random effect respectively. This model gives you a decomposition of random variation into the random effects and the unexplained residual. Depending on your research objective, this type of model may be appropriate. To see which Diet has a larger effect, you need to take additional steps to obtain the conditional means of random effect intercepts. Try model@u and fitted(model) for your Siamese model. You probably observed that the above models yield very different results (except for the similarity between a and c). Why? Are some of these models wrong? The right answer would be that all of them are wrong. In the experiment, Diet types are probably randomly assigned across Chicks, and there are no obvious sources of bias in linear model construction. Thus, you should get similar results across models if your modeling approach is on the right track. Now go back to the initial plot of weight growth by chick and think about how else you could approach the problem. Did it occur to you that the weight probably started out about the same across Diet groups and then took different paths given the Diet type? Let’s try varying linear time trends with the shared initial average weight at \\(Time_t=0\\). \\[ weight_{ijt} = \\alpha_{0} + \\beta_{1j}\\: time_t + \\varepsilon_{ijt}\\] where \\(\\beta_{1j}\\) is a (fixed) Diet-specific linear time trend. Hint: use Diet*Time and -Diet in the formula to create the interaction terms between Time and Diet and suppress the fixed intercept of Diet. Now try \\[ weight_{ijt} = \\alpha_{0} + \\beta_{1j}\\: time_t + \\alpha_i + \\varepsilon_{ijt}\\] where \\(\\alpha_i\\) is a Chick fixed effect. Now try \\[ weight_{ijt} = \\alpha_{0} + \\beta_{1j}\\: time_t + v_{ijt}, \\quad v_{ijt} =\\alpha_i + \\varepsilon_{ijt}\\] where \\(\\alpha_i\\) is a Chick random effect. This time you should get pretty similar results across models in e, f, and g. How would you interpret the coefficient \\(\\beta_{1j}\\) of interaction terms between Time and Diet? Now try Diet-specific quadratic time trends; \\[ weight_{ijt} = \\alpha_{0} + \\beta_{1j}\\: time_t + \\beta_{2j} \\: time^2_t + \\varepsilon_{ijt}\\] where \\(\\alpha_0\\) is the common intercept across Diets. Given quadratic factorization \\(a t^2 +bt + c = a(t + b/2a)^2 +c- b^2/4\\), we may again suppress Diet-specific intercepts (using -Diet) by assuming that the average weight was the same across Diet groups at \\(Time = 0\\). Repeat this for the fixed effect and random effect models. How would you interpret the coefficient estimates on those quadratic time trends? Let’s check whether our assumption on the common intercept \\(\\alpha_0\\) in model h was playing a significant role in the estimation. Run the model without -Diet term to obtain \\[ weight_{ijt} = \\alpha_{0j} + \\beta_{1j}\\: time_t + \\beta_{2j} \\: time^2_t + \\varepsilon_{ijt}\\] where \\(\\alpha_{0j}\\) now varies with Diet group \\(j\\). See whether the results on \\(\\alpha_{0j}\\) are statistically significant and whether \\(\\beta_{1j}\\) and \\(\\beta_{2j}\\) are similar to the previous model. You may wonder if there is a way to visually pick up hints early on from the data. Well, let’s try to produce such a plot in retrospect. Modify the earlier plot and examine time trends by Diet groups using geom_smooth(). Part D Apply bootstrapping to linear models Earlier we looked at how we could apply bootstrapping to a linear model. Recall that we obtained bootstrap standard errors for the OLS coefficient estimates. Here you will apply bootstrapping to the models in e, f, and g above. Start with model e: \\[weight_{ijt} = \\alpha_{0} + \\beta_{1j}\\: time_t + \\varepsilon_{ijt}\\] and apply the procedure that we generated sample_ols1 and bt_ols1 above. Then copy the following functions and apply them to your results. # function generating a matrix of ones ones &lt;- function(r,c) matrix(c(rep(1,r*c)),nrow=r,ncol=c) # confidence interval by bootstrapping # version 1 ci_ver1 &lt;- function(est_bt, alpha = 0.05) { # est_bt: bootstrap estimates with row = boot replications, col = coefficients apply(est_bt, 2, function(x) quantile(x, c(alpha/2, 1 - alpha/2))) %&gt;% t() } # version 2 ci_ver2 &lt;- function(est_sample, bt_sd, df, alpha = 0.05) { # est_semple: sample estimate vector # bt_sd: bootstrap sd estimates of est_sample-vector # df: model degree of freedom cbind(&#39;2.5%&#39; = est_sample + qt(alpha/2, df) * bt_sd, &#39;97.5%&#39; = est_sample + qt(1-alpha/2, df) * bt_sd) } # bootstrap p-value bt_p_val &lt;- function(est_sample, est_bt) { # est_semple: sample estimate vector # est_bt: bootstrap estimates with row = boot replications, col = coefficients est_bt_long &lt;- est_bt %&gt;% data.frame() %&gt;% gather() est_bt_long &lt;- est_bt_long %&gt;% mutate( center_var = kronecker(ones(nrow(est_bt),1), matrix(est_sample, nrow=1)) %&gt;% c(), extremes = abs(value - center_var) &gt;= abs(center_var) ) p_val &lt;- est_bt_long %&gt;% group_by(key) %&gt;% summarise(p_val = sum(extremes)/nrow(est_bt)) return(list(p_val=p_val, df_long=est_bt_long)) } You can follow what goes into the input arguments of these functions by observing and mimicking the following. ci_ver1(bt_est_ols1) ## 2.5% 97.5% ## Intercept 5.29713352 6.176960 ## genreRomance -0.01984114 1.246068 ci_ver2(sample_ols1$estimate, bt_sd_ols1, ols1$df.residual) ## 2.5% 97.5% ## Intercept 5.27717949 6.182821 ## genreRomance -0.04352989 1.250197 bt_p_val(sample_ols1$estimate, bt_est_ols1)$p_val ## # A tibble: 2 × 2 ## key p_val ## &lt;chr&gt; &lt;dbl&gt; ## 1 genreRomance 0.0604 ## 2 Intercept 0.0000 Do the same for the histogram representation of bootstrap results; # histogram visualization coeff_bt_histogram &lt;- function(est_sample, est_bt, centering =FALSE) { # est_semple: sample estimate vector # est_bt: bootstrap estimates with row = boot replications, col = coefficients est_bt_long &lt;- bt_p_val(est_sample, est_bt)$df_long if (centering) { est_bt_long &lt;- est_bt_long %&gt;% mutate( key = paste(key, &quot; - center&quot;), value = value - center_var, fill_var = extremes ) legend_lab &lt;- &quot;Extremes: | value - center | &gt; | center |&quot; x_lab &lt;- &quot;value - center&quot; } else { est_bt_long &lt;- est_bt_long %&gt;% mutate( sign = kronecker(ones(nrow(est_bt),1), matrix(ifelse(est_sample&gt;0,1,-1), nrow=1)) %&gt;% c(), fill_var = value * sign &lt;= 0 ) legend_lab &lt;- &quot;Crossing zero?&quot; x_lab &lt;- &quot;value&quot; } est_bt_long %&gt;% ggplot(aes(x = value, fill = fill_var)) + geom_histogram(color = &quot;white&quot;, bins = 40) + theme(legend.position=&quot;top&quot;) + facet_wrap(~key, scales = &quot;free&quot;) + labs(fill = legend_lab, x = x_lab) } Here is how you use it; coeff_bt_histogram(sample_ols1$estimate, bt_est_ols1, centering=FALSE) coeff_bt_histogram(sample_ols1$estimate, bt_est_ols1, centering=TRUE) Repeat it for model f: \\[ weight_{ijt} = \\alpha_{0} + \\beta_{1j}\\: time_t + \\alpha_i+ \\varepsilon_{ijt}\\]. For the histogram, there are too many parameters to plot in one figure; split them into time trends and chick effects by applying filter(df_sample_est, grepl(&quot;Intercept&quot;, term) | grepl(&quot;Time&quot;, term)) and filter(df_sample_est, grepl(&quot;Chick&quot;, term)) to your sample estimate df_sample_est and select(df_bt_est, contains(&quot;Intercept&quot;), contains(&quot;Time&quot;)) and select(df_bt_est, contains(&quot;Chick&quot;)) to your bootstrap estimate df_bt_est (note: put dplyr:: before select() function to indicate that we want the one from the dplyr package. Try ?select to see if any other package has a function with the same name). For the chick part, just draw the histogram for the first six chicks. Now we will extend the bootstrapping algorithm to deal with Chick random effects for model g: \\[ weight_{ijt} = \\alpha_{0} + \\beta_{1j}\\: time_t + v_{ijt}, \\:\\: v_{ijt} = \\alpha_i+ \\varepsilon_{ijt}\\]. Use the modified bootstrap function for the random effect model; getFormula &lt;- function(model, lmer=FALSE) gsub(&quot;()&quot;,&quot;&quot;, ifelse(!lmer, model$call[2], model@call[2])) getDependentVar &lt;- function(model, lmer=FALSE) { str &lt;- getFormula(model, lmer=lmer) gsub(&quot; &quot;,&quot;&quot;, substr(str, 1, (regexpr(&quot;~&quot;,str)[1]-1))) } run_lmer_boot &lt;- function(lmer_rlt, num_do = 5000) { # Randome effects (RE) model bootstrapping (random intercepts, not random slopes) rlt &lt;- list() rlt$model &lt;- model_g@pp$X # model data for the part of fixed coefficients N &lt;- length(residuals(lmer_rlt)) rlt$fitted_no_RE &lt;- rlt$model %*% matrix(fixef(lmer_rlt), ncol=1) RE_vals &lt;- lmer_rlt@flist[[1]] %&gt;% unique() # RE variable values N_RE &lt;- length(RE_vals) # number of RE variable values rlt$RE_idx &lt;- rep(NA, N) # index of RE variable values for (i in 1:N_RE) rlt$RE_idx[which(lmer_rlt@flist[[1]] == RE_vals[i])] &lt;- i sd_res &lt;- sigma(lmer_rlt) # standard deviation of the residuals sd_RE &lt;- lmer_rlt@theta * sd_res # standard deviation of RE dep_var &lt;- getDependentVar(lmer_rlt, lmer=TRUE) do(num_do) * ({ data_bt &lt;- data.frame(lmer_rlt@frame) # replace the dependent variable with its bootstrap counterpart data_bt[[dep_var]] &lt;- rlt$fitted_no_RE + # the predicted component + rnorm(N_RE, mean = 0, sd = sd_RE)[rlt$RE_idx] + # random draws of the RE + rnorm(N, mean = 0, sd = sd_res) # random draws of the residual # run the RE model with the same formula but with a new, bootstrap dataset lmer_bt &lt;- lmer(as.formula(getFormula(lmer_rlt, lmer=TRUE)), data = data_bt) sd_res_bt &lt;- sigma(lmer_bt) sd_RE_bt &lt;- lmer_bt@theta * sd_res_bt c(fixef(lmer_bt), sigma_RE = sd_RE_bt, sigma_res = sd_res_bt) # get parameters }) } Inside do(num_do)({...}), observe the additional random component for the random effects. It draws a vector of random intercepts at length being the number of Chicks, and then assign those to individual Chicks. Note that in the version 2 confidence interval calculation, the degree of freedom is given as the number of observations minus the number of parameters (fixed coefficient parameters plus two standard error parameters for RE and residuals). The Key To be posted. Reflections To be written. "],
["3-3-next.html", "3.3 Upcoming topics", " 3.3 Upcoming topics Statistical inferences with simulations Linear regressions -->"],
["4-resources.html", "4 Resources", " 4 Resources Here are more resources for learning R. Free Books Official CRAN R Manual Quick R The Art of R Programming by Norman Matloff ModernDive Impatient R Simple R by John Verzani Introduction to Probability and Statistics Using R By Jay Kerns R Wikibook Cookbook for R by Winston Chang OnePageR The R Inferno Videos Coursera’s four week course videos Workflow example video by Jermey Chacon Video on Youtube Tutorials Exploratory Data Analysis in R (Recommended) R Tutorial R Bootcamp - Jared Knowles Step-by-step (sequential) interactive tutorial- Try R Another step-by-step interactive tutorial - swirl With Small Fees: Tutorials from DataCamp Cleaning Data in R Data Manipulation in R with dplyr Data Visualization in R with ggvis Data Visualization with ggplot2 Introduction to Coding Coding Resources for Beginners by Tori Dykes "],
["references.html", "References", " References "]
]
